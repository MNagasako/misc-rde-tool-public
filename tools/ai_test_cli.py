#!/usr/bin/env python3
"""
AIæ€§èƒ½ãƒ†ã‚¹ãƒˆãƒ„ãƒ¼ãƒ« - ARIM RDE Tool v1.13.1

æ¦‚è¦:
AIæ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’è¡Œã†ç‹¬ç«‹ã—ãŸã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ãƒ„ãƒ¼ãƒ«ã€‚
GUIç‰ˆã¨åŒæ§˜ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã—ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¸¬å®šã—ã¾ã™ã€‚

ä½¿ç”¨ä¾‹:
python tools/ai_test_cli.py --model gpt-4.1 --chars 1000
python tools/ai_test_cli.py --provider gemini --model gemini-1.5-flash --chars 500 --repeat 3
python tools/ai_test_cli.py --provider local_llm --model llama3.2:3b --chars 2000 --padding
"""

import os
import sys
import json
import time
import random
import string
import argparse
import requests
from typing import Dict, List, Any
from datetime import datetime

# ãƒ‘ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ï¼ˆCWDéä¾å­˜ï¼‰
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from src.config.common import get_dynamic_file_path, get_base_dir

class AITestCLI:
    """AIæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ãƒ„ãƒ¼ãƒ«"""
    
    def __init__(self):
        self.config_path = get_dynamic_file_path("input/ai_config.json")
        self.config = self._load_config()
        self.session = requests.Session()
        self.results = []
        self.verbose = False  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯False
        
    def _load_config(self) -> Dict[str, Any]:
        """AIè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            if os.path.exists(self.config_path):
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                print(f"âš ï¸ AIè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.config_path}")
                return self._get_default_config()
        except Exception as e:
            print(f"âŒ AIè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’è¿”ã™"""
        return {
            "ai_providers": {
                "openai": {"enabled": False, "api_key": "", "models": ["gpt-4.1-mini", "gpt-4.1", "gpt-3.5-turbo"]},
                "gemini": {"enabled": False, "api_key": "", "models": ["gemini-1.5-flash", "gemini-1.5-pro"]},
                "local_llm": {"enabled": False, "base_url": "http://localhost:11434/v1", "models": ["llama3.2:3b"]}
            },
            "default_provider": "openai",
            "timeout": 30,
            "max_tokens": 1000,
            "temperature": 0.7
        }
    
    def _generate_test_text(self, target_chars: int, add_padding: bool = False) -> str:
        """ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        # ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹ææ–™ç ”ç©¶é–¢é€£ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        base_templates = [
            "ã“ã®ææ–™ã®æ©Ÿæ¢°çš„ç‰¹æ€§ã«ã¤ã„ã¦è©³ç´°ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "çµæ™¶æ§‹é€ ã¨é›»æ°—çš„æ€§è³ªã®é–¢ä¿‚æ€§ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚",
            "ç†±å‡¦ç†æ¡ä»¶ãŒææ–™çµ„ç¹”ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚",
            "Xç·šå›æŠ˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ç›¸åŒå®šã‚’è¡Œã„ã€æ ¼å­å®šæ•°ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚",
            "èµ°æŸ»é›»å­é¡•å¾®é¡è¦³å¯Ÿçµæœã«åŸºã¥ã„ã¦çµ„ç¹”è§£æã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚",
            "å¼•å¼µè©¦é¨“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é™ä¼å¼·åº¦ã¨ç ´æ–­ä¼¸ã³ã‚’æ±‚ã‚ã¦ãã ã•ã„ã€‚",
            "ç¤ºå·®ç†±åˆ†æçµæœã‚’ç”¨ã„ã¦ç›¸å¤‰æ…‹æ¸©åº¦ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚",
            "è…é£Ÿè©¦é¨“ã«ãŠã‘ã‚‹é‡é‡æ¸›å°‘ç‡ã¨è…é£Ÿæ©Ÿæ§‹ã‚’è€ƒå¯Ÿã—ã¦ãã ã•ã„ã€‚"
        ]
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é¸æŠã—ã¦ãƒ™ãƒ¼ã‚¹ã¨ã™ã‚‹
        base_text = random.choice(base_templates)
        
        if add_padding:
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ç”¨ã®è¿½åŠ ãƒ†ã‚­ã‚¹ãƒˆ
            padding_sentences = [
                "ã•ã‚‰ã«è©³ç´°ãªè§£æãŒå¿…è¦ã§ã™ã€‚",
                "å®Ÿé¨“æ¡ä»¶ã‚’å¤‰æ›´ã—ã¦è¿½åŠ æ¤œè¨¼ã‚’è¡Œã„ã¾ã™ã€‚",
                "çµ±è¨ˆçš„è§£æã«ã‚ˆã‚Šä¿¡é ¼æ€§ã‚’ç¢ºèªã—ã¾ã™ã€‚",
                "éå»ã®ç ”ç©¶äº‹ä¾‹ã¨ã®æ¯”è¼ƒæ¤œè¨ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚",
                "ç†è«–å€¤ã¨ã®ä¹–é›¢ã«ã¤ã„ã¦è€ƒå¯Ÿã—ã¾ã™ã€‚",
                "æ¸¬å®šèª¤å·®ã®ç¯„å›²å†…ã§ã®è©•ä¾¡ã‚’è¡Œã„ã¾ã™ã€‚"
            ]
            
            current_text = base_text
            
            # ç›®æ¨™æ–‡å­—æ•°ã«é”ã™ã‚‹ã¾ã§è¿½åŠ 
            while len(current_text) < target_chars:
                remaining_chars = target_chars - len(current_text)
                
                if remaining_chars > 50:
                    # ååˆ†ãªä½™è£•ãŒã‚ã‚‹å ´åˆã¯æ–‡ã‚’è¿½åŠ 
                    current_text += " " + random.choice(padding_sentences)
                else:
                    # æ®‹ã‚Šæ–‡å­—æ•°ãŒå°‘ãªã„å ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ æ–‡å­—ã§èª¿æ•´
                    current_text += " " + "".join(random.choices(string.ascii_letters + string.digits, k=remaining_chars))
                    break
            
            return current_text[:target_chars]
        else:
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãªã—ã®å ´åˆã¯ã€ãƒ™ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸºæº–æ–‡å­—æ•°ã«èª¿æ•´
            if len(base_text) > target_chars:
                return base_text[:target_chars]
            else:
                # ä¸è¶³åˆ†ã‚’è¿½åŠ ã®èª¬æ˜ã§è£œã†
                additional = "å…·ä½“çš„ãªãƒ‡ãƒ¼ã‚¿ã¨è§£ææ‰‹æ³•ã‚’å«ã‚ã¦è©³ç´°ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
                repeat_count = (target_chars - len(base_text)) // len(additional) + 1
                extended_text = base_text + " " + (additional + " ") * repeat_count
                return extended_text[:target_chars]
    
    def _call_openai_api(self, model: str, prompt: str) -> Dict[str, Any]:
        """OpenAI APIã‚’å‘¼ã³å‡ºã—"""
        config = self.config["ai_providers"]["openai"]
        api_key = config.get("api_key")
        
        if not api_key:
            raise ValueError("OpenAI API keyãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        url = "https://api.openai.com/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # ãƒ¢ãƒ‡ãƒ«åã«ã‚ˆã£ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆ‡ã‚Šæ›¿ãˆ
        data = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}]
        }
        
        if model.startswith("gpt-5") or model.startswith("gpt-4.1"):
            # GPT-5ç³»ãŠã‚ˆã³GPT-4.1ç³»ã§ã¯ max_completion_tokens ã‚’ä½¿ç”¨
            # ã¾ãšãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãªã—ã§è©¦ã—ã¦ã¿ã‚‹
            if model.startswith("gpt-5"):
                # GPT-5ç³»ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€å°é™ã«
                pass  # max_completion_tokensã‚’æŒ‡å®šã—ãªã„
            elif "nano" in model.lower():
                data["max_completion_tokens"] = 50  # nanoã¯æ¥µã‚ã¦å°ã•ãªå€¤
            else:
                data["max_completion_tokens"] = max(1000, self.config.get("max_tokens", 1000))
            # GPT-5ç³»ãŠã‚ˆã³GPT-4.1ç³»ã§ã¯ temperature ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®ãŸã‚çœç•¥
        else:
            # GPT-4ä»¥å‰ã§ã¯å¾“æ¥ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            data["max_tokens"] = self.config.get("max_tokens", 1000)
            data["temperature"] = self.config.get("temperature", 0.7)
        
        start_time = time.time()
        
        try:
            response = self.session.post(url, headers=headers, json=data, timeout=self.config.get("timeout", 120))
        except requests.exceptions.Timeout:
            return {
                "success": False,
                "error": f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({self.config.get('timeout', 120)}ç§’)",
                "response_time": time.time() - start_time
            }
        except requests.exceptions.RequestException as e:
            return {
                "success": False,
                "error": f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}",
                "response_time": time.time() - start_time
            }
        
        end_time = time.time()
        
        if response.status_code == 200:
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            
            return {
                "success": True,
                "content": content,
                "response_time": end_time - start_time,
                "tokens_used": result.get("usage", {}).get("total_tokens", 0),
                "model": model
            }
        else:
            return {
                "success": False,
                "error": f"HTTP {response.status_code}: {response.text}",
                "response_time": end_time - start_time,
                "model": model
            }
    
    def _call_gemini_api(self, model: str, prompt: str) -> Dict[str, Any]:
        """Gemini APIã‚’å‘¼ã³å‡ºã—"""
        config = self.config["ai_providers"]["gemini"]
        api_key = config.get("api_key")
        
        if not api_key:
            raise ValueError("Gemini API keyãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}"
        headers = {"Content-Type": "application/json"}
        
        data = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {
                "maxOutputTokens": self.config.get("max_tokens", 1000),
                "temperature": self.config.get("temperature", 0.7)
            }
        }
        
        start_time = time.time()
        response = self.session.post(url, headers=headers, json=data, timeout=self.config.get("timeout", 30))
        end_time = time.time()
        
        if response.status_code == 200:
            result = response.json()
            content = result["candidates"][0]["content"]["parts"][0]["text"]
            
            return {
                "success": True,
                "content": content,
                "response_time": end_time - start_time,
                "tokens_used": result.get("usageMetadata", {}).get("totalTokenCount", 0),
                "model": model
            }
        else:
            return {
                "success": False,
                "error": f"HTTP {response.status_code}: {response.text}",
                "response_time": end_time - start_time,
                "model": model
            }
    
    def _call_local_llm_api(self, model: str, prompt: str) -> Dict[str, Any]:
        """ãƒ­ãƒ¼ã‚«ãƒ«LLM APIã‚’å‘¼ã³å‡ºã—ï¼ˆOllamaå½¢å¼ï¼‰"""
        config = self.config["ai_providers"]["local_llm"]
        base_url = config.get("base_url", "http://localhost:11434/v1")
        
        url = f"{base_url}/chat/completions"
        headers = {"Content-Type": "application/json"}
        
        data = {
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": self.config.get("max_tokens", 1000),
            "temperature": self.config.get("temperature", 0.7)
        }
        
        start_time = time.time()
        try:
            response = self.session.post(url, headers=headers, json=data, timeout=self.config.get("timeout", 30))
            end_time = time.time()
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                return {
                    "success": True,
                    "content": content,
                    "response_time": end_time - start_time,
                    "tokens_used": result.get("usage", {}).get("total_tokens", 0),
                    "model": model
                }
            else:
                return {
                    "success": False,
                    "error": f"HTTP {response.status_code}: {response.text}",
                    "response_time": end_time - start_time,
                    "model": model
                }
        except requests.ConnectionError:
            end_time = time.time()
            return {
                "success": False,
                "error": "ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“",
                "response_time": end_time - start_time,
                "model": model
            }
    
    def run_test(self, provider: str, model: str, chars: int, add_padding: bool = False, repeat: int = 1) -> List[Dict[str, Any]]:
        """AIãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        print(f"ğŸš€ AIãƒ†ã‚¹ãƒˆé–‹å§‹")
        print(f"ğŸ“‹ è¨­å®š: Provider={provider}, Model={model}, æ–‡å­—æ•°={chars}, ç¹°ã‚Šè¿”ã—={repeat}")
        print(f"â° é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("-" * 60)
        
        results = []
        
        for i in range(repeat):
            print(f"\nğŸ“ ãƒ†ã‚¹ãƒˆ {i+1}/{repeat} å®Ÿè¡Œä¸­...")
            
            # ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
            test_prompt = self._generate_test_text(chars, add_padding)
            print(f"ğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(test_prompt)} æ–‡å­—")
            print(f"ğŸ”¤ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå…ˆé ­100æ–‡å­—ï¼‰: {test_prompt[:100]}...")
            
            try:
                # APIå‘¼ã³å‡ºã—
                if provider == "openai":
                    result = self._call_openai_api(model, test_prompt)
                elif provider == "gemini":
                    result = self._call_gemini_api(model, test_prompt)
                elif provider == "local_llm":
                    result = self._call_local_llm_api(model, test_prompt)
                else:
                    raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}")
                
                # çµæœè¡¨ç¤º
                if result["success"]:
                    print(f"âœ… æˆåŠŸ: {result['response_time']:.2f}ç§’")
                    print(f"ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: {result.get('tokens_used', 'N/A')}")
                    print(f"ğŸ“ ãƒ¬ã‚¹ãƒãƒ³ã‚¹é•·: {len(result['content'])} æ–‡å­—")
                    print(f"ğŸ”¤ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆå…ˆé ­200æ–‡å­—ï¼‰: {result['content'][:200]}...")
                else:
                    print(f"âŒ å¤±æ•—: {result['error']}")
                    print(f"â±ï¸ å¤±æ•—ã¾ã§ã®æ™‚é–“: {result['response_time']:.2f}ç§’")
                
                # çµæœã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
                result["test_number"] = i + 1
                result["prompt_length"] = len(test_prompt)
                result["timestamp"] = datetime.now().isoformat()
                results.append(result)
                
            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                results.append({
                    "success": False,
                    "error": str(e),
                    "test_number": i + 1,
                    "prompt_length": len(test_prompt),
                    "timestamp": datetime.now().isoformat(),
                    "model": model
                })
        
        return results
    
    def print_summary(self, results: List[Dict[str, Any]]):
        """ãƒ†ã‚¹ãƒˆçµæœã®ã‚µãƒãƒªã‚’è¡¨ç¤º"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒª")
        print("=" * 60)
        
        successful_tests = [r for r in results if r.get("success", False)]
        failed_tests = [r for r in results if not r.get("success", False)]
        
        print(f"ğŸ¯ ç·ãƒ†ã‚¹ãƒˆæ•°: {len(results)}")
        print(f"âœ… æˆåŠŸ: {len(successful_tests)}")
        print(f"âŒ å¤±æ•—: {len(failed_tests)}")
        
        if successful_tests:
            response_times = [r["response_time"] for r in successful_tests]
            avg_time = sum(response_times) / len(response_times)
            min_time = min(response_times)
            max_time = max(response_times)
            
            print(f"\nâ±ï¸ ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“çµ±è¨ˆ:")
            print(f"   å¹³å‡: {avg_time:.2f}ç§’")
            print(f"   æœ€çŸ­: {min_time:.2f}ç§’")
            print(f"   æœ€é•·: {max_time:.2f}ç§’")
            
            tokens_used = [r.get("tokens_used", 0) for r in successful_tests if r.get("tokens_used", 0) > 0]
            if tokens_used:
                avg_tokens = sum(tokens_used) / len(tokens_used)
                print(f"ğŸª™ å¹³å‡ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: {avg_tokens:.0f}")
        
        if failed_tests:
            print(f"\nâŒ å¤±æ•—ç†ç”±:")
            error_counts = {}
            for test in failed_tests:
                error = test.get("error", "Unknown error")
                error_counts[error] = error_counts.get(error, 0) + 1
            
            for error, count in error_counts.items():
                print(f"   {error}: {count}å›")
    
    def save_results(self, results: List[Dict[str, Any]], output_file: str = None):
        """çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"ai_test_results_{timestamp}.json"
        
        output_path = get_dynamic_file_path(f"output/log/{output_file}")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")

def main():
    parser = argparse.ArgumentParser(description="AIæ€§èƒ½ãƒ†ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«")
    parser.add_argument("--provider", choices=["openai", "gemini", "local_llm"], 
                       default="openai", help="AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ (default: openai)")
    parser.add_argument("--model", type=str, required=True, help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--chars", type=int, default=500, help="ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ–‡å­—æ•° (default: 500)")
    parser.add_argument("--padding", action="store_true", help="æ–‡å­—æ•°ã‹ã•å¢—ã—æ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–")
    parser.add_argument("--repeat", type=int, default=1, help="ãƒ†ã‚¹ãƒˆç¹°ã‚Šè¿”ã—å›æ•° (default: 1)")
    parser.add_argument("--output", type=str, help="çµæœä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«å (default: auto)")
    parser.add_argument("--verbose", action="store_true", help="è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤º")
    
    args = parser.parse_args()
    
    # ãƒ†ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«åˆæœŸåŒ–
    cli = AITestCLI()
    cli.verbose = args.verbose  # verboseãƒ•ãƒ©ã‚°ã‚’è¨­å®š
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    try:
        results = cli.run_test(args.provider, args.model, args.chars, args.padding, args.repeat)
        
        # ã‚µãƒãƒªè¡¨ç¤º
        cli.print_summary(results)
        
        # çµæœä¿å­˜
        cli.save_results(results, args.output)
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
