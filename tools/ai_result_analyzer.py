#!/usr/bin/env python3
"""
AIæ€§èƒ½ãƒ†ã‚¹ãƒˆçµæœåˆ†æãƒ„ãƒ¼ãƒ« - ARIM RDE Tool v1.13.1

æ¦‚è¦:
ai_test_cli.pyã§ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚¹ãƒˆçµæœã‚’åˆ†æã—ã€
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚

ä½¿ç”¨ä¾‹:
python tools/ai_result_analyzer.py output/log/ai_test_results_*.json
python tools/ai_result_analyzer.py --directory output/log --pattern "ai_test_*_20250818_*.json"
"""

import os
import sys
import json
import glob
import argparse
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from typing import List, Dict, Any
import seaborn as sns

# ãƒ‘ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ï¼ˆCWDéä¾å­˜ï¼‰
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from src.config.common import get_dynamic_file_path

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS', 'Hiragino Sans']

class AIResultAnalyzer:
    """AIæ€§èƒ½ãƒ†ã‚¹ãƒˆçµæœåˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.results_data = []
        self.summary_stats = {}
        
    def load_results(self, file_patterns: List[str]) -> int:
        """çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        loaded_count = 0
        
        for pattern in file_patterns:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã§ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
            files = glob.glob(pattern)
            
            for file_path in files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        
                    # ãƒ‡ãƒ¼ã‚¿ãŒé…åˆ—å½¢å¼ã®å ´åˆ
                    if isinstance(data, list):
                        for item in data:
                            item['source_file'] = os.path.basename(file_path)
                            self.results_data.append(item)
                    # ãƒ‡ãƒ¼ã‚¿ãŒå˜ä¸€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
                    else:
                        data['source_file'] = os.path.basename(file_path)
                        self.results_data.append(data)
                    
                    loaded_count += 1
                    print(f"âœ… èª­ã¿è¾¼ã¿å®Œäº†: {file_path}")
                    
                except Exception as e:
                    print(f"âŒ èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {file_path} - {e}")
        
        print(f"ğŸ“Š ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(self.results_data)} ä»¶ ({loaded_count} ãƒ•ã‚¡ã‚¤ãƒ«)")
        return loaded_count
    
    def analyze_performance(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã‚’å®Ÿè¡Œ"""
        if not self.results_data:
            print("âŒ åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return {}
        
        # æˆåŠŸã—ãŸãƒ†ã‚¹ãƒˆã®ã¿åˆ†æ
        successful_tests = [r for r in self.results_data if r.get("success", False)]
        
        if not successful_tests:
            print("âŒ æˆåŠŸã—ãŸãƒ†ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            return {}
        
        # DataFrameã«å¤‰æ›
        df = pd.DataFrame(successful_tests)
        
        # åŸºæœ¬çµ±è¨ˆã®è¨ˆç®—
        analysis = {
            "total_tests": len(self.results_data),
            "successful_tests": len(successful_tests),
            "success_rate": len(successful_tests) / len(self.results_data) * 100,
            "models": df['model'].unique().tolist(),
            "response_time_stats": {
                "mean": df['response_time'].mean(),
                "median": df['response_time'].median(),
                "std": df['response_time'].std(),
                "min": df['response_time'].min(),
                "max": df['response_time'].max()
            }
        }
        
        # ãƒ¢ãƒ‡ãƒ«åˆ¥çµ±è¨ˆ
        model_stats = {}
        for model in analysis["models"]:
            model_data = df[df['model'] == model]
            model_stats[model] = {
                "count": len(model_data),
                "avg_response_time": model_data['response_time'].mean(),
                "avg_tokens": model_data['tokens_used'].mean() if 'tokens_used' in model_data.columns else 0,
                "success_rate": len(model_data) / len([r for r in self.results_data if r.get('model') == model]) * 100
            }
        
        analysis["model_stats"] = model_stats
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·åˆ¥çµ±è¨ˆ
        if 'prompt_length' in df.columns:
            prompt_stats = {}
            for length in df['prompt_length'].unique():
                length_data = df[df['prompt_length'] == length]
                prompt_stats[int(length)] = {
                    "count": len(length_data),
                    "avg_response_time": length_data['response_time'].mean(),
                    "models_tested": length_data['model'].unique().tolist()
                }
            analysis["prompt_length_stats"] = prompt_stats
        
        self.summary_stats = analysis
        return analysis
    
    def generate_visualizations(self, output_dir: str = "output/log"):
        """å¯è¦–åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if not self.results_data:
            print("âŒ å¯è¦–åŒ–å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(output_dir, exist_ok=True)
        
        # æˆåŠŸã—ãŸãƒ†ã‚¹ãƒˆã®ã¿ä½¿ç”¨
        successful_tests = [r for r in self.results_data if r.get("success", False)]
        df = pd.DataFrame(successful_tests)
        
        if df.empty:
            print("âŒ å¯è¦–åŒ–å¯¾è±¡ã®æˆåŠŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
        # 1. ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®åˆ†å¸ƒ
        plt.figure(figsize=(12, 6))
        plt.subplot(1, 2, 1)
        plt.hist(df['response_time'], bins=20, alpha=0.7, edgecolor='black')
        plt.title('Response Time Distribution')
        plt.xlabel('Response Time (seconds)')
        plt.ylabel('Frequency')
        plt.grid(True, alpha=0.3)
        
        # 2. ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“æ¯”è¼ƒ
        plt.subplot(1, 2, 2)
        if 'model' in df.columns:
            sns.boxplot(data=df, x='model', y='response_time')
            plt.title('Response Time by Model')
            plt.xlabel('Model')
            plt.ylabel('Response Time (seconds)')
            plt.xticks(rotation=45)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'ai_performance_overview.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®é–¢ä¿‚
        if 'prompt_length' in df.columns:
            plt.figure(figsize=(10, 6))
            for model in df['model'].unique():
                model_data = df[df['model'] == model]
                plt.scatter(model_data['prompt_length'], model_data['response_time'], 
                          label=model, alpha=0.7, s=50)
            
            plt.xlabel('Prompt Length (characters)')
            plt.ylabel('Response Time (seconds)')
            plt.title('Response Time vs Prompt Length by Model')
            plt.legend()
            plt.grid(True, alpha=0.3)
            plt.savefig(os.path.join(output_dir, 'prompt_length_vs_response_time.png'), dpi=300, bbox_inches='tight')
            plt.close()
        
        # 4. æˆåŠŸç‡ã®æ¯”è¼ƒ
        success_rates = {}
        for model in df['model'].unique():
            total_tests = len([r for r in self.results_data if r.get('model') == model])
            successful_tests = len(df[df['model'] == model])
            success_rates[model] = successful_tests / total_tests * 100
        
        plt.figure(figsize=(10, 6))
        models = list(success_rates.keys())
        rates = list(success_rates.values())
        
        bars = plt.bar(models, rates, alpha=0.8, edgecolor='black')
        plt.title('Success Rate by Model')
        plt.xlabel('Model')
        plt.ylabel('Success Rate (%)')
        plt.ylim(0, 105)
        
        # æˆåŠŸç‡ã®å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for bar, rate in zip(bars, rates):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                    f'{rate:.1f}%', ha='center', va='bottom')
        
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3, axis='y')
        plt.savefig(os.path.join(output_dir, 'success_rate_by_model.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ å¯è¦–åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {output_dir}")
    
    def generate_report(self, output_file: str = None) -> str:
        """è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if not self.summary_stats:
            print("âŒ åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«analyze_performance()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            return ""
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report = f"""
# AIæ€§èƒ½ãƒ†ã‚¹ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {timestamp}  
**åˆ†æå¯¾è±¡**: {self.summary_stats['total_tests']} ä»¶ã®ãƒ†ã‚¹ãƒˆçµæœ

## ğŸ“Š å…¨ä½“ã‚µãƒãƒª

- **ç·ãƒ†ã‚¹ãƒˆæ•°**: {self.summary_stats['total_tests']}
- **æˆåŠŸãƒ†ã‚¹ãƒˆæ•°**: {self.summary_stats['successful_tests']}
- **æˆåŠŸç‡**: {self.summary_stats['success_rate']:.1f}%
- **ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ¢ãƒ‡ãƒ«**: {', '.join(self.summary_stats['models'])}

## â±ï¸ ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“çµ±è¨ˆ

- **å¹³å‡**: {self.summary_stats['response_time_stats']['mean']:.2f}ç§’
- **ä¸­å¤®å€¤**: {self.summary_stats['response_time_stats']['median']:.2f}ç§’
- **æ¨™æº–åå·®**: {self.summary_stats['response_time_stats']['std']:.2f}ç§’
- **æœ€çŸ­**: {self.summary_stats['response_time_stats']['min']:.2f}ç§’
- **æœ€é•·**: {self.summary_stats['response_time_stats']['max']:.2f}ç§’

## ğŸ¤– ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

"""
        
        for model, stats in self.summary_stats['model_stats'].items():
            report += f"""
### {model}
- **ãƒ†ã‚¹ãƒˆæ•°**: {stats['count']}å›
- **æˆåŠŸç‡**: {stats['success_rate']:.1f}%
- **å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“**: {stats['avg_response_time']:.2f}ç§’
- **å¹³å‡ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡**: {stats['avg_tokens']:.0f}
"""
        
        if 'prompt_length_stats' in self.summary_stats:
            report += "\n## ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·åˆ¥çµ±è¨ˆ\n"
            
            for length, stats in sorted(self.summary_stats['prompt_length_stats'].items()):
                report += f"""
### {length}æ–‡å­—
- **ãƒ†ã‚¹ãƒˆæ•°**: {stats['count']}å›
- **å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“**: {stats['avg_response_time']:.2f}ç§’
- **ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ¢ãƒ‡ãƒ«**: {', '.join(stats['models_tested'])}
"""
        
        report += f"""

## ğŸ“ˆ å¯è¦–åŒ–ãƒ¬ãƒãƒ¼ãƒˆ

ä»¥ä¸‹ã®ã‚°ãƒ©ãƒ•ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ:
- `ai_performance_overview.png`: ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“åˆ†å¸ƒã¨ãƒ¢ãƒ‡ãƒ«åˆ¥æ¯”è¼ƒ
- `prompt_length_vs_response_time.png`: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®é–¢ä¿‚
- `success_rate_by_model.png`: ãƒ¢ãƒ‡ãƒ«åˆ¥æˆåŠŸç‡

## ğŸ” æ¨å¥¨äº‹é …

"""
        
        # æ¨å¥¨äº‹é …ã®è‡ªå‹•ç”Ÿæˆ
        best_model = min(self.summary_stats['model_stats'].items(), 
                        key=lambda x: x[1]['avg_response_time'])
        fastest_model = best_model[0]
        fastest_time = best_model[1]['avg_response_time']
        
        report += f"- **æœ€é«˜é€Ÿãƒ¢ãƒ‡ãƒ«**: {fastest_model} (å¹³å‡{fastest_time:.2f}ç§’)\n"
        
        if self.summary_stats['success_rate'] < 95:
            report += "- **æˆåŠŸç‡æ”¹å–„**: å…¨ä½“çš„ãªæˆåŠŸç‡ãŒ95%ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™ã€‚APIã‚­ãƒ¼ã¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n"
        
        if self.summary_stats['response_time_stats']['max'] > 30:
            report += "- **ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š**: æœ€å¤§ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãŒ30ç§’ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã®è¦‹ç›´ã—ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚\n"
        
        report += "\n---\n*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ai_result_analyzer.pyã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚*\n"
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        if not output_file:
            timestamp_file = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"ai_performance_report_{timestamp_file}.md"
        
        output_path = get_dynamic_file_path(f"output/log/{output_file}")
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“„ åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {output_path}")
        return report

def main():
    parser = argparse.ArgumentParser(description="AIæ€§èƒ½ãƒ†ã‚¹ãƒˆçµæœåˆ†æãƒ„ãƒ¼ãƒ«")
    parser.add_argument("files", nargs="*", help="åˆ†æå¯¾è±¡ã®JSONãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--directory", "-d", type=str, help="çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--pattern", "-p", type=str, default="ai_test_*.json", 
                       help="ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ (default: ai_test_*.json)")
    parser.add_argument("--output", "-o", type=str, help="ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å")
    parser.add_argument("--no-viz", action="store_true", help="å¯è¦–åŒ–ã‚’ç„¡åŠ¹åŒ–")
    
    args = parser.parse_args()
    
    # åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®æ±ºå®š
    file_patterns = []
    
    if args.files:
        file_patterns = args.files
    elif args.directory:
        pattern_path = os.path.join(args.directory, args.pattern)
        file_patterns = [pattern_path]
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: output/logãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ai_test_*.json
        default_pattern = os.path.join("output", "log", args.pattern)
        file_patterns = [default_pattern]
    
    # åˆ†æå®Ÿè¡Œ
    analyzer = AIResultAnalyzer()
    
    print("ğŸ” çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    loaded_count = analyzer.load_results(file_patterns)
    
    if loaded_count == 0:
        print("âŒ èª­ã¿è¾¼ã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return 1
    
    print("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã‚’å®Ÿè¡Œä¸­...")
    analysis = analyzer.analyze_performance()
    
    if not analysis:
        print("âŒ åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return 1
    
    # å¯è¦–åŒ–
    if not args.no_viz:
        print("ğŸ“ˆ å¯è¦–åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
        analyzer.generate_visualizations()
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("ğŸ“„ åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
    analyzer.generate_report(args.output)
    
    print("âœ… åˆ†æå®Œäº†!")
    return 0

if __name__ == "__main__":
    exit(main())
