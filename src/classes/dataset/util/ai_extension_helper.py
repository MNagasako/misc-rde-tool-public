"""
AIæ‹¡å¼µè¨­å®šç®¡ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
AIæ‹¡å¼µæ©Ÿèƒ½ã®ãƒœã‚¿ãƒ³è¨­å®šã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ç®¡ç†ã‚’è¡Œã†
"""

import os
import json
import re
from dataclasses import dataclass
from functools import lru_cache
from typing import Dict, Iterable, List, Optional, Tuple
from config.common import get_dynamic_file_path

import logging

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logger = logging.getLogger(__name__)


def infer_ai_suggest_target_kind(button_config: Dict) -> str:
        """Infer target kind for an AI suggest button.

        Backward compatible default:
            - JSONé‡è¦æŠ€è¡“é ˜åŸŸ3 (id: json_suggest_important_tech_area3) is for report
            - everything else is for dataset (AIæ‹¡å¼µ/å¾“æ¥)

        Returns:
            "dataset" or "report"
        """
        if not isinstance(button_config, dict):
                return "dataset"
        raw = (button_config.get('target_kind') or '').strip().lower()
        if raw in {"dataset", "report"}:
                return raw

        button_id = (button_config.get('id') or '').strip()
        label = (button_config.get('label') or '').strip()
        if button_id == 'json_suggest_important_tech_area3' or label == 'JSONé‡è¦æŠ€è¡“é ˜åŸŸ3':
                return "report"
        return "dataset"


@dataclass(frozen=True)
class TemplatePlaceholder:
    name: str
    description: str
    example: str = ""
    source: str = ""


def _safe_str(value) -> str:
    if value is None:
        return ""
    try:
        return str(value)
    except Exception:
        return ""


def _sanitize_placeholder_key(text: str) -> str:
    """Convert arbitrary column/header text into a safe placeholder key.

    Note: Japanese headers may become empty; in that case we simply skip alias.
    """
    if text is None:
        return ""
    s = _safe_str(text).strip().lower()
    if not s:
        return ""
    s = re.sub(r"[^a-z0-9]+", "_", s)
    s = re.sub(r"_+", "_", s).strip("_")
    if not s:
        return ""
    if s[0].isdigit():
        s = f"col_{s}"
    return s


def _read_excel_headers(file_path: str) -> List[str]:
    """Read header row (column names) from an Excel file."""
    if not file_path or not os.path.exists(file_path):
        return []
    # Prefer pandas if available (fast for headers), else fallback to openpyxl.
    try:
        import pandas as pd  # type: ignore

        df = pd.read_excel(file_path, nrows=0)
        cols = [c for c in df.columns if c is not None]
        return [str(c) for c in cols]
    except Exception:
        pass

    try:
        from openpyxl import load_workbook  # type: ignore

        wb = load_workbook(file_path, read_only=True, data_only=True)
        ws = wb.active
        row = next(ws.iter_rows(min_row=1, max_row=1, values_only=True), None)
        if not row:
            return []
        headers = [h for h in row if h is not None]
        return [str(h) for h in headers]
    except Exception:
        return []


@lru_cache(maxsize=32)
def _load_converted_xlsx_records_cached(file_path: str, mtime: float) -> List[Dict]:
    """Load converted.xlsx into list-of-dicts. Cached by path+mtime."""
    try:
        import pandas as pd  # type: ignore

        df = pd.read_excel(file_path)
        return df.to_dict('records')
    except Exception as e:
        logger.debug("converted.xlsx èª­ã¿è¾¼ã¿ã«å¤±æ•—: %s", e)
        return []


def _load_converted_xlsx_records(file_path: str) -> List[Dict]:
    if not file_path or not os.path.exists(file_path):
        return []
    try:
        mtime = os.path.getmtime(file_path)
    except Exception:
        mtime = 0.0
    return _load_converted_xlsx_records_cached(file_path, mtime)


def _find_matching_record_by_grant_number(grant_number: str, records: List[Dict]) -> Optional[Dict]:
    if not grant_number or not records:
        return None
    target = str(grant_number).strip()
    if not target:
        return None

    candidate_keys = [
        'èª²é¡Œç•ªå·', 'ARIMNO', 'ARIM ID', 'ARIMID', 'grant_number', 'GrantNumber',
    ]

    for key in candidate_keys:
        for rec in records:
            try:
                v = rec.get(key)
            except Exception:
                continue
            if v is None:
                continue
            if str(v).strip() == target:
                return rec
    return None


def load_converted_xlsx_report_entries() -> List[Dict]:
    """Load converted.xlsx as list-of-dicts for UI usage.

    This is intended for the AISuggestionDialog "å ±å‘Šæ›¸" tab.
    It returns raw records as read from Excel (column names preserved).
    """
    report_path = get_dynamic_file_path("output/arim-site/reports/converted.xlsx")
    if not os.path.exists(report_path):
        return []
    return _load_converted_xlsx_records(report_path)


def placeholders_from_converted_xlsx_record(record: Dict) -> Dict[str, str]:
    """Convert a converted.xlsx record (row dict) into placeholder dict.

    - Keeps raw column names: {<column>}
    - Adds ASCII-safe aliases: {converted_xlsx_<sanitized>}
    - Derives arim_report_* (fallback) and report_* aliases
    """
    if not record:
        return {}

    placeholders: Dict[str, str] = {}

    for col, value in record.items():
        if col is None:
            continue
        col_name = str(col)
        placeholders[col_name] = _safe_str(value)

        alias = _sanitize_placeholder_key(col_name)
        if alias:
            placeholders[f"converted_xlsx_{alias}"] = _safe_str(value)

    # converted.xlsx ç”±æ¥ã® arim_report_* ã‚’è£œå®Œ
    try:
        derived_arim = derive_arim_report_placeholders_from_converted(placeholders)
        for k, v in derived_arim.items():
            placeholders.setdefault(k, _safe_str(v))
        derived_report = derive_report_aliases_from_arim_report({**placeholders, **derived_arim})
        for k, v in derived_report.items():
            placeholders.setdefault(k, _safe_str(v))
    except Exception:
        pass

    return placeholders


def load_converted_xlsx_placeholders(grant_number: str) -> Dict[str, str]:
    """Load output-side converted.xlsx row data and expose each column as placeholders.

    - Raw column names are available as placeholders: {<column>}
    - Additionally, ASCII-safe aliases are added: {converted_xlsx_<sanitized>}

    Existing implementation is preserved; this is additive.
    """
    placeholders: Dict[str, str] = {}

    # User requested path
    report_path = get_dynamic_file_path("output/arim-site/reports/converted.xlsx")
    if not os.path.exists(report_path):
        return placeholders

    records = _load_converted_xlsx_records(report_path)
    if not records:
        return placeholders

    record = _find_matching_record_by_grant_number(grant_number, records)
    if record is None:
        # If only one record exists, treat it as the target.
        if len(records) == 1:
            record = records[0]
        else:
            return placeholders

    for col, value in record.items():
        if col is None:
            continue
        col_name = str(col)
        placeholders[col_name] = _safe_str(value)

        alias = _sanitize_placeholder_key(col_name)
        if alias:
            placeholders[f"converted_xlsx_{alias}"] = _safe_str(value)

    return placeholders


def _first_non_empty(values: Iterable[str]) -> str:
    for v in values:
        if v is None:
            continue
        s = _safe_str(v).strip()
        if s:
            return s
    return ""


def derive_arim_report_placeholders_from_converted(converted_placeholders: Dict[str, str]) -> Dict[str, str]:
    """converted.xlsx ã®åˆ—ï¼ˆ2ãƒã‚¤ãƒˆåˆ—å/æ¨™æº–åˆ—åæ··åœ¨ï¼‰ã‹ã‚‰ arim_report_* ã‚’å°å‡ºã™ã‚‹ã€‚

    ç›®çš„: ãƒãƒƒãƒˆå–å¾—(arim_report_fetcher)ãŒå¤±æ•—/æœªå®Ÿè¡Œã§ã‚‚ã€converted.xlsx ãŒã‚ã‚Œã°
    åŒã˜ arim_report_* ã‚­ãƒ¼ã§ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒå‹•ãã‚ˆã†ã«ã™ã‚‹ã€‚
    """
    if not converted_placeholders:
        return {}

    def pick(*candidates: str) -> str:
        return _first_non_empty(converted_placeholders.get(k, "") for k in candidates)

    derived: Dict[str, str] = {}

    derived['arim_report_project_number'] = pick(
        'èª²é¡Œç•ªå·',
        'èª²é¡Œç•ªå· / Project Issue Number',
        'ARIMNO',
    )
    derived['arim_report_title'] = pick(
        'åˆ©ç”¨èª²é¡Œå',
        'åˆ©ç”¨èª²é¡Œå / Title',
    )
    derived['arim_report_institute'] = pick(
        'åˆ©ç”¨ã—ãŸå®Ÿæ–½æ©Ÿé–¢',
        'åˆ©ç”¨ã—ãŸå®Ÿæ–½æ©Ÿé–¢ / Support Institute',
    )
    derived['arim_report_usage_type'] = pick(
        'æ©Ÿé–¢å¤–ãƒ»æ©Ÿé–¢å†…ã®åˆ©ç”¨',
        'æ©Ÿé–¢å¤–ãƒ»æ©Ÿé–¢å†…ã®åˆ©ç”¨ / External or Internal Use',
    )
    derived['arim_report_keywords'] = pick(
        'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰',
        'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ / Keywords',
    )

    # æŠ€è¡“é ˜åŸŸï¼ˆä¸»/å‰¯ï¼‰
    derived['arim_report_cross_tech_main'] = pick(
        'æ¨ªæ–­æŠ€è¡“é ˜åŸŸãƒ»ä¸»',
        'æ¨ªæ–­æŠ€è¡“é ˜åŸŸï¼ˆä¸»ï¼‰',
    )
    derived['arim_report_cross_tech_sub'] = pick(
        'æ¨ªæ–­æŠ€è¡“é ˜åŸŸãƒ»å‰¯',
        'æ¨ªæ–­æŠ€è¡“é ˜åŸŸï¼ˆå‰¯ï¼‰',
    )
    derived['arim_report_important_tech_main'] = pick(
        'é‡è¦æŠ€è¡“é ˜åŸŸãƒ»ä¸»',
        'é‡è¦æŠ€è¡“é ˜åŸŸï¼ˆä¸»ï¼‰',
    )
    derived['arim_report_important_tech_sub'] = pick(
        'é‡è¦æŠ€è¡“é ˜åŸŸãƒ»å‰¯',
        'é‡è¦æŠ€è¡“é ˜åŸŸï¼ˆå‰¯ï¼‰',
    )

    # ç©ºã¯è½ã¨ã™
    return {k: v for k, v in derived.items() if _safe_str(v).strip()}


def derive_report_aliases_from_arim_report(arim_report_data: Dict[str, str]) -> Dict[str, str]:
    """arim_report_* ã‹ã‚‰ report_*ï¼ˆæ¬§æ–‡åŒ–ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼‰ã‚’ä½œã‚‹ã€‚"""
    if not arim_report_data:
        return {}
    aliases: Dict[str, str] = {}
    mapping = {
        'arim_report_project_number': 'report_project_number',
        'arim_report_title': 'report_title',
        'arim_report_institute': 'report_institute',
        'arim_report_usage_type': 'report_usage_type',
        'arim_report_semiconductor': 'report_semiconductor',
        'arim_report_tech_area': 'report_tech_area',
        'arim_report_keywords': 'report_keywords',
        'arim_report_user_name': 'report_user_name',
        'arim_report_affiliation': 'report_affiliation',
        'arim_report_collaborators': 'report_collaborators',
        'arim_report_supporters': 'report_supporters',
        'arim_report_support_type': 'report_support_type',
        'arim_report_abstract': 'report_abstract',
        'arim_report_experimental': 'report_experimental',
        'arim_report_results': 'report_results',
        'arim_report_remarks': 'report_remarks',
        'arim_report_publications': 'report_publications',
        'arim_report_presentations': 'report_presentations',
        'arim_report_patents': 'report_patents',
        'arim_report_cross_tech_main': 'report_cross_tech_main',
        'arim_report_cross_tech_sub': 'report_cross_tech_sub',
        'arim_report_important_tech_main': 'report_important_tech_main',
        'arim_report_important_tech_sub': 'report_important_tech_sub',
    }
    for src, dst in mapping.items():
        v = arim_report_data.get(src)
        if v is None:
            continue
        sv = _safe_str(v).strip()
        if not sv:
            continue
        aliases[dst] = sv
    return aliases


def list_available_placeholders() -> List[TemplatePlaceholder]:
    """Return the full catalog of placeholders supported by prompt editing UI.

    This is intentionally conservative: it reflects placeholders actually implemented
    in code paths used by AIæ‹¡å¼µ (dataset suggestion / AI suggest).
    """

    placeholders: List[TemplatePlaceholder] = []

    def add(name: str, description: str, example: str = "", source: str = ""):
        placeholders.append(TemplatePlaceholder(name=name, description=description, example=example, source=source))

    # åŸºæœ¬ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰
    add("name", "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå", "ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ", "dataset")
    add("type", "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ï¼ˆdataset_type ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼‰", "experimental", "dataset")
    add("dataset_type", "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—", "experimental", "dataset")
    add("grant_number", "èª²é¡Œç•ªå·", "JPMXP1234567890", "dataset")
    add("description", "æ—¢å­˜ã®èª¬æ˜æ–‡ï¼ˆexisting_description ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼‰", "èª¬æ˜æ–‡...", "dataset")
    add("existing_description", "æ—¢å­˜ã®èª¬æ˜æ–‡", "èª¬æ˜æ–‡...", "dataset")

    # æ§‹é€ åŒ–ãƒ•ã‚¡ã‚¤ãƒ«/ãƒ•ã‚¡ã‚¤ãƒ«ãƒ„ãƒªãƒ¼
    add("file_tree", "ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆï¼ˆfile_info ã‚’ç°¡ç•¥åŒ–ã—ãŸã‚­ãƒ¼ï¼‰", "...", "dataset_context")
    add("text_from_structured_files", "STRUCTURED ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆ", "...", "dataset_context")
    add("json_from_structured_files", "STRUCTURED ãƒ•ã‚¡ã‚¤ãƒ«ã®JSONè¡¨ç¾", "{...}", "dataset_context")

    # ARIMèª²é¡Œãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«Excelç­‰ã‹ã‚‰ï¼‰
    add("dataset_existing_info", "ARIMèª²é¡Œãƒ‡ãƒ¼ã‚¿: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ—¢å­˜æƒ…å ±", "...", "arim_data_collector")
    add("arim_extension_data", "ARIMèª²é¡Œãƒ‡ãƒ¼ã‚¿: æ‹¡å¼µæƒ…å ±ï¼ˆconverted.xlsxç”±æ¥ / è¦ç´„æ¸ˆã¿ï¼‰", "...", "arim_data_collector")
    add("arim_experiment_data", "ARIMèª²é¡Œãƒ‡ãƒ¼ã‚¿: å®Ÿé¨“æƒ…å ±ï¼ˆarim_exp.xlsxç”±æ¥ï¼‰", "...", "arim_data_collector")
    add("arim_detailed_experiment", "ARIMèª²é¡Œãƒ‡ãƒ¼ã‚¿: æ‹¡å¼µå®Ÿé¨“æƒ…å ±ï¼ˆæ‹¡å¼µæƒ…å ±å†…ã®å®Ÿé¨“/çµæœã¨è€ƒå¯Ÿï¼‰", "...", "arim_data_collector")
    add("experiment_summary", "å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ãªã©ã®ã‚µãƒãƒªãƒ¼", "å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: 0ä»¶", "arim_data_collector")
    add("equipment_ids", "æŠ½å‡ºã•ã‚ŒãŸè¨­å‚™IDä¸€è¦§", "['TU-507']", "arim_data_collector")

    # MI/è£…ç½®
    add("material_index_data", "ãƒãƒ†ãƒªã‚¢ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆæ•´å½¢æ¸ˆã¿ï¼‰", "{...}", "ai_extension")
    add("equipment_data", "è£…ç½®æƒ…å ±ãƒ‡ãƒ¼ã‚¿ï¼ˆæ•´å½¢æ¸ˆã¿ï¼‰", "...", "ai_extension")
    add("static_material_index", "é™çš„ãƒãƒ†ãƒªã‚¢ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (input/ai/MI.json)", "{...JSON...}", "static")

    # ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¿ãƒ«ãƒã‚¹ã‚¿
    add("dataportal_material_index", "ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¿ãƒ«: ãƒãƒ†ãƒªã‚¢ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒã‚¹ã‚¿", "{...JSON...}", "dataportal_master")
    add("dataportal_tag", "ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¿ãƒ«: ã‚¿ã‚°ãƒã‚¹ã‚¿", "{...JSON...}", "dataportal_master")
    add("dataportal_equipment", "ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¿ãƒ«: è£…ç½®åˆ†é¡ãƒã‚¹ã‚¿", "{...JSON...}", "dataportal_master")

    # ARIMåˆ©ç”¨å ±å‘Šæ›¸ï¼ˆãƒãƒƒãƒˆå–å¾—ï¼‰
    # å®Ÿè£…ã¯ arim_report_fetcher.map_header_to_key ã«æº–æ‹ 
    add("arim_report_project_number", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: èª²é¡Œç•ªå·", "...", "arim_report")
    add("arim_report_title", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: åˆ©ç”¨èª²é¡Œå", "...", "arim_report")
    add("arim_report_institute", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: åˆ©ç”¨ã—ãŸå®Ÿæ–½æ©Ÿé–¢", "...", "arim_report")
    add("arim_report_usage_type", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: æ©Ÿé–¢å¤–ãƒ»æ©Ÿé–¢å†…ã®åˆ©ç”¨", "...", "arim_report")
    add("arim_report_semiconductor", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: ARIMåŠå°ä½“åŸºç›¤PFé–¢é€£èª²é¡Œ", "...", "arim_report")
    add("arim_report_tech_area", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: æŠ€è¡“é ˜åŸŸ", "...", "arim_report")
    add("arim_report_keywords", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "...", "arim_report")
    add("arim_report_cross_tech_main", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: æ¨ªæ–­æŠ€è¡“é ˜åŸŸï¼ˆä¸»ï¼‰", "...", "arim_report")
    add("arim_report_cross_tech_sub", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: æ¨ªæ–­æŠ€è¡“é ˜åŸŸï¼ˆå‰¯ï¼‰", "...", "arim_report")
    add("arim_report_important_tech_main", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: é‡è¦æŠ€è¡“é ˜åŸŸï¼ˆä¸»ï¼‰", "...", "arim_report")
    add("arim_report_important_tech_sub", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: é‡è¦æŠ€è¡“é ˜åŸŸï¼ˆå‰¯ï¼‰", "...", "arim_report")
    add("arim_report_user_name", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: åˆ©ç”¨è€…åï¼ˆèª²é¡Œç”³è«‹è€…ï¼‰", "...", "arim_report")
    add("arim_report_affiliation", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: æ‰€å±å", "...", "arim_report")
    add("arim_report_collaborators", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: å…±åŒåˆ©ç”¨è€…æ°å", "...", "arim_report")
    add("arim_report_supporters", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: ARIMå®Ÿæ–½æ©Ÿé–¢æ”¯æ´æ‹…å½“è€…", "...", "arim_report")
    add("arim_report_support_type", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: åˆ©ç”¨å½¢æ…‹", "...", "arim_report")
    add("arim_report_abstract", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: æ¦‚è¦ï¼ˆç›®çš„ãƒ»ç”¨é€”ãƒ»å®Ÿæ–½å†…å®¹ï¼‰", "...", "arim_report")
    add("arim_report_experimental", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: å®Ÿé¨“", "...", "arim_report")
    add("arim_report_results", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: çµæœã¨è€ƒå¯Ÿ", "...", "arim_report")
    add("arim_report_remarks", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: ãã®ä»–ãƒ»ç‰¹è¨˜äº‹é …ï¼ˆå‚è€ƒæ–‡çŒ®ãƒ»è¬è¾ç­‰ï¼‰", "...", "arim_report")
    add("arim_report_publications", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: è«–æ–‡ãƒ»ãƒ—ãƒ­ã‚·ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°", "...", "arim_report")
    add("arim_report_presentations", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: å£é ­ç™ºè¡¨/ãƒã‚¹ã‚¿ãƒ¼ç™ºè¡¨/ãã®ä»–", "...", "arim_report")
    add("arim_report_patents", "ARIMåˆ©ç”¨å ±å‘Šæ›¸: ç‰¹è¨±", "...", "arim_report")

    # arim_report_* ã®è‹±èªã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆæ¬§æ–‡åŒ–ï¼‰
    add("report_project_number", "Report: Project number (alias of arim_report_project_number)", "...", "alias")
    add("report_title", "Report: Title (alias of arim_report_title)", "...", "alias")
    add("report_institute", "Report: Support institute (alias of arim_report_institute)", "...", "alias")
    add("report_usage_type", "Report: External/Internal use (alias of arim_report_usage_type)", "...", "alias")
    add("report_semiconductor", "Report: Semiconductor PF (alias of arim_report_semiconductor)", "...", "alias")
    add("report_tech_area", "Report: Technology area (alias of arim_report_tech_area)", "...", "alias")
    add("report_keywords", "Report: Keywords (alias of arim_report_keywords)", "...", "alias")
    add("report_user_name", "Report: User name (alias of arim_report_user_name)", "...", "alias")
    add("report_affiliation", "Report: Affiliation (alias of arim_report_affiliation)", "...", "alias")
    add("report_collaborators", "Report: Collaborators (alias of arim_report_collaborators)", "...", "alias")
    add("report_supporters", "Report: Supporters (alias of arim_report_supporters)", "...", "alias")
    add("report_support_type", "Report: Support type (alias of arim_report_support_type)", "...", "alias")
    add("report_abstract", "Report: Abstract (alias of arim_report_abstract)", "...", "alias")
    add("report_experimental", "Report: Experimental (alias of arim_report_experimental)", "...", "alias")
    add("report_results", "Report: Results (alias of arim_report_results)", "...", "alias")
    add("report_remarks", "Report: Remarks (alias of arim_report_remarks)", "...", "alias")
    add("report_publications", "Report: Publications (alias of arim_report_publications)", "...", "alias")
    add("report_presentations", "Report: Presentations (alias of arim_report_presentations)", "...", "alias")
    add("report_patents", "Report: Patents (alias of arim_report_patents)", "...", "alias")
    add("report_cross_tech_main", "Report: Cross technology area main (alias)", "...", "alias")
    add("report_cross_tech_sub", "Report: Cross technology area sub (alias)", "...", "alias")
    add("report_important_tech_main", "Report: Important technology area main (alias)", "...", "alias")
    add("report_important_tech_sub", "Report: Important technology area sub (alias)", "...", "alias")

    # LLMè¨­å®š
    add("llm_provider", "LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å", "gemini", "ai_config")
    add("llm_model", "LLMãƒ¢ãƒ‡ãƒ«å", "gemini-2.0-flash", "ai_config")
    add("llm_model_name", "LLMè­˜åˆ¥å­ï¼ˆprovider:modelï¼‰", "gemini:gemini-2.0-flash", "ai_config")

    # converted.xlsxï¼ˆoutputå´ï¼‰åˆ—
    report_path = get_dynamic_file_path("output/arim-site/reports/converted.xlsx")
    headers = _read_excel_headers(report_path)
    for col in headers:
        # raw column placeholder
        add(col, f"converted.xlsx åˆ— '{col}'", "...", "converted.xlsx")
        alias = _sanitize_placeholder_key(col)
        if alias:
            add(f"converted_xlsx_{alias}", f"converted.xlsx åˆ— '{col}'ï¼ˆå®‰å…¨ã‚­ãƒ¼ï¼‰", "...", "converted.xlsx")

    return placeholders

def load_ai_extension_config():
    """AIæ‹¡å¼µè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        config_path = get_dynamic_file_path("input/ai/ai_ext_conf.json")
        
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                logger.info("AIæ‹¡å¼µè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: %s", config_path)

                # Backward compatible normalization: add target_kind in-memory
                try:
                    for btn in config.get('buttons', []) or []:
                        if isinstance(btn, dict):
                            btn.setdefault('target_kind', infer_ai_suggest_target_kind(btn))
                    for btn in config.get('default_buttons', []) or []:
                        if isinstance(btn, dict):
                            btn.setdefault('target_kind', infer_ai_suggest_target_kind(btn))
                except Exception:
                    pass
                return config
        else:
            logger.info("AIæ‹¡å¼µè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™: %s", config_path)
            return get_default_ai_extension_config()

    except Exception as e:
        logger.error("AIæ‹¡å¼µè¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: %s", e)
        logger.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™")
        return get_default_ai_extension_config()


def normalize_results_json_keys(keys) -> List[str]:
    """Normalize configured JSON display keys for results table.

    Accepts:
      - list/tuple of strings
      - a single string (comma/newline separated)

    Returns:
      - de-duplicated list (preserving order)
      - empty strings removed
    """
    if keys is None:
        return []

    items: List[str] = []
    if isinstance(keys, (list, tuple)):
        for v in keys:
            if v is None:
                continue
            s = str(v).strip()
            if s:
                items.append(s)
    else:
        s = str(keys)
        # split by newline or comma
        parts = re.split(r"[\n,]+", s)
        for p in parts:
            sp = str(p).strip()
            if sp:
                items.append(sp)

    # de-dup preserve order
    seen = set()
    out: List[str] = []
    for it in items:
        if it in seen:
            continue
        seen.add(it)
        out.append(it)
    return out


def save_ai_extension_config(config: Dict):
    """AIæ‹¡å¼µè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹"""
    try:
        config_path = get_dynamic_file_path("input/ai/ai_ext_conf.json")
        os.makedirs(os.path.dirname(config_path), exist_ok=True)
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        logger.info("AIæ‹¡å¼µè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: %s", config_path)
        return True
    except Exception as e:
        logger.error("AIæ‹¡å¼µè¨­å®šä¿å­˜ã‚¨ãƒ©ãƒ¼: %s", e)
        return False

def get_default_ai_extension_config():
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®AIæ‹¡å¼µè¨­å®šã‚’å–å¾—"""
    return {
        "version": "1.0.0",
        "description": "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆAIæ‹¡å¼µè¨­å®š",
        "buttons": [
            {
                "id": "default_analysis",
                "label": "ç·åˆåˆ†æ",
                "description": "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç·åˆçš„ãªåˆ†æã‚’å®Ÿè¡Œ",
                "prompt_template": "ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ç·åˆçš„ãªåˆ†æã‚’è¡Œã£ã¦ãã ã•ã„ã€‚\n\nãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå: {name}\nã‚¿ã‚¤ãƒ—: {type}\nèª²é¡Œç•ªå·: {grant_number}\næ—¢å­˜èª¬æ˜: {description}\n\nåˆ†æé …ç›®:\n1. æŠ€è¡“çš„ç‰¹å¾´\n2. å­¦è¡“çš„ä¾¡å€¤\n3. å¿œç”¨å¯èƒ½æ€§\n4. ãƒ‡ãƒ¼ã‚¿å“è³ª\n5. æ”¹å–„ææ¡ˆ\n\nå„é …ç›®ã«ã¤ã„ã¦è©³ã—ãåˆ†æã—ã€200æ–‡å­—ç¨‹åº¦ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚",
                "icon": "ğŸ“Š",
                "category": "ç·åˆ"
            }
        ],
        "default_buttons": [],
        "ui_settings": {
            "buttons_per_row": 3,
            "button_height": 60,
            "button_width": 140,
            "response_area_height": 400,
            "enable_categories": True,
            "show_icons": True
        }
    }

def load_prompt_file(prompt_file_path):
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        # çµ¶å¯¾ãƒ‘ã‚¹ã‹ãƒã‚§ãƒƒã‚¯
        if os.path.isabs(prompt_file_path):
            full_path = prompt_file_path
        else:
            # ç›¸å¯¾ãƒ‘ã‚¹ã¯å‹•çš„ãƒ‘ã‚¹ã¨ã—ã¦è§£æ±ºï¼ˆãƒã‚¤ãƒŠãƒªæ™‚ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨ï¼‰
            full_path = get_dynamic_file_path(prompt_file_path)
        
        logger.debug("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿è©¦è¡Œ: %s", full_path)
        
        if os.path.exists(full_path):
            with open(full_path, 'r', encoding='utf-8') as f:
                content = f.read()
                logger.info("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: %s", full_path)
                return content
        else:
            logger.warning("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: %s", full_path)
            return None
            
    except Exception as e:
        logger.error("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: %s", e)
        return None

def save_prompt_file(prompt_file_path, content):
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹"""
    try:
        full_path = get_dynamic_file_path(prompt_file_path)
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        os.makedirs(os.path.dirname(full_path), exist_ok=True)
        
        with open(full_path, 'w', encoding='utf-8') as f:
            f.write(content)
        return True
            
    except Exception as e:
        logger.error("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: %s", e)
        return False

def format_prompt_with_context(prompt_template, context_data):
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ç½®æ›ã™ã‚‹ï¼ˆARIMå ±å‘Šæ›¸å¯¾å¿œãƒ»ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¿ãƒ«ãƒã‚¹ã‚¿å¯¾å¿œï¼‰"""
    try:
        # åŸºæœ¬çš„ãªç½®æ›å‡¦ç†
        formatted_prompt = prompt_template
        
        # ARIMå ±å‘Šæ›¸ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ãƒ»çµ±åˆ
        enhanced_context = context_data.copy()
        # ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¨ä¸è¶³ã‚­ãƒ¼ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’äº‹å‰é©ç”¨
        try:
            if 'type' not in enhanced_context and 'dataset_type' in enhanced_context:
                enhanced_context['type'] = enhanced_context.get('dataset_type') or ''
            # description ã¨ existing_description ã®ç›¸äº’ã‚¨ã‚¤ãƒªã‚¢ã‚¹
            if 'existing_description' not in enhanced_context and 'description' in enhanced_context:
                enhanced_context['existing_description'] = enhanced_context.get('description') or ''
            if 'description' not in enhanced_context and 'existing_description' in enhanced_context:
                enhanced_context['description'] = enhanced_context.get('existing_description') or ''
            if 'llm_model_name' not in enhanced_context:
                provider = enhanced_context.get('llm_provider') or ''
                model = enhanced_context.get('llm_model') or ''
                # provider/model ãŒä¸¡æ–¹ç©ºã®å ´åˆã€AIManagerã‹ã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’å–å¾—
                if not provider and not model:
                    try:
                        from classes.ai.core.ai_manager import AIManager
                        ai_mgr = AIManager()
                        provider = ai_mgr.get_default_provider()
                        model = ai_mgr.get_default_model(provider)
                        logger.debug(f"llm_model_nameæœªè¨­å®šã®ãŸã‚ã€AIManagerã‹ã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå–å¾—: {provider}:{model}")
                    except Exception as e:
                        logger.debug(f"AIManagerè¨­å®šå–å¾—å¤±æ•—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨: {e}")
                        provider = 'gemini'
                        model = 'gemini-2.0-flash'
                enhanced_context['llm_model_name'] = f"{provider}:{model}".strip(':')
            # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã«ç©ºæ–‡å­—ã‚’å…¥ã‚Œã¦æœªç½®æ›ã‚’é˜²ã
            for k in ['material_index_data', 'equipment_data', 'file_tree', 'text_from_structured_files', 'json_from_structured_files']:
                if k not in enhanced_context:
                    enhanced_context[k] = ''
        except Exception as _alias_err:
            logger.debug("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç½®æ›ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹é©ç”¨ã§è­¦å‘Š: %s", _alias_err)
        grant_number = context_data.get('grant_number')
        offline_mode = os.environ.get('ARIM_FETCHER_OFFLINE', '').lower() in ('1', 'true', 'yes')
        
        if grant_number and grant_number != "æœªè¨­å®š" and not offline_mode:
            logger.debug("ARIMå ±å‘Šæ›¸ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹: %s", grant_number)
            try:
                from classes.dataset.util.arim_report_fetcher import fetch_arim_report_data
                arim_data = fetch_arim_report_data(grant_number)
                
                if arim_data:
                    enhanced_context.update(arim_data)
                    logger.info("ARIMå ±å‘Šæ›¸ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ: %sé …ç›®", len(arim_data))

                    # arim_report_* ã®è‹±èªã‚¨ã‚¤ãƒªã‚¢ã‚¹ report_* ã‚’è¿½åŠ 
                    try:
                        enhanced_context.update(derive_report_aliases_from_arim_report(arim_data))
                    except Exception as e:
                        logger.debug("report_* ã‚¨ã‚¤ãƒªã‚¢ã‚¹ç”Ÿæˆå¤±æ•—: %s", e)
                    
                    # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šå–å¾—ã—ãŸã‚­ãƒ¼ã‚’è¡¨ç¤º
                    for key in arim_data.keys():
                        logger.debug("ARIM ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼: %s", key)
                else:
                    logger.info("ARIMå ±å‘Šæ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: %s", grant_number)
            except Exception as e:
                logger.warning("ARIMå ±å‘Šæ›¸å–å¾—ã§ã‚¨ãƒ©ãƒ¼: %s", e)
                # ã‚¨ãƒ©ãƒ¼ãŒã‚ã£ã¦ã‚‚ãƒ™ãƒ¼ã‚¹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ç¶šè¡Œ
        elif offline_mode:
            logger.info("ARIMå ±å‘Šæ›¸å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼ˆARIM_FETCHER_OFFLINE ãƒ¢ãƒ¼ãƒ‰ï¼‰")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¿ãƒ«ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ãƒ»çµ±åˆ
        try:
            master_data = load_dataportal_master_data()
            if master_data:
                enhanced_context.update(master_data)
                logger.debug("ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¿ãƒ«ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ: %sé …ç›®", len(master_data))
        except Exception as e:
            logger.warning("ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¿ãƒ«ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ã§ã‚¨ãƒ©ãƒ¼: %s", e)

        # é™çš„ãƒãƒ†ãƒªã‚¢ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆMI.jsonï¼‰ã‚’å–å¾—ãƒ»çµ±åˆ
        try:
            static_mi = load_static_material_index()
            if static_mi:
                enhanced_context.update(static_mi)
                logger.debug("é™çš„ãƒãƒ†ãƒªã‚¢ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’çµ±åˆ")
        except Exception as e:
            logger.warning("é™çš„ãƒãƒ†ãƒªã‚¢ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å–å¾—ã§ã‚¨ãƒ©ãƒ¼: %s", e)

        # output/arim-site/reports/converted.xlsx ã®åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ãƒ»çµ±åˆï¼ˆåˆ—â†’ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€æ‹¡å¼µï¼‰
        try:
            if grant_number and grant_number != "æœªè¨­å®š":
                converted_placeholders = load_converted_xlsx_placeholders(str(grant_number))
                if converted_placeholders:
                    enhanced_context.update(converted_placeholders)
                    logger.debug("converted.xlsx åˆ—ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã‚’çµ±åˆ: %sé …ç›®", len(converted_placeholders))

                    # converted.xlsx ã‹ã‚‰ arim_report_* ã‚’ä¸è¶³è£œå®Œï¼ˆãƒãƒƒãƒˆå–å¾—ã‚ˆã‚Šå„ªå…ˆã—ãªã„ï¼‰
                    try:
                        derived_report = derive_arim_report_placeholders_from_converted(converted_placeholders)
                        for k, v in derived_report.items():
                            if not _safe_str(enhanced_context.get(k, '')).strip():
                                enhanced_context[k] = v
                        # è£œå®Œå¾Œã« report_* ã®è‹±èªã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚‚ä½œã‚‹
                        enhanced_context.update(
                            {
                                k: v
                                for k, v in derive_report_aliases_from_arim_report(enhanced_context).items()
                                if not _safe_str(enhanced_context.get(k, '')).strip()
                            }
                        )
                    except Exception as e:
                        logger.debug("converted.xlsx ç”±æ¥ arim_report_* è£œå®Œå¤±æ•—: %s", e)
        except Exception as e:
            logger.warning("converted.xlsx ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€çµ±åˆã§ã‚¨ãƒ©ãƒ¼: %s", e)
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¼ã¨å€¤ã§ç½®æ›
        for key, value in enhanced_context.items():
            placeholder = f"{{{key}}}"
            if placeholder in formatted_prompt:
                # å€¤ãŒNoneã¾ãŸã¯ç©ºã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
                replacement_value = str(value) if value is not None else "æœªè¨­å®š"
                formatted_prompt = formatted_prompt.replace(placeholder, replacement_value)
        
        return formatted_prompt
        
    except Exception as e:
        logger.error("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç½®æ›ã‚¨ãƒ©ãƒ¼: %s", e)
        return prompt_template


def load_dataportal_master_data():
    """ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¿ãƒ«ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
    
    Returns:
        dict: ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ç”¨ã®ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿è¾æ›¸
            - dataportal_material_index: ãƒãƒ†ãƒªã‚¢ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒã‚¹ã‚¿ï¼ˆJSONæ–‡å­—åˆ—ï¼‰
            - dataportal_tag: ã‚¿ã‚°ãƒã‚¹ã‚¿ï¼ˆJSONæ–‡å­—åˆ—ï¼‰
            - dataportal_equipment: è£…ç½®åˆ†é¡ãƒã‚¹ã‚¿ï¼ˆJSONæ–‡å­—åˆ—ï¼‰
    """
    result = {}
    
    # ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å®šç¾©ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    master_types = [
        ('dataportal_material_index', 'material_index'),
        ('dataportal_tag', 'tag'),
        ('dataportal_equipment', 'equipment')
    ]
    
    for placeholder_key, file_prefix in master_types:
        try:
            # productionå„ªå…ˆã€ãªã‘ã‚Œã°testã‚’ä½¿ç”¨
            production_path = get_dynamic_file_path(f'input/master_data/{file_prefix}_production.json')
            test_path = get_dynamic_file_path(f'input/master_data/{file_prefix}_test.json')
            
            target_path = None
            if os.path.exists(production_path):
                target_path = production_path
                logger.debug("ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆproductionï¼‰: %s", file_prefix)
            elif os.path.exists(test_path):
                target_path = test_path
                logger.debug("ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆtestï¼‰: %s", file_prefix)
            else:
                logger.warning("ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: %s", file_prefix)
                result[placeholder_key] = "ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ãªã—"
                continue
            
            # JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            with open(target_path, 'r', encoding='utf-8') as f:
                master_json = json.load(f)
            
            # JSONæ–‡å­—åˆ—ã¨ã—ã¦æ ¼ç´ï¼ˆæ•´å½¢ã—ã¦è¦‹ã‚„ã™ãï¼‰
            result[placeholder_key] = json.dumps(master_json, ensure_ascii=False, indent=2)
            logger.info("ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ: %s (ä»¶æ•°: %s)", file_prefix, master_json.get('count', 'N/A'))
            
        except Exception as e:
            logger.error("ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ (%s): %s", file_prefix, e)
            result[placeholder_key] = f"ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    return result


def load_static_material_index():
    """é™çš„ãƒãƒ†ãƒªã‚¢ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹(MI.json)ã‚’èª­ã¿è¾¼ã¿ã€ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€æä¾›

    Returns:
        dict: { 'static_material_index': '<JSONæ–‡å­—åˆ—>' }
    """
    try:
        mi_path = get_dynamic_file_path('input/ai/MI.json')
        if not os.path.exists(mi_path):
            logger.info("MI.jsonãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: %s", mi_path)
            # ãƒ†ã‚¹ãƒˆã®å®‰å®šæ€§ã®ãŸã‚ã€ç©ºé…åˆ—ã®JSONã‚’è¿”ã™
            return {'static_material_index': '[]'}

        with open(mi_path, 'r', encoding='utf-8') as f:
            mi_json = json.load(f)

        mi_str = json.dumps(mi_json, ensure_ascii=False, indent=2)
        logger.info("MI.jsonèª­ã¿è¾¼ã¿æˆåŠŸï¼ˆã‚«ãƒ†ã‚´ãƒªæ•°æ¨å®šï¼‰")
        return {'static_material_index': mi_str}

    except Exception as e:
        logger.error("MI.jsonèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: %s", e)
        # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ç©ºé…åˆ—ã®JSONã‚’è¿”ã™
        return {'static_material_index': '[]'}