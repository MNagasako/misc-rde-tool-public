import json
import os
from config.common import get_dynamic_file_path
import logging
from qt_compat.widgets import QMessageBox
from qt_compat.core import QMetaObject, Qt, Q_ARG, QTimer
from config.common import OUTPUT_DIR
from classes.utils.api_request_helper import api_request, fetch_binary, download_request  # refactored to use api_request_helper
from core.bearer_token_manager import BearerTokenManager

# ãƒ­ã‚°è¨­å®š
logger = logging.getLogger(__name__)

def safe_show_message(parent, title, message, message_type="warning"):
    """
    ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤ºï¼ˆPySide6ï¼‰
    - åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰ã®å‘¼ã³å‡ºã—ã¯ QTimer.singleShot(0, ...) ã§ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã«å§”è­²
    - è¦ªã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã® show_* ã‚¹ãƒ­ãƒƒãƒˆå‘¼ã³å‡ºã—ã«ä¾å­˜ã›ãšã€ç›´æ¥ QMessageBox ã‚’ä½¿ç”¨
    """
    if parent is None:
        return

    def _show():
        try:
            if message_type == "warning":
                QMessageBox.warning(parent, title, message)
            elif message_type == "critical":
                QMessageBox.critical(parent, title, message)
            elif message_type == "information":
                QMessageBox.information(parent, title, message)
            else:
                QMessageBox.information(parent, title, message)
        except Exception as e:
            logger.error(f"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒœãƒƒã‚¯ã‚¹è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"[{message_type.upper()}] {title}: {message}")

    try:
        from qt_compat.core import QThread
        if hasattr(parent, 'thread') and parent.thread() != QThread.currentThread():
            QTimer.singleShot(0, _show)
        else:
            _show()
    except Exception as e:
        logger.error(f"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒœãƒƒã‚¯ã‚¹è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"[{message_type.upper()}] {title}: {message}")

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã«ä½¿ç”¨ã§ããªã„æ–‡å­—ã‚’å…¨è§’è¨˜å·ã«ç½®æ›
def replace_invalid_path_chars(s):
    if not s:
        return ""
    # Windowsã®ç¦æ­¢æ–‡å­—: \ / : * ? " < > | 
    # ä»£è¡¨çš„ãªå…¨è§’è¨˜å·ã«ç½®æ›
    table = str.maketrans({
        '\\': 'ï¿¥',
        '/': 'ï¼',
        ':': 'ï¼š',
        '*': 'ï¼Š',
        '?': 'ï¼Ÿ',
        '"': 'â€',
        '<': 'ï¼œ',
        '>': 'ï¼',
        '|': 'ï½œ',
    })
    return s.translate(table)

def get_dataset_filetype_counts(dataset_obj: dict, bearer_token: str, file_filter_config: dict | None = None) -> dict:
    """æŒ‡å®šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® fileType ä»¶æ•°ã‚’ã€å®Ÿãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã¨åŒã˜ãƒ‘ã‚¹ã§é›†è¨ˆã™ã‚‹ã€‚
    - dataEntry/{dataset_id}.json ãŒç„¡ã‘ã‚Œã° API ã‹ã‚‰ include=files ã‚’å–å¾—
    - included ã® file.type='file' ã® attributes.fileType ã‚’é›†è¨ˆ
    - file_filter_config ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã¯ã€'file_types' ã«ã‚ˆã‚‹çµã‚Šè¾¼ã¿ã‚’é©ç”¨
    """
    try:
        if not isinstance(dataset_obj, dict):
            return {}
        dataset_id = dataset_obj.get('id', '')
        dataset_attributes = dataset_obj.get('attributes', {})
        grantNumber = dataset_attributes.get('grantNumber', '')

        # dataEntry ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æœ‰ç„¡ã‚’ç¢ºèªï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã«åˆã‚ã›ã‚‹ï¼‰
        entry_path = get_dynamic_file_path(f'output/rde/data/dataEntry/{dataset_id}.json')

        counts: dict = {}

        def _apply_filter(ftype: str) -> bool:
            if not file_filter_config:
                return True
            ft_list = file_filter_config.get('file_types') or []
            return (not ft_list) or (ftype in ft_list)

        # ãƒ­ãƒ¼ã‚«ãƒ« dataEntry ãŒãªã‘ã‚Œã° API ã‹ã‚‰å–å¾—
        url = f"https://rde-api.nims.go.jp/data?filter%5Bdataset.id%5D={dataset_id}&sort=-created&page%5Boffset%5D=0&page%5Blimit%5D=100&include=files"
        headers = {
            "Accept": "application/vnd.api+json",
            "Authorization": f"Bearer {bearer_token}",
            "Origin": "https://rde.nims.go.jp",
            "Referer": "https://rde.nims.go.jp/",
        }
        resp = api_request("GET", url, headers=headers, params=None)
        if not resp or resp.status_code != 200:
            return {}
        jd = resp.json()
        included = jd.get('included', [])
        for item in included:
            if item.get('type') == 'file':
                ftype = item.get('attributes', {}).get('fileType', 'UNKNOWN')
                if _apply_filter(ftype):
                    counts[ftype] = counts.get(ftype, 0) + 1
        return counts
    except Exception as e:
        logger.debug(f"get_dataset_filetype_counts error: {e}")
        return {}

def download_all_files_from_files_json(data_id, bearer_token=None, parent=None, progress_callback=None):
    """
    dataFiles/{data_id}.json ã® data é…åˆ—å†…ã®å„ãƒ•ã‚¡ã‚¤ãƒ«IDãƒ»fileNameã§å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ä¿å­˜
    ä¿å­˜å…ˆ: output/rde/data/dataFiles/{data_id}/{fileName}
    
    Args:
        data_id: ãƒ‡ãƒ¼ã‚¿ID
        bearer_token: èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆçœç•¥æ™‚ã¯BearerTokenManagerã‹ã‚‰å–å¾—ï¼‰
        parent: è¦ªã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ
        progress_callback: ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•° (current, total, message) -> None
    """
    from urllib.parse import unquote
    
    # Bearer Tokençµ±ä¸€ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã§å–å¾—
    if not bearer_token:
        bearer_token = BearerTokenManager.get_token_with_relogin_prompt(parent)
        if not bearer_token:
            logger.error("Bearer TokenãŒå–å¾—ã§ãã¾ã›ã‚“ã€‚ãƒ­ã‚°ã‚¤ãƒ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            safe_show_message(parent, "èªè¨¼ã‚¨ãƒ©ãƒ¼", "Bearer TokenãŒå–å¾—ã§ãã¾ã›ã‚“ã€‚ãƒ­ã‚°ã‚¤ãƒ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚", "critical")
            return False
    
    files_json_path = get_dynamic_file_path(f'output/rde/data/dataFiles/{data_id}.json')
    save_dir = get_dynamic_file_path(f'output/rde/data/dataFiles/{data_id}')
    
    if not os.path.exists(files_json_path):
        error_msg = f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {files_json_path}"
        logger.error(error_msg)
        if parent:
            safe_show_message(parent, "ã‚¨ãƒ©ãƒ¼", error_msg, "warning")
        return
        
    try:
        with open(files_json_path, encoding="utf-8") as f:
            obj = json.load(f)
    except (json.JSONDecodeError, IOError) as e:
        error_msg = f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {files_json_path}, {e}"
        logger.error(error_msg)
        if parent:
            safe_show_message(parent, "ã‚¨ãƒ©ãƒ¼", error_msg, "warning")
        return
    
    file_entries = obj.get("data", [])
    os.makedirs(save_dir, exist_ok=True)
    
    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJPG/PNGï¼‰ã®ã¿ã‚’æŠ½å‡ºã—ã€ãƒ™ãƒ¼ã‚¹åã§é‡è¤‡ã‚’æ’é™¤
    image_entries = []
    seen_basenames = set()
    skipped_count = 0
    for entry in file_entries:
        attrs = entry.get("attributes", {})
        fname = attrs.get("fileName")
        if fname:
            fext = os.path.splitext(fname)[1].lower()
            if fext in ['.jpg', '.jpeg', '.png']:
                # ãƒ™ãƒ¼ã‚¹åï¼ˆæ‹¡å¼µå­ã‚’é™¤ãï¼‰ã‚’å–å¾—
                basename = os.path.splitext(fname)[0]
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯: åŒã˜ãƒ™ãƒ¼ã‚¹åã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯1å›ã ã‘ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                if basename not in seen_basenames:
                    seen_basenames.add(basename)
                    image_entries.append(entry)
                    logger.debug(f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ç™»éŒ²: {fname} (basename: {basename})")
                else:
                    skipped_count += 1
                    logger.debug(f"é‡è¤‡ã‚¹ã‚­ãƒƒãƒ—: {fname} (basename: {basename} ã¯æ—¢ã«ç™»éŒ²æ¸ˆã¿)")
    
    total_images = len(image_entries)
    if skipped_count > 0:
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: data_id={data_id}, ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«æ•°={total_images}/{len(file_entries)} (é‡è¤‡{skipped_count}ä»¶é™¤å¤–)")
    else:
        logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: data_id={data_id}, ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«æ•°={total_images}/{len(file_entries)}")
    downloaded_count = 0
    
    for idx, file_entry in enumerate(image_entries):
        file_id = file_entry.get("id")
        attributes = file_entry.get("attributes", {})
        file_name = attributes.get("fileName")
        
        if not file_id or not file_name:
            logger.warning(f"ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’ã‚¹ã‚­ãƒƒãƒ—: file_id={file_id}, file_name={file_name}")
            continue
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯æ›´æ–°
        if progress_callback:
            progress_callback(idx + 1, total_images, file_name)
            
        url = f"https://rde-api.nims.go.jp/files/{file_id}"
        headers = {
            "Authorization": f"Bearer {bearer_token}",
            "Accept": "*/*",
            "Referer": "https://rde.nims.go.jp/",
            "Origin": "https://rde.nims.go.jp",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
        }
        
        try:
            resp = download_request(url, bearer_token=bearer_token, headers=headers, stream=True)  # use download_request for Response object
            if resp is None:
                logger.warning(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—: {url}")
                continue
            if resp.status_code == 422:
                # 422ã‚¨ãƒ©ãƒ¼ï¼ˆåˆ©ç”¨ä¸å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã¯è­¦å‘Šã¨ã—ã¦ãƒ­ã‚°ã®ã¿
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«åˆ©ç”¨ä¸å¯: {file_name} (HTTP 422)")
                continue
            if resp.status_code != 200:
                logger.warning(f"HTTP {resp.status_code}ã‚¨ãƒ©ãƒ¼: {file_name}")
                continue
            
            # Content-Dispositionãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰æ­£ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—
            from urllib.parse import unquote
            import re
            
            final_file_name = file_name
            cd = resp.headers.get('Content-Disposition')
            if cd:
                # filename*=UTF-8''... å½¢å¼ã‚’å„ªå…ˆ
                match_star = re.search(r"filename\*=(?:UTF-8'')?([^;\r\n]+)", cd)
                if match_star:
                    final_file_name = unquote(match_star.group(1).strip('"'))
                else:
                    # filename="..." å½¢å¼
                    match = re.search(r'filename="?([^";\r\n]+)"?', cd)
                    if match:
                        final_file_name = match.group(1)
            
            # æ‹¡å¼µå­ã®æ­£è¦åŒ–ï¼ˆ.jpegã‚’.jpgã«çµ±ä¸€ï¼‰
            name_without_ext, ext = os.path.splitext(final_file_name)
            if ext.lower() == '.jpeg':
                ext = '.jpg'
            final_file_name = name_without_ext + ext
                
            save_path = os.path.join(save_dir, final_file_name)
            with open(save_path, 'wb') as outf:
                for chunk in resp.iter_content(chunk_size=8192):
                    if chunk:
                        outf.write(chunk)
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {save_path}")
            downloaded_count += 1
            
        except Exception as e:
            error_msg = f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {url}, {e}"
            logger.error(error_msg)
            if parent:
                safe_show_message(parent, "ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—", error_msg, "critical")
    
    logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: data_id={data_id}, æˆåŠŸ={downloaded_count}/{total_images}")
    # DatasetUploadTabã‹ã‚‰ã®å‘¼ã³å‡ºã—ã®å ´åˆã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤ºã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç‹¬è‡ªã®å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ï¼‰
    if parent and parent.__class__.__name__ != 'DatasetUploadTab':
        safe_show_message(parent, "å®Œäº†", f"{data_id} ã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚ï¼ˆ{downloaded_count}/{total_images}ä»¶æˆåŠŸï¼‰", "information")

def download_file_for_data_id(data_id, bearer_token=None, save_dir_base=None, file_name=None, grantNumber=None, dataset_name=None, tile_name=None, tile_number=None, parent=None):
    """
    æŒ‡å®šdata_idã®ãƒ•ã‚¡ã‚¤ãƒ«æœ¬ä½“ã‚’APIã‹ã‚‰å–å¾—ã—ã€output/rde/data/dataFiles/{data_id}/ ã«ä¿å­˜
    
    Args:
        data_id: ãƒ‡ãƒ¼ã‚¿ID
        bearer_token: èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆçœç•¥æ™‚ã¯BearerTokenManagerã‹ã‚‰å–å¾—ï¼‰
        save_dir_base: ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ™ãƒ¼ã‚¹
        file_name: ãƒ•ã‚¡ã‚¤ãƒ«å
        grantNumber: ã‚°ãƒ©ãƒ³ãƒˆç•ªå·
        dataset_name: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
        tile_name: ã‚¿ã‚¤ãƒ«å
        tile_number: ã‚¿ã‚¤ãƒ«ç•ªå·
        parent: è¦ªã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ
    """
    import os
    
    # Bearer Tokençµ±ä¸€ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã§å–å¾—
    if not bearer_token:
        bearer_token = BearerTokenManager.get_token_with_relogin_prompt(parent)
        if not bearer_token:
            logger.error("Bearer TokenãŒå–å¾—ã§ãã¾ã›ã‚“ã€‚ãƒ­ã‚°ã‚¤ãƒ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            safe_show_message(parent, "èªè¨¼ã‚¨ãƒ©ãƒ¼", "Bearer TokenãŒå–å¾—ã§ãã¾ã›ã‚“ã€‚ãƒ­ã‚°ã‚¤ãƒ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚", "critical")
            return False
    from urllib.parse import unquote, urlparse


    safe_dataset_name = (replace_invalid_path_chars(dataset_name) or "unknown_dataset").strip()
    safe_tile_name = (replace_invalid_path_chars(tile_name) or "unknown_tile").strip()
    safe_grant_number = (str(grantNumber) if grantNumber is not None else "unknown_grant").strip()
    safe_tile_number = (str(tile_number) if tile_number is not None else "unknown_number").strip()
    # æ”¹è¡Œã‚„ç©ºç™½ã‚’é™¤å»ã—ã¦ã‹ã‚‰é€£çµ
    def safe_path_join(*args):
        return os.path.join(*(str(a).strip().replace('\n','').replace('\r','') for a in args if a is not None))
    try:
        save_dir = safe_path_join(save_dir_base, safe_grant_number, safe_dataset_name, f"{safe_tile_number}_{safe_tile_name}")
        os.makedirs(save_dir, exist_ok=True)
    except Exception as e:
        base_log_dir = save_dir_base
        log_path = os.path.join(base_log_dir, "download_error.log")
        with open(log_path, "a", encoding="utf-8") as logf:
            logf.write(f"[ERROR] makedirs failed for data_id={data_id}, grantNumber={grantNumber}, dataset_name={dataset_name}, tile_name={tile_name}, tile_number={tile_number}\nException: {e}\n")
        logger.error("makedirs failed: %s", e)
        return False
    url = f"https://rde-api.nims.go.jp/files/{data_id}?isDownload=true"
    headers = {
        "Authorization": f"Bearer {bearer_token}",
        "Accept": "*/*",
        "Referer": "https://rde.nims.go.jp/",
        "Origin": "https://rde.nims.go.jp",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
    }
    logger.info("Downloading file for data_id: %s from %s", data_id, url)
    try:
        resp = download_request(url, bearer_token=None, timeout=30, headers=headers, stream=True)  # download_request returns Response object
        logger.debug("%s -> HTTP %s ", url, resp.status_code if resp else 'No Response')
        if resp is None:
            logger.error("Request failed for data_id: %s", data_id)
            if parent:
                safe_show_message(parent, "ã‚¨ãƒ©ãƒ¼", f"data_id {data_id} ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚", "warning")
            return False
        if resp.status_code != 200:
            if parent:
                from qt_compat.widgets import QMessageBox
                logger.warning("data_id %s has no fileName in attributes, skipping download.", data_id)
            else:
                logger.error("%s: %s\n%s", resp.status_code, url, resp.text)
            return False
        # ãƒ•ã‚¡ã‚¤ãƒ«åæ±ºå®š: Content-Dispositionå„ªå…ˆã€ãªã‘ã‚Œã°URLæœ«å°¾
        fname = None
        cd = resp.headers.get('Content-Disposition')
        import re
        if cd:
            match_star = re.search(r"filename\*=((?:UTF-8'')?[^;\r\n]+)", cd)
            if match_star:
                fname = unquote(match_star.group(1).strip('"'))
            else:
                match = re.search(r'filename="?([^";\r\n]+)"?', cd)
                if match:
                    fname = match.group(1)
        if not fname:
            fname = os.path.basename(urlparse(url).path)
        fname = re.sub(r'[\\/:*?"<>|]', '_', fname)
        save_path = os.path.join(save_dir, file_name)
        with open(save_path, 'wb') as f:
            for chunk in resp.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        logger.info("Saved: %s", save_path)
        return save_path
    except Exception as e:
        import traceback
        traceback.print_exc()
        # å¤±æ•—æ™‚ã¯æœ€ã‚‚æµ…ã„ä¿å­˜ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’æ®‹ã™
        base_log_dir = save_dir_base
        log_path = os.path.join(base_log_dir, "download_error.log")
        with open(log_path, "a", encoding="utf-8") as logf:
            logf.write(f"[ERROR] download failed for data_id={data_id}, grantNumber={grantNumber}, dataset_name={dataset_name}, tile_name={tile_name}, tile_number={tile_number}\nException: {e}\n")
        if parent:
            safe_show_message(parent, "ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—", f"{url}\n{e}", "critical")
        else:
            logger.error("%s: %s", url, e)
        return False
# ä»»æ„ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰dataé…åˆ—å†…ã®idã‚’å…¨ã¦å–å¾—ã—è¡¨ç¤ºã™ã‚‹é–¢æ•°


def show_all_data_ids_from_json(json_path, parent=None):
    if not os.path.exists(json_path):
        msg = f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {json_path}"
        if parent:
            safe_show_message(parent, "ã‚¨ãƒ©ãƒ¼", msg, "warning")
        else:
            print(msg)
        return
    with open(json_path, encoding="utf-8") as f:
        obj = json.load(f)
    ids = [entry.get("id") for entry in obj.get("data", []) if "id" in entry]
    msg = "\n".join(ids) if ids else "IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    if parent:
        safe_show_message(parent, "IDä¸€è¦§", msg, "information")
    else:
        print(msg)


import shutil

def anonymize_json(data, grant_number):
        # attributeså†…ã®datasetTypeãŒANALYSISãªã‚‰ç‰¹åˆ¥å‡¦ç†
        if isinstance(data, dict):
            out = {}
            for k, v in data.items():
                kl = k.lower()
                # attributesç‰¹åˆ¥å‡¦ç†
                if k == "attributes" and isinstance(v, dict):
                    attrs = v.copy()
                    if attrs.get("datasetType") == "ANALYSIS":
                        attrs["grantNumber"] = "JPMXP12********"
                        attrs["subjectTitle"] = "*******éé–‹ç¤º*******"
                        attrs["name"] = "*******éé–‹ç¤º*******"
                    else:
                        for key, val in [("grantNumber", "JPMXP12********"), ("subjectTitle", "*******éé–‹ç¤º*******"), ("name", "*******éé–‹ç¤º*******")]:
                            if key in attrs:
                                attrs[key] = val
                    out[k] = attrs
                # grantNumber/grant_number/subjectTitle/nameã¯å†å¸°çš„ã«åŒ¿ååŒ–
                elif kl in ("grantnumber", "grant_number"):
                    out[k] = "***"
                elif kl == "subjecttitle":
                    out[k] = "*******éé–‹ç¤º*******"
                elif kl == "name":
                    out[k] = "*******éé–‹ç¤º*******"
                else:
                    out[k] = anonymize_json(v, grant_number)
            return out
        elif isinstance(data, list):
            return [anonymize_json(v, grant_number) for v in data]
        return data

def _process_data_entry_for_parallel(bearer_token, data_entry, save_dir_base, grantNumber, dataset_name, file_filter_config, parent=None, file_progress_callback=None):
    """
    ä¸¦åˆ—å‡¦ç†ç”¨ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°: å˜ä¸€data_entryã®files APIå‘¼ã³å‡ºã—ã¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    
    Args:
        bearer_token: èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³
        data_entry: ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        save_dir_base: ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ™ãƒ¼ã‚¹
        grantNumber: ã‚°ãƒ©ãƒ³ãƒˆç•ªå·
        dataset_name: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
        file_filter_config: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ•ã‚£ãƒ«ã‚¿è¨­å®š
        parent: è¦ªã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆï¼ˆã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”¨ï¼‰
        file_progress_callback: ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ã®ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ (file_name) -> None
        
    Returns:
        dict: {"status": "success"/"failed"/"skipped", "downloaded_count": int, "error": str}
    """
    try:
        data_id = data_entry.get('id')
        attributes = data_entry.get('attributes', {})
        tile_name = attributes.get("name", "")
        tile_number = attributes.get("dataNumber", "")
        
        if not data_id:
            logger.warning(f"ãƒ‡ãƒ¼ã‚¿IDãŒç©ºã®ã‚¨ãƒ³ãƒˆãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—: {data_entry}")
            return {"status": "skipped", "downloaded_count": 0}
        
        # 1. files API (json)ã‚’å¾“æ¥é€šã‚Šä¿å­˜
        files_dir = get_dynamic_file_path(f'output/rde/data/dataFiles/sub')
        os.makedirs(files_dir, exist_ok=True)
        
        files_url = (
            f"https://rde-api.nims.go.jp/data/{data_id}/files"
            "?page%5Blimit%5D=100"
            "&page%5Boffset%5D=0"
            "&filter%5BfileType%5D%5B%5D=META"
            "&filter%5BfileType%5D%5B%5D=MAIN_IMAGE"
            "&filter%5BfileType%5D%5B%5D=OTHER_IMAGE"
            "&filter%5BfileType%5D%5B%5D=NONSHARED_RAW"
            "&filter%5BfileType%5D%5B%5D=RAW"
            "&filter%5BfileType%5D%5B%5D=STRUCTURED"
            "&fileTypeOrder=RAW%2CNONSHARED_RAW%2CMETA%2CSTRUCTURED%2CMAIN_IMAGE%2COTHER_IMAGE"
        )
        headers = {
            "Accept": "application/vnd.api+json",
            "Accept-Encoding": "gzip, deflate, br, zstd",
            "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
            "Authorization": f"Bearer {bearer_token}",
            "Connection": "keep-alive",
            "Host": "rde-api.nims.go.jp",
            "Origin": "https://rde.nims.go.jp",
            "Referer": "https://rde.nims.go.jp/",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
            "sec-ch-ua": '"Not)A;Brand";v="8", "Chromium";v="138", "Google Chrome";v="138"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
        }
        
        resp = api_request("GET", files_url, headers=headers)  # Bearer Tokençµ±ä¸€ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œ
        
        if resp is None:
            logger.error(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•— (data_id: {data_id}): {files_url}")
            return {"status": "failed", "downloaded_count": 0, "error": "API request failed"}
            
        if resp.status_code == 404:
            logger.warning(f"ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (data_id: {data_id}): 404ã‚¨ãƒ©ãƒ¼")
            return {"status": "skipped", "downloaded_count": 0}
            
        if resp.status_code != 200:
            logger.error(f"HTTP {resp.status_code}ã‚¨ãƒ©ãƒ¼ (data_id: {data_id}): {files_url}")
            return {"status": "failed", "downloaded_count": 0, "error": f"HTTP {resp.status_code}"}
            
        resp.raise_for_status()
        files_data = resp.json()
        save_path = os.path.join(files_dir, f"{data_id}.json")
        
        with open(save_path, "w", encoding="utf-8") as outf:
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {save_path}")
            json.dump(files_data, outf, ensure_ascii=False, indent=2)
        
        # 2. ãƒ•ã‚¡ã‚¤ãƒ«æœ¬ä½“ã‚‚å–å¾—ã—ã¦ä¿å­˜
        save_dir_base_full = get_dynamic_file_path("output/rde/data/dataFiles")
        logger.debug(f"save_path: {save_path}, data_id: {data_id}")
        
        # files_dataã¯dictå‹ã®ã¯ãšãªã®ã§dataã‚­ãƒ¼ã‚’ç›´æ¥å‚ç…§
        data_entries_files = files_data.get("data", [])
        
        # ãƒ•ã‚£ãƒ«ã‚¿å‡¦ç†ã‚’é©ç”¨
        try:
            from classes.data_fetch2.util.file_filter_util import filter_file_list
            filtered_files = filter_file_list(data_entries_files, file_filter_config)
            logger.debug(f"ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨: {len(data_entries_files)}ä»¶ â†’ {len(filtered_files)}ä»¶")
        except ImportError:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®MAIN_IMAGEãƒ•ã‚£ãƒ«ã‚¿ã®ã¿
            logger.warning("ãƒ•ã‚£ãƒ«ã‚¿ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã¾ã›ã‚“ã€‚å¾“æ¥ãƒ•ã‚£ãƒ«ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            filtered_files = [entry for entry in data_entries_files 
                            if entry.get("attributes", {}).get("fileType") == "MAIN_IMAGE"]
        
        download_count = 0
        max_download = file_filter_config.get("max_download_count", 0)
        
        for dataentry in filtered_files:
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°ä¸Šé™ãƒã‚§ãƒƒã‚¯
            if max_download > 0 and download_count >= max_download:
                logger.info(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°ä¸Šé™ã«é”ã—ã¾ã—ãŸ: {max_download}ä»¶")
                break
                
            logger.debug(f"ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªå‡¦ç†ä¸­: {dataentry}")
            
            if not isinstance(dataentry, dict):
                logger.warning(f"è¾æ›¸å‹ã§ãªã„ã‚¨ãƒ³ãƒˆãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—: {dataentry}")
                continue
                
            if dataentry.get("type") != "file":
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«å‹ã§ãªã„ã‚¨ãƒ³ãƒˆãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—: {dataentry}")
                continue
                
            attributes_file = dataentry.get("attributes", {})
            fileType = attributes_file.get("fileType", "")
            logger.debug(f"fileType: {fileType} for data_id: {data_id}")
            
            # idã‚’å–å¾—
            entry_data_id = dataentry.get("id")
            if not entry_data_id:
                logger.warning(f"IDãŒç„¡ã„ã‚¨ãƒ³ãƒˆãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—: {dataentry}")
                continue
                
            # attributes.fileNameã‚’å‚ç…§
            file_name = attributes_file.get('fileName', '')
            if not file_name:
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«åãŒç„¡ã„ã‚¨ãƒ³ãƒˆãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—: data_id={entry_data_id}")
                continue
                
            # ãƒ•ã‚¡ã‚¤ãƒ«æœ¬ä½“ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            download_success = download_file_for_data_id(entry_data_id, bearer_token, save_dir_base_full, file_name, grantNumber, dataset_name, tile_name, tile_number, parent)
            if download_success:
                download_count += 1
                
            # ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ã®ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹é€šçŸ¥
            if file_progress_callback:
                file_progress_callback(file_name)
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªå‡¦ç†å®Œäº†: data_id={data_id}, ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {download_count}")
        return {"status": "success", "downloaded_count": download_count}
        
    except Exception as e:
        error_msg = f"ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ (data_id: {data_id}): {e}"
        logger.error(error_msg)
        import traceback
        traceback.print_exc()
        return {"status": "failed", "downloaded_count": 0, "error": str(e)}

def fetch_files_json_for_dataset(parent, dataset_obj, bearer_token=None, save_dir=None, progress_callback=None, file_filter_config=None):
    
    tile_name = "tile_name"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    tile_number = "tile_number"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    """
    æŒ‡å®šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDã®files_{id}.jsonã‚’APIçµŒç”±ã§å–å¾—ã—ä¿å­˜ã™ã‚‹
    ãƒ•ã‚¡ã‚¤ãƒ«ãƒ•ã‚£ãƒ«ã‚¿æ©Ÿèƒ½ä»˜ã
    
    Args:
        parent: è¦ªã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ
        dataset_obj: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        bearer_token: èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆçœç•¥æ™‚ã¯BearerTokenManagerã‹ã‚‰å–å¾—ï¼‰
        save_dir: ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        progress_callback: ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹é€šçŸ¥ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        file_filter_config: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ•ã‚£ãƒ«ã‚¿è¨­å®šè¾æ›¸
    """
    if not dataset_obj:
        error_msg = "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        logger.error(error_msg)
        if parent:
            safe_show_message(parent, "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæœªé¸æŠ", error_msg, "warning")
        return None
    
    # Bearer Tokençµ±ä¸€ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã§å–å¾—
    if not bearer_token:
        bearer_token = BearerTokenManager.get_token_with_relogin_prompt(parent)
        if not bearer_token:
            error_msg = "Bearer TokenãŒå–å¾—ã§ãã¾ã›ã‚“ã€‚ãƒ­ã‚°ã‚¤ãƒ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            logger.error(error_msg)
            if parent:
                safe_show_message(parent, "èªè¨¼ã‚¨ãƒ©ãƒ¼", error_msg, "warning")
            return None

    # ãƒ•ã‚£ãƒ«ã‚¿è¨­å®šã®åˆæœŸåŒ–
    if file_filter_config is None:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚£ãƒ«ã‚¿ï¼ˆå¾“æ¥é€šã‚ŠMAIN_IMAGEã®ã¿ï¼‰
        file_filter_config = {
            "file_types": ["MAIN_IMAGE"],
            "media_types": [],
            "extensions": [],
            "size_min": 0,
            "size_max": 0,
            "filename_pattern": "",
            "max_download_count": 0
        }

    try:
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆæœŸåŒ–
        if progress_callback:
            if not progress_callback(0, 1, "å‡¦ç†ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™..."):
                return None

        dataset_id = dataset_obj.get('id', '')
        dataset_attributes = dataset_obj.get('attributes', {})
        dataset_name = dataset_attributes.get('name', 'ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåæœªè¨­å®š')
        grantNumber= dataset_attributes.get('grantNumber', 'ä¸æ˜')
        safe_dataset_name = replace_invalid_path_chars(dataset_name)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDã®å½¢å¼æ¤œè¨¼
        import re
        uuid_pattern = r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$'
        if not re.match(uuid_pattern, dataset_id):
            error_msg = f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDã®å½¢å¼ãŒç„¡åŠ¹ã§ã™: {dataset_id}"
            logger.error(error_msg)
            if parent:
                safe_show_message(parent, "IDå½¢å¼ã‚¨ãƒ©ãƒ¼", error_msg, "warning")
            return None
        
        logger.info(f"å‡¦ç†å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: ID={dataset_id}, Name={dataset_name}, Grant={grantNumber}")
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
        if progress_callback:
            if not progress_callback(0, 1, "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’å‡¦ç†ä¸­..."):
                return None
        
        # dataset_obj ã‚’ã€€ä¿å­˜
        dataset_dir=get_dynamic_file_path(f"output/rde/data/dataFiles/{grantNumber}/{safe_dataset_name}")
        original_dataset_dir = get_dynamic_file_path("output/rde/data/datasets")
        os.makedirs(dataset_dir, exist_ok=True)
        dataset_json_path = os.path.join(dataset_dir, f"{dataset_id}.json") 
        original_dataset_json_path = os.path.join(original_dataset_dir, f"{dataset_id}.json")

        # original_dataset_jsonã‚’dataset_dirã«ã‚³ãƒ”ãƒ¼ ä¿å­˜
        if os.path.exists(original_dataset_json_path):
            shutil.copy2(original_dataset_json_path, dataset_json_path)

        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
        if progress_callback:
            if not progress_callback(0, 1, "åŒ¿ååŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆä¸­..."):
                return None

        # anonymize_json() ã®å®Ÿè£…ï¼ˆclasses.anonymizer ç›¸å½“ï¼‰ã‚’ä½¿ã„ã€original_dataset_json_path ã®åŒ¿ååŒ–ç‰ˆã‚’åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
        dataset_anonymized_path = os.path.join(dataset_dir, f"{dataset_id}_anonymized.json")
        if os.path.exists(original_dataset_json_path):
            with open(original_dataset_json_path, "r", encoding="utf-8") as f:
                dataset_obj = json.load(f)
            # åŒ¿ååŒ–å‡¦ç†ã‚’è¡Œã†é–¢æ•°ã‚’å‘¼ã³å‡ºã™
            anonymized_obj = anonymize_json(dataset_obj, grantNumber)
            with open(dataset_anonymized_path, "w", encoding="utf-8") as f:
                json.dump(anonymized_obj, f, ensure_ascii=False, indent=2)

        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
        if progress_callback:
            if not progress_callback(0, 1, "ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                return None

        # 1. dataEntry/{dataset_id}.jsonã‚’èª­ã‚€
        entry_path = get_dynamic_file_path(f'output/rde/data/dataEntry/{dataset_id}.json')
        if not os.path.exists(entry_path):
            logger.warning(f"{entry_path} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚APIã‹ã‚‰ç›´æ¥å–å¾—ã‚’è©¦è¡Œã—ã¾ã™ã€‚")
            
            # dataEntryãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã€APIã‹ã‚‰ç›´æ¥å–å¾—ã‚’è©¦è¡Œ
            try:
                logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ {dataset_id} ã®ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±ã‚’APIã‹ã‚‰å–å¾—ä¸­...")
                
                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°ï¼ˆAPIãƒªã‚¯ã‚¨ã‚¹ãƒˆå‰ï¼‰
                if progress_callback:
                    if not progress_callback(0, 1, f"APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªå–å¾—ä¸­..."):
                        return None
                
                # dataEntry APIã‚’å‘¼ã³å‡ºã—ï¼ˆæ­£ã—ã„ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå½¢å¼ï¼‰
                # URLã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ç›´æ¥URLã«å«ã‚ã‚‹æ–¹å¼ã‚’ä½¿ç”¨
                entry_url = f"https://rde-api.nims.go.jp/data?filter%5Bdataset.id%5D={dataset_id}&page%5Blimit%5D=100&page%5Boffset%5D=0"
                params = None  # URLã«ç›´æ¥å«ã‚ãŸãŸã‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä½¿ã‚ãªã„
                headers = {
                    "Accept": "application/vnd.api+json",
                    "Accept-Encoding": "gzip, deflate, br, zstd",
                    "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
                    "Authorization": f"Bearer {bearer_token}",
                    "Connection": "keep-alive",
                    "Host": "rde-api.nims.go.jp",
                    "Origin": "https://rde.nims.go.jp",
                    "Referer": "https://rde.nims.go.jp/",
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
                }
                
                logger.debug(f"API URL: {entry_url}")
                logger.debug(f"Headers: {headers}")
                
                logger.info(f"APIãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡: {entry_url}")
                
                resp = api_request("GET", entry_url, headers=headers, params=params)  # Bearer Tokençµ±ä¸€ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œ
                
                if resp is None:
                    error_msg = f"APIã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {dataset_id}"
                    logger.error(error_msg)
                    if parent:
                        safe_show_message(parent, "ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—", error_msg, "warning")
                    return None
                    
                if resp.status_code == 400:
                    # 400ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ãƒ­ã‚°
                    try:
                        error_detail = resp.json()
                        logger.error(f"API 400ã‚¨ãƒ©ãƒ¼è©³ç´°: {error_detail}")
                    except:
                        logger.error(f"API 400ã‚¨ãƒ©ãƒ¼: {resp.text}")
                    
                    error_msg = f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ {dataset_id} ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒç„¡åŠ¹ã§ã™ï¼ˆ400ã‚¨ãƒ©ãƒ¼ï¼‰\nè©³ç´°: APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¾ãŸã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ­£ã—ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™"
                    logger.error(error_msg)
                    if parent:
                        safe_show_message(parent, "ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼", error_msg, "warning")
                    return None
                    
                if resp.status_code == 404:
                    error_msg = f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ {dataset_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆ404ã‚¨ãƒ©ãƒ¼ï¼‰"
                    logger.error(error_msg)
                    if parent:
                        safe_show_message(parent, "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæœªç™ºè¦‹", error_msg, "warning")
                    return None
                    
                if resp.status_code == 403:
                    error_msg = f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ {dataset_id} ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸï¼ˆ403ã‚¨ãƒ©ãƒ¼ï¼‰\nèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
                    logger.error(error_msg)
                    if parent:
                        safe_show_message(parent, "ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦", error_msg, "warning")
                    return None
                    
                if resp.status_code != 200:
                    error_msg = f"APIã‚¨ãƒ©ãƒ¼ï¼ˆHTTP {resp.status_code}ï¼‰: {dataset_id}\nãƒ¬ã‚¹ãƒãƒ³ã‚¹: {resp.text[:200]}..."
                    logger.error(error_msg)
                    if parent:
                        safe_show_message(parent, "APIã‚¨ãƒ©ãƒ¼", error_msg, "warning")
                    return None
                
                entry_json = resp.json()
                
                # APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æ§‹é€ ç¢ºèª
                if "data" not in entry_json:
                    logger.warning(f"APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«'data'ã‚­ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“: {list(entry_json.keys())}")
                    if "errors" in entry_json:
                        logger.error(f"APIã‚¨ãƒ©ãƒ¼: {entry_json['errors']}")
                
                # å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆæ¬¡å›ä»¥é™ã®ãŸã‚ï¼‰
                os.makedirs(os.path.dirname(entry_path), exist_ok=True)
                with open(entry_path, "w", encoding="utf-8") as f:
                    json.dump(entry_json, f, ensure_ascii=False, indent=2)
                logger.info(f"å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªã‚’ä¿å­˜ã—ã¾ã—ãŸ: {entry_path}")
                
            except Exception as e:
                error_msg = f"ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
                logger.error(error_msg)
                import traceback
                logger.error(f"ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹: {traceback.format_exc()}")
                if parent:
                    safe_show_message(parent, "å–å¾—ã‚¨ãƒ©ãƒ¼", error_msg, "critical")
                return None
        else:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®å¾“æ¥å‡¦ç†
            try:
                with open(entry_path, encoding='utf-8') as f:
                    entry_json = json.load(f)
                logger.info(f"æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {entry_path}")
            except (json.JSONDecodeError, IOError) as e:
                error_msg = f"ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {entry_path}, {e}"
                logger.error(error_msg)
                if parent:
                    safe_show_message(parent, "ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼", error_msg, "critical")
                return None
        
        data_entries = entry_json.get('data', [])
        if not data_entries:
            warning_msg = f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ {dataset_id} ã«ã¯ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“"
            logger.warning(warning_msg)
            if parent:
                safe_show_message(parent, "ãƒ‡ãƒ¼ã‚¿ãªã—", warning_msg, "information")
            # ãƒ‡ãƒ¼ã‚¿ãªã—ã®å ´åˆã§ã‚‚å‡¦ç†ã‚’ç¶™ç¶šã™ã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚ã€ç‰¹åˆ¥ãªæˆ»ã‚Šå€¤ã‚’è¿”ã™
            return "no_data"

        # 2. å„dataã‚¨ãƒ³ãƒˆãƒªã®idã”ã¨ã«files APIã‚’å‘¼ã³ã€dataFiles/{id}.jsonã«ä¿å­˜ï¼ˆä¸¦åˆ—åŒ–å¯¾å¿œï¼‰
        # v2.1.1: ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾å¿œï¼ˆ50ä»¶ä»¥ä¸Šã§è‡ªå‹•ä¸¦åˆ—åŒ–ï¼‰
        # v2.1.3: ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºå¯¾å¿œ
        from net.http_helpers import parallel_download
        import threading
        
        files_dir = get_dynamic_file_path(f'output/rde/data/dataFiles/sub')
        os.makedirs(files_dir, exist_ok=True)
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ç®¡ç†å¤‰æ•°
        total_entries = len(data_entries)
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°ï¼ˆå‡¦ç†é–‹å§‹ï¼‰
        if progress_callback:
            if not progress_callback(0, 1, f"ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—é–‹å§‹... ({total_entries}ã‚¨ãƒ³ãƒˆãƒª)"):
                return None
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: å…¨ã‚¨ãƒ³ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—ã—ã¦ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ç®—å‡º
        logger.info("ã‚¹ãƒ†ãƒƒãƒ—1: å…¨ã‚¨ãƒ³ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±å–å¾—ä¸­...")
        total_files = 0
        entry_file_counts = {}  # {data_entry_id: ãƒ•ã‚¡ã‚¤ãƒ«æ•°}
        filetype_counts = {}
        
        for idx, entry in enumerate(data_entries):
            data_id = entry.get('id')
            if not data_id:
                continue
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆå–å¾—ä¸­ï¼‰
            if progress_callback:
                progress_pct = int((idx + 1) / total_entries * 100)
                if not progress_callback(progress_pct, 100, f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆå–å¾—ä¸­... ({idx+1}/{total_entries}ã‚¨ãƒ³ãƒˆãƒª)"):
                    return None
            
            # files APIå‘¼ã³å‡ºã—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®ã¿å–å¾—ï¼‰
            files_url = (
                f"https://rde-api.nims.go.jp/data/{data_id}/files"
                "?page%5Blimit%5D=100"
                "&page%5Boffset%5D=0"
                "&filter%5BfileType%5D%5B%5D=META"
                "&filter%5BfileType%5D%5B%5D=MAIN_IMAGE"
                "&filter%5BfileType%5D%5B%5D=OTHER_IMAGE"
                "&filter%5BfileType%5D%5B%5D=NONSHARED_RAW"
                "&filter%5BfileType%5D%5B%5D=RAW"
                "&filter%5BfileType%5D%5B%5D=STRUCTURED"
                "&fileTypeOrder=RAW%2CNONSHARED_RAW%2CMETA%2CSTRUCTURED%2CMAIN_IMAGE%2COTHER_IMAGE"
            )
            headers = {
                "Accept": "application/vnd.api+json",
                "Authorization": f"Bearer {bearer_token}",
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            }
            
            try:
                resp = api_request("GET", files_url, headers=headers)
                if resp and resp.status_code == 200:
                    files_data = resp.json()
                    data_entries_files = files_data.get("data", [])
                    
                    # ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
                    try:
                        from classes.data_fetch2.util.file_filter_util import filter_file_list
                        filtered_files = filter_file_list(data_entries_files, file_filter_config)
                    except ImportError:
                        filtered_files = [e for e in data_entries_files 
                                        if e.get("attributes", {}).get("fileType") == "MAIN_IMAGE"]
                    
                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°ä¸Šé™ãƒã‚§ãƒƒã‚¯
                    max_download = file_filter_config.get("max_download_count", 0)
                    file_count = len(filtered_files)
                    if max_download > 0 and file_count > max_download:
                        # ä¸Šé™ã«åˆã‚ã›ã¦å…ˆé ­ã‹ã‚‰æ¡ç”¨
                        filtered_files = filtered_files[:max_download]
                        file_count = len(filtered_files)
                    
                    # fileTypeåˆ¥ä»¶æ•°ã‚’é›†è¨ˆï¼ˆå®Ÿãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰äºˆå®šã«æº–æ‹ ï¼‰
                    for f in filtered_files:
                        ftype = f.get("attributes", {}).get("fileType", "UNKNOWN")
                        filetype_counts[ftype] = filetype_counts.get(ftype, 0) + 1

                    entry_file_counts[data_id] = file_count
                    total_files += file_count
                    logger.debug(f"ã‚¨ãƒ³ãƒˆãƒª {data_id}: {file_count}ãƒ•ã‚¡ã‚¤ãƒ«")
            except Exception as e:
                logger.warning(f"ã‚¨ãƒ³ãƒˆãƒª {data_id} ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆå–å¾—å¤±æ•—: {e}")
                entry_file_counts[data_id] = 0
        
        logger.info(f"ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}ãƒ•ã‚¡ã‚¤ãƒ« ({total_entries}ã‚¨ãƒ³ãƒˆãƒª)")
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
        logger.info("ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹...")
        
        # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        file_counter_lock = threading.Lock()
        downloaded_file_count = [0]  # ãƒªã‚¹ãƒˆã§ãƒ©ãƒƒãƒ—ã—ã¦ã‚¹ãƒ¬ãƒƒãƒ‰é–“å…±æœ‰
        
        def file_progress_callback(file_name):
            """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†æ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
            with file_counter_lock:
                downloaded_file_count[0] += 1
                current = downloaded_file_count[0]
            
            if progress_callback:
                progress_pct = int((current / max(total_files, 1)) * 100)
                progress_callback(progress_pct, 100, 
                                f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­... ({current}/{total_files}) - {file_name}")
        
        # ä¸¦åˆ—åŒ–ç”¨ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆä½œæˆ
        tasks = [
            (bearer_token, entry, os.path.join(OUTPUT_DIR, "rde", "data", "dataFiles"), 
             grantNumber, dataset_name, file_filter_config, parent, file_progress_callback)
            for entry in data_entries
        ]
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ©ãƒƒãƒ‘ãƒ¼ï¼ˆã‚¨ãƒ³ãƒˆãƒªå˜ä½ã¯ä½¿ç”¨ã—ãªã„ï¼‰
        def entry_progress_callback(current, total, message):
            """ã‚¨ãƒ³ãƒˆãƒªå‡¦ç†é€²æ—ï¼ˆãƒ€ãƒŸãƒ¼ - ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ã§æ›´æ–°ã™ã‚‹ãŸã‚ï¼‰"""
            return True
        
        # ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œï¼ˆ50ä»¶ä»¥ä¸Šã§è‡ªå‹•ä¸¦åˆ—åŒ–ã€æœ€å¤§10ä¸¦åˆ—ï¼‰
        result = parallel_download(
            tasks=tasks,
            worker_function=_process_data_entry_for_parallel,
            max_workers=10,
            progress_callback=entry_progress_callback,
            threshold=50  # 50ã‚¨ãƒ³ãƒˆãƒªä»¥ä¸Šã§ä¸¦åˆ—åŒ–
        )
        
        # çµæœã®é›†è¨ˆ
        success_count = result.get("success_count", 0)
        failed_count = result.get("failed_count", 0)
        skipped_count = result.get("skipped_count", 0)
        cancelled = result.get("cancelled", False)
        errors = result.get("errors", [])
        
        if cancelled:
            # ã‚­ãƒ£ãƒ³ã‚»ãƒ«æ™‚ã§ã‚‚ã“ã“ã¾ã§ã® filetype_counts / total_files / downloaded_file_count ã‚’ä½¿ã£ã¦å†…è¨³ä»˜ãè¿”å´
            logger.warning(f"å‡¦ç†ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ: {success_count}ä»¶æˆåŠŸ, {failed_count}ä»¶å¤±æ•—, {skipped_count}ä»¶ã‚¹ã‚­ãƒƒãƒ—")
            if filetype_counts:
                parts = [f"  ãƒ» {k}: {v}ä»¶" for k, v in sorted(filetype_counts.items())]
                inner = "\n".join(parts)
                counts_text = f"\n\nâ””â”€ å†…è¨³ï¼ˆfileTypeåˆ¥ï¼‰:\n{inner}"
            else:
                counts_text = "\n\nâ””â”€ å†…è¨³ï¼ˆfileTypeåˆ¥ï¼‰: å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãªã—"
            actual_downloaded = downloaded_file_count[0] if isinstance(downloaded_file_count, list) and downloaded_file_count else 0
            cancel_msg = (
                "âš ï¸ ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ" +
                f"\n\nğŸ“Š åˆè¨ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰äºˆå®šãƒ•ã‚¡ã‚¤ãƒ«: {total_files}ä»¶" + counts_text +
                f"\n\nâœ… å®Ÿãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ãƒ•ã‚¡ã‚¤ãƒ«: {actual_downloaded}ä»¶"
            )
            logger.info(cancel_msg)
            return cancel_msg
        
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°å‡ºåŠ›
        if errors:
            logger.error(f"ã‚¨ãƒ©ãƒ¼ãŒ{len(errors)}ä»¶ç™ºç”Ÿã—ã¾ã—ãŸ:")
            for err in errors[:10]:  # æœ€åˆã®10ä»¶ã®ã¿ãƒ­ã‚°å‡ºåŠ›
                logger.error(f"  - {err}")
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹å®Œäº†
        if progress_callback:
            progress_callback(1.0, 1.0, 
                            f"å‡¦ç†å®Œäº†: {success_count}ã‚¨ãƒ³ãƒˆãƒªæˆåŠŸ, {failed_count}ã‚¨ãƒ³ãƒˆãƒªå¤±æ•—, {skipped_count}ã‚¨ãƒ³ãƒˆãƒªã‚¹ã‚­ãƒƒãƒ—")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—å†…è¨³ãƒ†ã‚­ã‚¹ãƒˆ
        if filetype_counts:
            parts = [f"  ãƒ» {k}: {v}ä»¶" for k, v in sorted(filetype_counts.items())]
            inner = "\n".join(parts)
            counts_text = f"\n\nâ””â”€ å†…è¨³ï¼ˆfileTypeåˆ¥ï¼‰:\n{inner}"
        else:
            counts_text = "\n\nâ””â”€ å†…è¨³ï¼ˆfileTypeåˆ¥ï¼‰: å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãªã—"
        actual_downloaded = downloaded_file_count[0] if isinstance(downloaded_file_count, list) and downloaded_file_count else 0
        success_msg = (
            f"dataEntry/{dataset_id}.jsonå†…ã®å„data idã«ã¤ã„ã¦dataFiles/ã«ä¿å­˜ã—ã¾ã—ãŸã€‚\n"
            f"\nâœ” æˆåŠŸ: {success_count}ä»¶"
            f"\nâœ– å¤±æ•—: {failed_count}ä»¶"
            f"\nâ¦¿ ã‚¹ã‚­ãƒƒãƒ—: {skipped_count}ä»¶"
            f"\n\nğŸ“Š åˆè¨ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰äºˆå®šãƒ•ã‚¡ã‚¤ãƒ«: {total_files}ä»¶" + counts_text +
            f"\n\nâœ… å®Ÿãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ãƒ•ã‚¡ã‚¤ãƒ«: {actual_downloaded}ä»¶"
        )
        logger.info(success_msg)
        return success_msg

    except Exception as e:
        error_msg = f"ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—å‡¦ç†ä¸­ã«äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
        logger.error(error_msg)
        import traceback
        traceback.print_exc()
        return None
