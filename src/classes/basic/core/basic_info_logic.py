
"""
åŸºæœ¬æƒ…å ±å–å¾—ãƒ»å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯

RDEã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰åŸºæœ¬æƒ…å ±ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€è£…ç½®ã€çµ„ç¹”ç­‰ï¼‰ã‚’å–å¾—ã—ã€
JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã™ã‚‹å‡¦ç†ã‚’æä¾›ã—ã¾ã™ã€‚

ä¸»è¦æ©Ÿèƒ½:
- ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—ï¼ˆself.jsonï¼‰
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±å–å¾—ï¼ˆdataset.jsonï¼‰
- ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±å–å¾—
- è£…ç½®ã‚¿ã‚¤ãƒ—ãƒ»çµ„ç¹”æƒ…å ±å–å¾—
- ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ»è¨­å‚™æƒ…å ±å–å¾—
- Excelå‡ºåŠ›æ©Ÿèƒ½

æŠ€è¡“ä»•æ§˜:
- çµ±ä¸€ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ 
- å …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- API ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å…±é€šåŒ–
"""

import os
import json
import logging
import sys
import traceback
import glob
import shutil
from pathlib import Path

from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import Callable, Dict, List, Optional
from urllib.parse import quote, urlencode

from dateutil.parser import parse as parse_datetime  # ISO8601å¯¾å¿œã®ãŸã‚
from ..util.xlsx_exporter import apply_basic_info_to_Xlsx_logic, summary_basic_info_to_Xlsx_logic
from classes.utils.api_request_helper import api_request  # refactored to use api_request_helper
from classes.basic.core.api_recording_wrapper import (
    record_api_call_for_dataset_list,
    record_api_call_for_instruments,
    record_api_call_for_template,
)
from config.common import (
    DATA_ENTRY_DIR,
    DATASET_JSON_CHUNKS_DIR,
    DATASET_JSON_PATH,
    GROUP_DETAIL_JSON_PATH,
    GROUP_JSON_PATH,
    GROUP_ORGNIZATION_DIR,
    GROUP_PROJECT_DIR,
    INFO_JSON_PATH,
    LICENSES_JSON_PATH,
    ORGANIZATION_JSON_PATH,
    INSTRUMENT_JSON_CHUNKS_DIR,
    INSTRUMENTS_JSON_PATH,
    INSTRUMENT_TYPE_JSON_PATH,
    INVOICE_DIR,
    LEGACY_SUBGROUP_DETAILS_DIR,
    OUTPUT_DIR as COMMON_OUTPUT_DIR,
    OUTPUT_RDE_DATA_DIR,
    SELF_JSON_PATH,
    SUBGROUP_REL_DETAILS_DIR,
    SUBGROUP_DETAILS_DIR,
    SUBGROUP_JSON_PATH,
    TEMPLATE_JSON_CHUNKS_DIR,
    TEMPLATE_JSON_PATH,
    get_dynamic_file_path,
)

# ãƒ­ã‚¬ãƒ¼è¨­å®šï¼ˆæ¨™æº–å‡ºåŠ›ã«ã‚‚å‡ºã™ï¼‰
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    stream=sys.stdout
)
logger = logging.getLogger(__name__)

# === è¨­å®šå€¤ ===
OUTPUT_DIR = COMMON_OUTPUT_DIR

PROGRAM_SELECTION_CONTEXT = "basic.program.root"
PROJECT_SELECTION_CONTEXT = "basic.project.detail"
SUBGROUP_SELECTION_CONTEXT = "basic.project.subgroup"

DATASET_LIST_PAGE_SIZE = 1000
DATASET_LIST_REQUEST_TIMEOUT = 30  # seconds
DATASET_CHUNK_FILE_TEMPLATE = "dataset_chunk_{:04d}.json"
_DATASET_RESERVED_KEYS = {"data", "included", "meta", "links"}
TEMPLATE_CHUNK_FILE_TEMPLATE = "template_chunk_{:04d}.json"
INSTRUMENT_CHUNK_FILE_TEMPLATE = "instrument_chunk_{:04d}.json"

# ä»–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ã‚‚åŒä¸€é–¾å€¤ã‚’ç”¨ã„ã‚‹ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æœ›: 1000ä»¶å˜ä½ï¼‰
DEFAULT_CHUNK_PAGE_SIZE = 1000

# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆAPIã¯ offset>0 ã®ãƒšãƒ¼ã‚¸ãƒ³ã‚°å–å¾—ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ/ä¸å®‰å®šã«ãªã‚Šã‚„ã™ã„å ±å‘ŠãŒã‚ã‚‹ãŸã‚ã€
# æ—§å®Ÿè£…äº’æ›ã¨ã—ã¦ã¾ãšã¯å¤§ãã‚limitã§å˜ç™ºå–å¾—ã‚’å„ªå…ˆã™ã‚‹ï¼ˆé€šå¸¸ã¯1å›žã§å®Œçµï¼‰ã€‚
# â€»å®Ÿéš›ã« 10,000 ã‚’è¶…ãˆã‚‹å ´åˆã®ã¿ãƒšãƒ¼ã‚¸ãƒ³ã‚°ã¨ãªã‚‹ã€‚
TEMPLATE_PAGE_SIZE = 10_000

INSTRUMENT_PAGE_SIZE = DEFAULT_CHUNK_PAGE_SIZE

# ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¯æ—§å®Ÿè£…ç›¸å½“ï¼ˆçŸ­ç¸®ã™ã‚‹ã¨ read timeout ã‚’èª˜ç™ºã—ã‚„ã™ã„ï¼‰
TEMPLATE_REQUEST_TIMEOUT = 30
INSTRUMENT_REQUEST_TIMEOUT = 10

TEMPLATE_API_BASE_URL = "https://rde-api.nims.go.jp/datasetTemplates"
INSTRUMENT_API_BASE_URL = "https://rde-instrument-api.nims.go.jp/instruments"
DEFAULT_PROGRAM_ID = "4bbf62be-f270-4a46-9682-38cd064607ba"
DEFAULT_TEAM_ID = "1e44cefd-85ba-49cb-bc7e-196a0ef379b0"

def stage_error_handler(operation_name: str):
    """
    æ®µéšŽå®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰ç”¨çµ±ä¸€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
    
    Args:
        operation_name: æ“ä½œåï¼ˆã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”¨ï¼‰
    
    Returns:
        ã‚¨ãƒ©ãƒ¼æ™‚ã¯ '{operation_name}ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error}'
        æˆåŠŸæ™‚ã¯å…ƒã®æˆ»ã‚Šå€¤ã‚’ãã®ã¾ã¾è¿”ã™
    """
    def decorator(func):
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except GroupFetchCancelled:
                # UIã®ã‚­ãƒ£ãƒ³ã‚»ãƒ«æ“ä½œã¯ã‚¨ãƒ©ãƒ¼æ‰±ã„ã«ã›ãšã€çµ±ä¸€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            except Exception as e:
                error_msg = f"{operation_name}ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
                logger.error(error_msg)
                return error_msg
        return wrapper
    return decorator

def save_json(data, *path):
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹å…±é€šé–¢æ•°"""
    filepath = os.path.join(*path)
    # ç›¸å¯¾ãƒ‘ã‚¹ã§æ¸¡ã•ã‚ŒãŸå ´åˆã§ã‚‚ã€å‹•çš„ãƒ‘ã‚¹è§£æ±ºã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã«ä¿å­˜ã™ã‚‹
    if not os.path.isabs(filepath):
        filepath = get_dynamic_file_path(filepath)
    try:
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {filepath}")
    except Exception as e:
        logger.error(f"JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å¤±æ•—: {filepath}, error={e}")
        raise


def _subgroups_folder_complete() -> bool:
    """ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—è©³ç´°ãƒ•ã‚©ãƒ«ãƒ€ã®å®Œå…¨æ€§ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹å…±é€šãƒ˜ãƒ«ãƒ‘ãƒ¼"""
    try:
        expected_ids = set()
        logger.info("\n[ãƒ•ã‚©ãƒ«ãƒ€å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯é–‹å§‹] v2.1.24")

        def _expected_team_ids_from_subgroup_json() -> set[str]:
            """subGroup.json ã‹ã‚‰æœŸå¾…TEAM IDã‚’æŠ½å‡ºã™ã‚‹ï¼ˆå¯èƒ½ãªã‚‰ã“ã‚Œã‚’æœ€å„ªå…ˆï¼‰ã€‚"""
            try:
                subgroup_json_path = Path(SUBGROUP_JSON_PATH)
                if not subgroup_json_path.exists():
                    return set()
                with open(subgroup_json_path, "r", encoding="utf-8") as f:
                    subgroup_data = json.load(f)

                extracted: set[str] = set()
                included = subgroup_data.get("included", [])
                for item in included:
                    if (
                        item.get("type") == "group"
                        and item.get("attributes", {}).get("groupType") == "TEAM"
                    ):
                        gid = item.get("id")
                        if isinstance(gid, str) and gid:
                            extracted.add(gid)

                # included ã« TEAM ãŒç„¡ã„å ´åˆã€relationships.children ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§åˆ©ç”¨
                if not extracted:
                    relationships = subgroup_data.get("data", {}).get("relationships", {})
                    children = relationships.get("children", {}).get("data", []) if isinstance(relationships, dict) else []
                    if isinstance(children, list):
                        for child in children:
                            if not isinstance(child, dict):
                                continue
                            gid = child.get("id")
                            if isinstance(gid, str) and gid:
                                extracted.add(gid)

                return extracted
            except Exception as e:
                logger.debug("subGroup.json ã‹ã‚‰ã®ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—æŽ¨å®šã«å¤±æ•—ï¼ˆå–å¾—ã‚’ç¶šè¡Œï¼‰: %s", e)
                return set()

        # v2.2.x: ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Œå…¨æ€§åˆ¤å®šã¯ subGroup.json ã‚’å„ªå…ˆï¼ˆgroupOrgnizations ãŒ stale ã§ã‚‚å¼•ããšã‚‰ã‚Œãªã„ï¼‰
        expected_ids = _expected_team_ids_from_subgroup_json()

        # subGroup.json ã‹ã‚‰å–ã‚Œãªã„å ´åˆã®ã¿ã€äº’æ›ã®ãŸã‚ groupOrgnizations/ ã‚’å‚ç…§
        if not expected_ids:
            org_dir = Path(GROUP_ORGNIZATION_DIR)
            if org_dir.exists():
                logger.info(f"  ðŸ“‚ groupOrgnizations/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒ£ãƒ³: {org_dir}")

                org_json_files = list(org_dir.glob("*.json"))
                logger.info(f"  ðŸ“‹ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆJSONãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(org_json_files)}å€‹")

                for json_file in org_json_files:
                    try:
                        with open(json_file, 'r', encoding='utf-8') as f:
                            proj_data = json.load(f)

                        included = proj_data.get("included", [])
                        subgroup_count = 0
                        for item in included:
                            if (
                                item.get("type") == "group" and
                                item.get("attributes", {}).get("groupType") == "TEAM"
                            ):
                                item_id = item.get("id")
                                if isinstance(item_id, str) and item_id:
                                    expected_ids.add(item_id)
                                    subgroup_count += 1

                        logger.debug(f"    âœ“ {json_file.name}: {subgroup_count}å€‹ã®ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æŠ½å‡º")
                    except Exception as e:
                        logger.warning(f"    âŒ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆJSONèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ï¼ˆ{json_file.name}ï¼‰: {e}")
                        continue
            else:
                logger.info(f"  â„¹ï¸  groupOrgnizations/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {org_dir}")

        # æœŸå¾…ã•ã‚Œã‚‹ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ãŒ0ä»¶ãªã‚‰ã€subGroups/ ã®ãƒ•ã‚¡ã‚¤ãƒ«æœ‰ç„¡ã§æ¬ ææ‰±ã„ã«ã—ãªã„
        if not expected_ids:
            logger.info("  â„¹ï¸  æœŸå¾…ã•ã‚Œã‚‹ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—IDãŒ0ä»¶ã®ãŸã‚ã€subGroups/å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return True

        expected_count = len(expected_ids)
        logger.info(f"  ðŸ“Š æœŸå¾…ã•ã‚Œã‚‹ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ç·æ•°: {expected_count}å€‹")
        logger.debug(f"  ðŸ“‹ æœŸå¾…ã•ã‚Œã‚‹IDä¸€è¦§ï¼ˆæœ€åˆ10å€‹ï¼‰: {list(expected_ids)[:10]}")

        subgroups_dir = Path(SUBGROUP_DETAILS_DIR)
        if not subgroups_dir.exists():
            logger.warning(f"  âŒ subGroups/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {subgroups_dir}")
            logger.warning(f"     æœŸå¾…: {expected_count}ä»¶ã®ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ãƒ•ã‚¡ã‚¤ãƒ«")
            return False

        logger.info(f"  ðŸ“‚ subGroups/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèª: {subgroups_dir}")
        json_files = list(subgroups_dir.glob("*.json"))
        actual_count = len(json_files)

        logger.info(f"  ðŸ“Š å®Ÿéš›ã®ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {actual_count}å€‹")

        actual_ids = {json_file.stem for json_file in json_files}
        missing_ids = expected_ids - actual_ids
        if missing_ids:
            logger.warning("\n  âš ï¸  [æ¬ ææ¤œå‡º] subGroups/ãƒ•ã‚©ãƒ«ãƒ€ã«æ¬ æãƒ•ã‚¡ã‚¤ãƒ«!")
            logger.warning(
                f"     æœŸå¾…: {expected_count}å€‹ | å®Ÿéš›: {actual_count}å€‹ | æ¬ æ: {len(missing_ids)}å€‹"
            )
            logger.warning(f"     æ¬ æIDä¸€è¦§ï¼ˆæœ€åˆ10å€‹ï¼‰: {list(missing_ids)[:10]}")
            if len(missing_ids) > 10:
                logger.debug(f"     æ¬ æIDä¸€è¦§ï¼ˆã™ã¹ã¦ï¼‰: {sorted(list(missing_ids))}")
            return False

        logger.info(f"  âœ… subGroups/ãƒ•ã‚©ãƒ«ãƒ€ã®å®Œå…¨æ€§ç¢ºèªå®Œäº†: {actual_count}å€‹ã™ã¹ã¦æƒã£ã¦ã„ã‚‹")
        logger.info("[ãƒ•ã‚©ãƒ«ãƒ€å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯çµ‚äº†] æ¬ æãªã—\n")
        return True
    except Exception as e:
        logger.debug(f"subGroups/ãƒ•ã‚©ãƒ«ãƒ€ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ï¼ˆå–å¾—ã‚’ç¶šè¡Œï¼‰: {e}")
        return False

def _make_headers(bearer_token, host, origin, referer):
    """API ãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç”Ÿæˆ"""
    return {
        "Accept": "application/vnd.api+json",
        "Accept-Encoding": "gzip, deflate, br, zstd",
        "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
        "Authorization": f"Bearer {bearer_token}",
        "Connection": "keep-alive",
        "Host": host,
        "Origin": origin,
        "Referer": referer,
        "Sec-Fetch-Dest": "empty",
        "Sec-Fetch-Mode": "cors",
        "Sec-Fetch-Site": "same-site",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
        "sec-ch-ua": '"Not)A;Brand";v="8", "Chromium";v="138", "Google Chrome";v="138"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
    }


def _progress_ok(progress_callback, percent: int, total: int, message: str) -> bool:
    """Run progress callback and treat None as OK (only False means cancel)."""
    if not progress_callback:
        return True
    try:
        result = progress_callback(percent, total, message)
        return result is not False
    except Exception as e:
        logger.debug("progress callback error ignored: %s", e)
        return True


def _clear_dataset_entry_cache():
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ã®ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
    
    ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±å–å¾—å¾Œã«ã€ã‚³ãƒ³ãƒœãƒœãƒƒã‚¯ã‚¹ã®ãƒ¡ãƒ¢ãƒªä¸Šã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå¤ã„ã¾ã¾ã«ãªã‚‰ãªã„ã‚ˆã†ã«ã‚¯ãƒªã‚¢ã™ã‚‹
    ç›®çš„ï¼š
    - ãƒ‡ãƒ¼ã‚¿å–å¾—2æ©Ÿèƒ½ã®ã‚³ãƒ³ãƒœãƒœãƒƒã‚¯ã‚¹ã«å¤ã„å€™è£œã—ã‹è¡¨ç¤ºã•ã‚Œãªã„å•é¡Œã‚’è§£æ±º
    - container.dataset_mapã‚’ã‚¯ãƒªã‚¢ã—ã¦ã€æ¬¡å›žã‚¢ã‚¯ã‚»ã‚¹æ™‚ã«dataset.jsonã‚’å†èª­ã¿è¾¼ã¿ã•ã›ã‚‹
    """
    try:
        # UIã‚³ãƒ³ãƒ†ãƒŠã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ã‹ã‚‰å–å¾—
        # ï¼ˆPySide6ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å ´åˆã€QWidgetã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«å‚ç…§å¯èƒ½ï¼‰
        from classes.ui.controllers.ui_controller import main_app_instance
        
        if main_app_instance and hasattr(main_app_instance, 'fetch2_dropdown_widget'):
            fetch2_widget = main_app_instance.fetch2_dropdown_widget
            if hasattr(fetch2_widget, 'clear_cache'):
                fetch2_widget.clear_cache()
                logger.info("UIã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼çµŒç”±ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                return
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æŽ¥ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚’ã‚¹ã‚­ãƒ£ãƒ³
        import gc
        for obj in gc.get_objects():
            try:
                if hasattr(obj, 'clear_cache') and hasattr(obj, 'dataset_map'):
                    obj.clear_cache()
                    logger.info("ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªçµŒç”±ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                    return
            except (TypeError, AttributeError):
                pass
        
        logger.debug("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªã‚¢: å¯¾è±¡ã‚³ãƒ³ãƒ†ãƒŠãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
    except ImportError:
        logger.debug("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªã‚¢: UIã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ãŒåˆ©ç”¨ä¸å¯")
    except Exception as e:
        logger.warning("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªã‚¢ä¸­ã«ã‚¨ãƒ©ãƒ¼: %s", e)


def _prepare_dataset_chunk_directory() -> Path:
    """dataset.jsonãƒãƒ£ãƒ³ã‚¯ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åˆæœŸåŒ–ã—ã¦è¿”ã™"""
    chunk_dir = Path(DATASET_JSON_CHUNKS_DIR)
    if chunk_dir.exists():
        for entry in chunk_dir.iterdir():
            try:
                if entry.is_file():
                    entry.unlink()
                elif entry.is_dir():
                    shutil.rmtree(entry)
            except Exception as cleanup_error:
                logger.warning("ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ (%s): %s", entry, cleanup_error)
    chunk_dir.mkdir(parents=True, exist_ok=True)
    logger.debug("datasetJsonChunksãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ: %s", chunk_dir)
    return chunk_dir


def _prepare_template_chunk_directory() -> Path:
    """template.jsonç”¨ãƒãƒ£ãƒ³ã‚¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åˆæœŸåŒ–"""
    chunk_dir = Path(TEMPLATE_JSON_CHUNKS_DIR)
    if chunk_dir.exists():
        for entry in chunk_dir.iterdir():
            try:
                if entry.is_file():
                    entry.unlink()
                elif entry.is_dir():
                    shutil.rmtree(entry)
            except Exception as cleanup_error:
                logger.warning("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ (%s): %s", entry, cleanup_error)
    chunk_dir.mkdir(parents=True, exist_ok=True)
    logger.debug("templateJsonChunksãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ: %s", chunk_dir)
    return chunk_dir


def _prepare_instrument_chunk_directory() -> Path:
    """instruments.jsonç”¨ãƒãƒ£ãƒ³ã‚¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åˆæœŸåŒ–"""
    chunk_dir = Path(INSTRUMENT_JSON_CHUNKS_DIR)
    if chunk_dir.exists():
        for entry in chunk_dir.iterdir():
            try:
                if entry.is_file():
                    entry.unlink()
                elif entry.is_dir():
                    shutil.rmtree(entry)
            except Exception as cleanup_error:
                logger.warning("è¨­å‚™ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ (%s): %s", entry, cleanup_error)
    chunk_dir.mkdir(parents=True, exist_ok=True)
    logger.debug("instrumentJsonChunksãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ: %s", chunk_dir)
    return chunk_dir


def _load_json_if_exists(path: str) -> Optional[Dict]:
    """å­˜åœ¨ã™ã‚‹å ´åˆã®ã¿JSONã‚’èª­ã¿è¾¼ã‚€"""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        logger.debug("JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: %s", path)
    except Exception as exc:
        logger.warning("JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ (%s): %s", path, exc)
    return None


def _resolve_program_id_for_templates(
    default_program_id: str = DEFAULT_PROGRAM_ID,
    output_dir: Optional[str] = None,
) -> str:
    """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå–å¾—ç”¨ã®programIdã‚’æ±ºå®š"""
    candidate_paths = []
    if output_dir:
        candidate_paths.append(os.path.join(output_dir, "group.json"))
    candidate_paths.append(GROUP_JSON_PATH)

    for path in candidate_paths:
        group_data = _load_json_if_exists(path)
        if group_data:
            program_id = parse_group_id_from_data(group_data, preferred_program_id=default_program_id)
            if program_id:
                return program_id
    return default_program_id


def _iterate_template_team_ids(output_dir: Optional[str] = None) -> List[str]:
    """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå–å¾—ã«ä½¿ç”¨ã™ã‚‹teamIdå€™è£œã‚’é †åºä»˜ãã§è¿”ã™"""
    env_override = os.environ.get("RDE_TEMPLATE_TEAM_ID") or os.environ.get("ARIM_TEMPLATE_TEAM_ID")
    if env_override:
        team_id = env_override.strip()
        if team_id:
            logger.info("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå–å¾—: ç’°å¢ƒå¤‰æ•°ã‹ã‚‰teamId=%sã®ã¿ã‚’å€™è£œã¨ã—ã¦ä½¿ç”¨", team_id[:12])
            return [team_id]

    def _append_unique(target: List[str], value: Optional[str]) -> None:
        if value and value not in target:
            target.append(value)

    def _extract_children_ids(payload: Dict) -> List[str]:
        children = payload.get("data", {}).get("relationships", {}).get("children", {}).get("data", [])
        if not isinstance(children, list):
            return []
        ids: List[str] = []
        for item in children:
            if isinstance(item, dict):
                child_id = item.get("id")
                if isinstance(child_id, str) and child_id:
                    ids.append(child_id)
        return ids

    def _resolve_self_user_id(target_dir: Optional[str]) -> Optional[str]:
        # output_dir ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã® self.json ã®ã¿ã‚’å‚ç…§ã™ã‚‹ã€‚
        # æ—¢å®šãƒ‘ã‚¹ã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã™ã‚‹ã¨ã€Œåˆ¥ãƒ¦ãƒ¼ã‚¶ãƒ¼/åˆ¥ç’°å¢ƒã®JSONã€ãŒæ··åœ¨ã—å¾—ã‚‹ãŸã‚ã€‚
        candidate_paths: List[str] = []
        if target_dir:
            candidate_paths.append(os.path.join(target_dir, "self.json"))
        else:
            candidate_paths.append(SELF_JSON_PATH)

        for path in candidate_paths:
            payload = _load_json_if_exists(path)
            if not payload:
                continue
            user_id = payload.get("data", {}).get("id")
            if isinstance(user_id, str) and user_id:
                return user_id
        return None

    def _user_has_project_role(project_item: Dict, user_id: str) -> bool:
        attrs = project_item.get("attributes", {})
        if not isinstance(attrs, dict):
            return False
        roles = attrs.get("roles", [])
        if not isinstance(roles, list):
            return False
        for role in roles:
            if isinstance(role, dict) and role.get("userId") == user_id:
                return True
        return False

    def _team_has_user_role(team_id: str, user_id: str, target_dir: Optional[str]) -> Optional[bool]:
        # None: åˆ¤å®šä¸èƒ½ï¼ˆè©³ç´°ãƒ•ã‚¡ã‚¤ãƒ«ãŒç„¡ã„/å£Šã‚Œã¦ã„ã‚‹ç­‰ï¼‰
        # True/False: åˆ¤å®šçµæžœ
        # output_dir ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã® subGroups ã®ã¿ã‚’å‚ç…§ã™ã‚‹ã€‚
        candidate_paths: List[str] = []
        if target_dir:
            candidate_paths.append(os.path.join(target_dir, "subGroups", f"{team_id}.json"))
        else:
            candidate_paths.append(os.path.join(SUBGROUP_DETAILS_DIR, f"{team_id}.json"))
            candidate_paths.append(os.path.join(LEGACY_SUBGROUP_DETAILS_DIR, f"{team_id}.json"))

        subgroup_detail: Optional[Dict] = None
        for path in candidate_paths:
            subgroup_detail = _load_json_if_exists(path)
            if subgroup_detail:
                break
        if not subgroup_detail:
            return None

        roles = subgroup_detail.get("data", {}).get("attributes", {}).get("roles", [])
        if not isinstance(roles, list):
            return None
        for role in roles:
            if isinstance(role, dict) and role.get("userId") == user_id:
                return True
        return False

    # ã¾ãšã¯ groupDetail.json / subGroup.json ã‹ã‚‰å€™è£œteamIdã‚’æŠ½å‡º
    # NOTE: teamId ã¯ã€Œã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªTEAMã‚°ãƒ«ãƒ¼ãƒ—IDã€ã§ã‚ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã€‚
    #       groupDetail.json ã® included(PROJECT) ã® children ãŒå®Ÿé‹ç”¨ä¸Šã®æœ‰åŠ›å€™è£œã€‚
    team_ids: List[str] = []

    # groupDetail.json ã‹ã‚‰ã€Œãƒ­ã‚°ã‚¤ãƒ³ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒ­ãƒ¼ãƒ«ã‚’æŒã¤PROJECTã®children(TEAM)ã€ã‚’å„ªå…ˆçš„ã«æŠ½å‡º
    user_id = _resolve_self_user_id(output_dir)
    preferred_team_ids: List[str] = []

    group_detail_paths: List[str] = []
    if output_dir:
        group_detail_paths.append(os.path.join(output_dir, "groupDetail.json"))
    else:
        group_detail_paths.append(GROUP_DETAIL_JSON_PATH)
    for path in group_detail_paths:
        group_detail = _load_json_if_exists(path)
        if not group_detail:
            continue

        # groupDetail.json ã¯ PROGRAM ã®è©³ç´°ã§ã€data.children ã¯ PROJECT ã‚’æŒ‡ã™ã“ã¨ãŒã‚ã‚‹ã€‚
        # ãã®ãŸã‚ã€included å†…ã® PROJECT ã® children (=TEAM) ã‚’å€™è£œã¨ã—ã¦æŽ¡ç”¨ã™ã‚‹ã€‚
        included = group_detail.get("included", [])
        if isinstance(included, list):
            for item in included:
                if not isinstance(item, dict):
                    continue
                attrs = item.get("attributes", {})
                if not isinstance(attrs, dict):
                    continue
                group_type = attrs.get("groupType")
                if group_type == "PROJECT":
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒPROJECTãƒ­ãƒ¼ãƒ«ã‚’æŒã¤ãªã‚‰ã€ãã®children(TEAM)ã¯ä½¿ãˆã‚‹å¯èƒ½æ€§ãŒé«˜ã„
                    if user_id and _user_has_project_role(item, user_id):
                        for child_id in _extract_children_ids({"data": item}):
                            _append_unique(preferred_team_ids, child_id)
                    for child_id in _extract_children_ids({"data": item}):
                        _append_unique(team_ids, child_id)
                elif group_type == "TEAM":
                    _append_unique(team_ids, item.get("id"))
        if team_ids:
            logger.debug(
                "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå–å¾—: %s ã® children ã‹ã‚‰ %d ä»¶ã®teamIdå€™è£œã‚’æŠ½å‡º",
                os.path.basename(path),
                len(team_ids),
            )
            break

    sub_group_paths: List[str] = []
    if output_dir:
        sub_group_paths.append(os.path.join(output_dir, "subGroup.json"))
    else:
        sub_group_paths.append(SUBGROUP_JSON_PATH)
    for path in sub_group_paths:
        sub_group_data = _load_json_if_exists(path)
        if not sub_group_data:
            continue

        # subGroup.json è‡ªä½“ãŒ PROJECT è©³ç´°ã§ã€children ã« TEAM ç¾¤ãŒä¸¦ã¶ã“ã¨ãŒã‚ã‚‹
        for child_id in _extract_children_ids(sub_group_data):
            _append_unique(team_ids, child_id)

        # æ—§æ¥ã® included(groupType=TEAM) ã‚‚äº’æ›çš„ã«æ‹¾ã†
        included = sub_group_data.get("included", [])
        if isinstance(included, list):
            for item in included:
                if not isinstance(item, dict):
                    continue
                attrs = item.get("attributes", {})
                if isinstance(attrs, dict) and attrs.get("groupType") == "TEAM":
                    _append_unique(team_ids, item.get("id"))

        if team_ids:
            logger.debug(
                "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå–å¾—: %s ã‹ã‚‰ %d ä»¶ã®teamIdå€™è£œã‚’æŠ½å‡º",
                os.path.basename(path),
                len(team_ids),
            )
            break

    # PROJECTãƒ­ãƒ¼ãƒ«çµŒç”±ã§å€™è£œãŒå–ã‚ŒãŸå ´åˆã¯ã€ãã‚Œã‚’å„ªå…ˆï¼ˆTEAMè©³ç´°ã§ã®æ‰€å±žåˆ¤å®šã«ä¾å­˜ã—ãªã„ï¼‰
    if preferred_team_ids:
        logger.info(
            "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå–å¾—: PROJECTãƒ­ãƒ¼ãƒ«ã«åŸºã¥ãteamIdå€™è£œã‚’æŽ¡ç”¨ã—ã¾ã™ (preferred=%d, total=%d)",
            len(preferred_team_ids),
            len(team_ids),
        )
        return preferred_team_ids

    # æ¬¡ã«ã€Œãƒ­ã‚°ã‚¤ãƒ³ä¸­ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ‰€å±žã™ã‚‹TEAMã€ã«çµžã‚Šè¾¼ã‚€ï¼ˆTEAMè©³ç´°ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆï¼‰
    if user_id and team_ids:
        member_team_ids: List[str] = []
        unknown_team_ids: List[str] = []
        for team_id in team_ids:
            verdict = _team_has_user_role(team_id, user_id, output_dir)
            if verdict is True:
                member_team_ids.append(team_id)
            elif verdict is None:
                unknown_team_ids.append(team_id)

        # æ‰€å±žTEAMãŒç‰¹å®šã§ããŸå ´åˆã¯ãã‚Œã‚’å„ªå…ˆã€‚
        # ç‰¹å®šã§ããªã„å ´åˆã¯å¾“æ¥é€šã‚Šå€™è£œå…¨ä½“ã‚’è¿”ã™ã€‚
        if member_team_ids:
            logger.info(
                "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå–å¾—: ãƒ­ã‚°ã‚¤ãƒ³ãƒ¦ãƒ¼ã‚¶ãƒ¼æ‰€å±žTEAMã«çµžã‚Šè¾¼ã¿ã¾ã—ãŸ (member=%d, unknown=%d, total=%d)",
                len(member_team_ids),
                len(unknown_team_ids),
                len(team_ids),
            )
            return member_team_ids

    if not team_ids:
        logger.info("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå–å¾—: teamIdå€™è£œãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ%sã®ã¿ã‚’ä½¿ç”¨", DEFAULT_TEAM_ID)
        team_ids.append(DEFAULT_TEAM_ID)

    return team_ids


def _template_payload_is_preferred(payload: Dict) -> bool:
    data_items = payload.get("data") or []
    if data_items:
        return True
    total_counts = payload.get("meta", {}).get("totalCounts")
    return isinstance(total_counts, int) and total_counts > 0


def _build_dataset_list_query_params(page_size: int, offset: int, search_words: Optional[str]) -> Dict[str, str]:
    params = {
        "sort": "-modified",
        "page[limit]": str(page_size),
        "page[offset]": str(offset),
        "include": "manager,releases",
        "fields[user]": "id,userName,organizationName,isDeleted",
        "fields[release]": "version,releaseNumber",
    }
    if search_words is not None:
        params["searchWords"] = search_words
    return params


def _build_dataset_list_url(query_params: Dict[str, str]) -> str:
    query = urlencode(query_params, quote_via=quote)
    return f"https://rde-api.nims.go.jp/datasets?{query}"


def _record_dataset_list_api_call(
    url: str,
    headers: Dict[str, str],
    status_code: int,
    elapsed_ms: float,
    query_params: Dict[str, str],
    success: bool,
    error: Optional[str] = None,
):
    try:
        record_api_call_for_dataset_list(
            url,
            headers,
            status_code,
            elapsed_ms,
            query_params=query_params,
            success=success,
            error=error,
        )
    except Exception as record_error:
        logger.debug("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§APIè¨˜éŒ²ã«å¤±æ•—ã—ã¾ã—ãŸ: %s", record_error)


def _merge_dataset_chunk_payloads(chunks: List[Dict]) -> Dict:
    if not chunks:
        return {"data": []}

    merged: Dict = {}
    first_chunk = chunks[0]
    for key, value in first_chunk.items():
        if key not in _DATASET_RESERVED_KEYS:
            merged[key] = value

    combined_data: List[Dict] = []
    included_map: Dict[tuple, Dict] = {}
    include_present = False

    for chunk in chunks:
        chunk_data = chunk.get("data", [])
        if chunk_data:
            combined_data.extend(chunk_data)

        included_section = chunk.get("included")
        if included_section is not None:
            include_present = True
            for item in included_section:
                item_id = item.get("id")
                item_type = item.get("type")
                if not item_id or not item_type:
                    continue
                key = (item_type, item_id)
                if key not in included_map:
                    included_map[key] = item

    merged["data"] = combined_data
    if include_present:
        merged["included"] = list(included_map.values())

    latest_meta = None
    latest_links = None
    for chunk in reversed(chunks):
        if latest_meta is None and chunk.get("meta") is not None:
            latest_meta = chunk.get("meta")
        if latest_links is None and chunk.get("links"):
            latest_links = chunk.get("links")
        if latest_meta is not None and latest_links is not None:
            break

    if latest_meta is not None:
        merged["meta"] = latest_meta
    if latest_links is not None:
        merged["links"] = latest_links

    return merged


def _merge_dataset_search_payloads(payloads: List[Dict]) -> Dict:
    """Merge multiple dataset payloads produced by different search words."""
    if not payloads:
        return {"data": []}

    merged_data: List[Dict] = []
    seen_ids = set()
    included_map: Dict[tuple, Dict] = {}

    for payload in payloads:
        for item in payload.get("data", []) or []:
            ds_id = item.get("id")
            if ds_id and ds_id not in seen_ids:
                merged_data.append(item)
                seen_ids.add(ds_id)

        for inc in payload.get("included", []) or []:
            inc_id = inc.get("id")
            inc_type = inc.get("type")
            if inc_id and inc_type:
                key = (inc_type, inc_id)
                if key not in included_map:
                    included_map[key] = inc

    base_payload = payloads[-1]
    merged: Dict = {k: v for k, v in base_payload.items() if k not in ("data", "included")}
    merged["data"] = merged_data
    if included_map:
        merged["included"] = list(included_map.values())

    meta = dict(base_payload.get("meta", {}) or {})
    meta["totalCounts"] = len(merged_data)
    merged["meta"] = meta
    return merged


def _download_paginated_resource(
    *,
    base_url: str,
    base_params: Dict[str, str],
    headers: Dict[str, str],
    bearer_token: Optional[str],
    page_size: int,
    timeout: int,
    record_callback: Optional[Callable[..., None]] = None,
    progress_callback: Optional[Callable[[int, int, str], bool]] = None,
    chunk_label: str,
    chunk_dir_factory: Optional[Callable[[], Path]] = None,
    chunk_file_template: Optional[str] = None,
) -> Dict:
    """å…±é€šã®ãƒšãƒ¼ã‚¸ãƒ³ã‚°å–å¾—ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆ1000ä»¶å˜ä½ã®åˆ†å‰²å–å¾—ç”¨ï¼‰"""
    import time

    chunk_dir: Optional[Path] = None
    if chunk_dir_factory:
        chunk_dir = chunk_dir_factory()

    offset = 0
    chunk_index = 1
    total_expected = None
    total_processed = 0
    chunk_payloads: List[Dict] = []

    while True:
        params = dict(base_params or {})
        params["page[limit]"] = str(page_size)
        params["page[offset]"] = str(offset)
        query = urlencode(params, quote_via=quote)
        url = f"{base_url}?{query}"

        start_time = time.time()
        resp = api_request(
            "GET",
            url,
            bearer_token=bearer_token,
            headers=headers,
            timeout=timeout,
        )
        elapsed_ms = (time.time() - start_time) * 1000

        if resp is None:
            error_msg = "APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒNoneã‚’è¿”ã—ã¾ã—ãŸ"
            if record_callback:
                record_callback(url, headers, 0, elapsed_ms, success=False, error=error_msg)
            raise RuntimeError(f"{chunk_label}: {error_msg}")

        try:
            resp.raise_for_status()
        except Exception as http_error:
            status_code = getattr(resp, "status_code", 500)
            if record_callback:
                record_callback(
                    url,
                    headers,
                    status_code,
                    elapsed_ms,
                    success=False,
                    error=str(http_error),
                )
            raise

        if record_callback:
            record_callback(url, headers, resp.status_code, elapsed_ms, success=True)

        payload = resp.json()
        chunk_payloads.append(payload)

        if chunk_dir and chunk_file_template:
            chunk_path = chunk_dir / chunk_file_template.format(chunk_index)
            try:
                with open(chunk_path, "w", encoding="utf-8") as f:
                    json.dump(payload, f, ensure_ascii=False, indent=2)
            except Exception as write_error:
                logger.warning("%s: ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ (%s): %s", chunk_label, chunk_path, write_error)

        chunk_count = len(payload.get("data", []))
        total_processed += chunk_count
        if total_expected is None:
            total_expected = payload.get("meta", {}).get("totalCounts")

        if progress_callback:
            try:
                total_for_progress = int(total_expected) if total_expected is not None else 0
            except Exception:
                total_for_progress = 0

            if not _progress_ok(
                progress_callback,
                int(total_processed),
                int(total_for_progress),
                f"{chunk_label}: {total_processed}/{total_for_progress if total_for_progress else '?'} (chunk={chunk_index}, offset={offset})",
            ):
                raise GroupFetchCancelled("ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")

        logger.info(
            "%s: ãƒãƒ£ãƒ³ã‚¯%04dã‚’å–å¾— (ä»¶æ•°=%d, offset=%d)",
            chunk_label,
            chunk_index,
            chunk_count,
            offset,
        )

        if total_expected is not None and total_processed >= total_expected:
            break
        if chunk_count == 0:
            break
        if total_expected is None and chunk_count < page_size:
            break

        offset += page_size
        chunk_index += 1

    if not chunk_payloads:
        return {"data": []}

    merged_payload = _merge_dataset_chunk_payloads(chunk_payloads)
    logger.info(
        "%s: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å–å¾—å®Œäº† (chunks=%d, records=%d, expected=%s)",
        chunk_label,
        len(chunk_payloads),
        total_processed,
        total_expected if total_expected is not None else "unknown",
    )
    return merged_payload


def _download_dataset_list_in_chunks(
    bearer_token: Optional[str],
    headers: Dict[str, str],
    search_words: Optional[str] = None,
    page_size: int = DATASET_LIST_PAGE_SIZE,
) -> Dict:
    import time

    chunk_dir = _prepare_dataset_chunk_directory()
    offset = 0
    chunk_index = 1
    total_expected = None
    total_processed = 0
    chunk_payloads: List[Dict] = []

    while True:
        query_params = _build_dataset_list_query_params(page_size, offset, search_words)
        url = _build_dataset_list_url(query_params)
        start_time = time.time()
        resp = api_request(
            "GET",
            url,
            bearer_token=bearer_token,
            headers=headers,
            timeout=DATASET_LIST_REQUEST_TIMEOUT,
        )
        elapsed_ms = (time.time() - start_time) * 1000

        if resp is None:
            error_msg = "APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒNoneã‚’è¿”ã—ã¾ã—ãŸ"
            _record_dataset_list_api_call(url, headers, 0, elapsed_ms, query_params, False, error_msg)
            raise RuntimeError(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {error_msg}")

        try:
            resp.raise_for_status()
        except Exception as http_error:
            status_code = getattr(resp, "status_code", 500)
            _record_dataset_list_api_call(url, headers, status_code, elapsed_ms, query_params, False, str(http_error))
            raise

        _record_dataset_list_api_call(url, headers, resp.status_code, elapsed_ms, query_params, True)

        payload = resp.json()
        chunk_payloads.append(payload)
        chunk_path = chunk_dir / DATASET_CHUNK_FILE_TEMPLATE.format(chunk_index)
        with open(chunk_path, "w", encoding="utf-8") as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)

        chunk_count = len(payload.get("data", []))
        total_processed += chunk_count
        if total_expected is None:
            total_expected = payload.get("meta", {}).get("totalCounts")

        logger.info(
            "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§: ãƒãƒ£ãƒ³ã‚¯%04dã‚’å–å¾— (ä»¶æ•°=%d, offset=%d)",
            chunk_index,
            chunk_count,
            offset,
        )

        if total_expected is not None and total_processed >= total_expected:
            break
        if chunk_count == 0:
            break
        if total_expected is None and chunk_count < page_size:
            break

        offset += page_size
        chunk_index += 1

    merged_payload = _merge_dataset_chunk_payloads(chunk_payloads)
    logger.info(
        "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å–å¾—å®Œäº† (chunks=%d, records=%d, expected=%s)",
        len(chunk_payloads),
        total_processed,
        total_expected if total_expected is not None else "unknown",
    )
    return merged_payload

def fetch_invoice_schemas(bearer_token, output_dir, progress_callback=None, max_workers: int = 10):
    """
    template.jsonã®å…¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆIDã«ã¤ã„ã¦invoiceSchemasã‚’å–å¾—ã—ä¿å­˜ã™ã‚‹
    v2.1.0: ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾å¿œï¼ˆ50ä»¶ä»¥ä¸Šã§è‡ªå‹•ä¸¦åˆ—åŒ–ï¼‰
    """
    try:
        from net.http_helpers import parallel_download
        import threading
        
        if progress_callback:
            if not progress_callback(0, 100, f"invoiceSchemaså–å¾—ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™... (ä¸¦åˆ—: {max_workers})"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
                
        os.makedirs(os.path.join(output_dir, "invoiceSchemas"), exist_ok=True)
        template_json_path = os.path.join(output_dir, "template.json")
        log_path = os.path.join(output_dir, "invoiceSchemas", "invoiceSchemas_fetch.log")

        if progress_callback:
            if not progress_callback(5, 100, f"template.jsonã‚’èª­ã¿è¾¼ã¿ä¸­... (ä¸¦åˆ—: {max_workers})"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        try:
            with open(template_json_path, encoding="utf-8") as f:
                template_data = json.load(f)
            template_ids = [t["id"] for t in template_data.get("data", []) if "id" in t]
            logger.info(f"template.jsonèª­ã¿è¾¼ã¿å®Œäº†: {len(template_ids)}ä»¶ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆIDå–å¾—")
        except Exception as e:
            logger.error(f"template.jsonèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            template_ids = []

        if progress_callback:
            if not progress_callback(10, 100, f"å–å¾—å¯¾è±¡: {len(template_ids)}ä»¶ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ (ä¸¦åˆ—: {max_workers})"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        summary_path = os.path.join(output_dir, "invoiceSchemas", "summary.json")
        # æ—¢å­˜summary.jsonã®èª­ã¿è¾¼ã¿
        if os.path.exists(summary_path):
            with open(summary_path, "r", encoding="utf-8") as f:
                summary = json.load(f)
        else:
            summary = {"success": [], "failed": {}}

        # æ—§å½¢å¼/å£Šã‚ŒãŸsummaryã®äº’æ›è£œæ­£
        if not isinstance(summary, dict):
            summary = {"success": [], "failed": {}}
        if not isinstance(summary.get("success"), list):
            summary["success"] = []
        if not isinstance(summary.get("failed"), dict):
            summary["failed"] = {}

        # teamId ã¯ template.json å–å¾—æ™‚ã¨åŒæ§˜ã« subGroup.json ã‹ã‚‰æŠ½å‡ºã—ãŸå€™è£œã‚’ä½¿ã†
        team_id_candidates = _iterate_template_team_ids(output_dir)
        logger.info("invoiceSchemaså–å¾—: teamIdå€™è£œ=%s", [t[:12] for t in team_id_candidates])

        # ä¸¦åˆ—å®Ÿè¡Œæ™‚ã«summary/log/summary.jsonã‚’æ›¸ãæ›ãˆã‚‹ãŸã‚ãƒ­ãƒƒã‚¯ã‚’å…±æœ‰
        summary_lock = threading.Lock()

        total_templates = len(template_ids)
        
        # ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã‚’ä½œæˆï¼ˆä¸¦åˆ—å®Ÿè¡Œç”¨ï¼‰
        tasks = [
            (bearer_token, template_id, output_dir, summary, log_path, summary_path, team_id_candidates, summary_lock)
            for template_id in template_ids
        ]
        
        # ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œï¼ˆ50ä»¶ä»¥ä¸Šã§è‡ªå‹•ä¸¦åˆ—åŒ–ï¼‰
        def worker(token, template_id, out_dir, summ, log_p, summ_p, team_ids, lock):
            """ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°"""
            try:
                return fetch_invoice_schema_from_api(token, template_id, out_dir, summ, log_p, summ_p, team_ids, lock)
            except Exception as e:
                logger.error(f"invoiceSchemaå–å¾—å¤±æ•— (template_id: {template_id}): {e}")
                try:
                    with lock:
                        if not isinstance(summ, dict):
                            return f"failed: {e}"
                        summ.setdefault("failed", {})[template_id] = str(e)
                        with open(summ_p, "w", encoding="utf-8") as f:
                            json.dump(summ, f, ensure_ascii=False, indent=2)
                except Exception:
                    logger.debug("invoiceSchemaså–å¾—: summaryæ›´æ–°ã«å¤±æ•—", exc_info=True)
                return f"failed: {e}"
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’èª¿æ•´ï¼ˆ10-95%ã®ç¯„å›²ã«ãƒžãƒƒãƒ”ãƒ³ã‚°ï¼‰
        def adjusted_progress_callback(current, total, message):
            if progress_callback:
                progress_percent = 10 + int((current / 100) * 85)  # 10-95%
                return progress_callback(progress_percent, 100, f"ä¸¦åˆ—invoiceSchemaå–å¾—ä¸­: {message}")
            return True
        
        result = parallel_download(
            tasks=tasks,
            worker_function=worker,
            max_workers=max_workers,
            progress_callback=adjusted_progress_callback,
            threshold=50
        )

        # æœ€çµ‚ä¿å­˜
        with open(summary_path, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        if progress_callback:
            progress_callback(100, 100, f"invoiceSchemaå–å¾—å®Œäº† (ä¸¦åˆ—: {max_workers})")
            
        success_count = len(summary.get("success", []))
        failed_count = len(summary.get("failed", {}))
        result_msg = f"invoiceSchemaå–å¾—å®Œäº†: æˆåŠŸ={success_count}, å¤±æ•—={failed_count}, ç·æ•°={total_templates}"
        logger.info(result_msg)
        
        if result['cancelled']:
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        return result_msg
        
    except Exception as e:
        error_msg = f"invoiceSchemaå–å¾—å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}"
        logger.error(error_msg)
        if progress_callback:
            progress_callback(100, 100, f"ã‚¨ãƒ©ãƒ¼: {error_msg}")
        return error_msg

def get_self_username_from_json(json_path=None):
    """self.json ã‹ã‚‰ userName ã‚’å–å¾—ã—ã¦è¿”ã™ã€‚å­˜åœ¨ã—ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ã€‚"""
    resolved_path = json_path or get_dynamic_file_path("output/rde/data/self.json")
    abs_json = os.path.abspath(resolved_path)
    if not os.path.exists(abs_json):
        logger.warning(f"self.jsonãŒå­˜åœ¨ã—ã¾ã›ã‚“: {abs_json}")
        return ""
    try:
        with open(abs_json, "r", encoding="utf-8") as f:
            data = json.load(f)
        username = data.get("data", {}).get("attributes", {}).get("userName", "")
        logger.debug(f"self.jsonã‹ã‚‰userNameå–å¾—: {username}")
        return username
    except Exception as e:
        logger.error(f"self.jsonã®userNameå–å¾—å¤±æ•—: {e}")
        return ""
# --- ãƒ¦ãƒ¼ã‚¶ãƒ¼è‡ªèº«æƒ…å ±å–å¾— ---
def fetch_self_info_from_api(bearer_token=None, output_dir=None, parent_widget=None):
    """
    https://rde-user-api.nims.go.jp/users/self ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—ã—ã€self.jsonã¨ã—ã¦ä¿å­˜
    
    v2.0.1æ”¹å–„:
    - HTTPã‚¨ãƒ©ãƒ¼ã®è©³ç´°ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    - 401/403ã‚¨ãƒ©ãƒ¼ã®æ˜Žç¤ºçš„ãªæ¤œå‡ºã¨é€šçŸ¥
    - ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ”¹å–„
    
    Args:
        bearer_token: Bearer Tokenï¼ˆéžæŽ¨å¥¨ã€äº’æ›æ€§ã®ãŸã‚æ®‹å­˜ã€‚Noneã®å ´åˆã¯è‡ªå‹•é¸æŠžï¼‰
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        parent_widget: è¦ªã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆï¼ˆæœªä½¿ç”¨ã€äº’æ›æ€§ã®ãŸã‚æ®‹å­˜ï¼‰
    """
    # v1.18.4: bearer_tokenå¼•æ•°ã¯äº’æ›æ€§ã®ãŸã‚ã«æ®‹ã—ã¦ã„ã‚‹ãŒã€
    # api_request_helperãŒè‡ªå‹•é¸æŠžã™ã‚‹ãŸã‚ã€æ˜Žç¤ºçš„ã«æ¸¡ã•ãªã„
    
    url = "https://rde-user-api.nims.go.jp/users/self"
    headers = {
        "Accept": "application/vnd.api+json",
        "Host": "rde-user-api.nims.go.jp",
        "Origin": "https://rde.nims.go.jp",
        "Referer": "https://rde.nims.go.jp/"
    }
    
    # APIè¨˜éŒ²æ©Ÿèƒ½ã®åˆæœŸåŒ–
    import time
    start_time = time.time()
    
    try:
        logger.info("ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—é–‹å§‹")
        # v1.18.4: bearer_token=Noneã§è‡ªå‹•é¸æŠžã•ã›ã‚‹
        resp = api_request("GET", url, bearer_token=None, headers=headers, timeout=10)
        elapsed_ms = (time.time() - start_time) * 1000
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯
        if resp is None:
            error_msg = "ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—å¤±æ•—: APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒNoneã‚’è¿”ã—ã¾ã—ãŸï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã¾ãŸã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼‰"
            logger.error(error_msg)
            
            # APIè¨˜éŒ²ã‚’è¿½åŠ ï¼ˆå¤±æ•—ï¼‰
            try:
                from classes.basic.core.api_recording_wrapper import record_api_call_for_self_info
                record_api_call_for_self_info(url, headers, 0, elapsed_ms, False, "APIãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—")
            except Exception as e:
                logger.debug(f"APIè¨˜éŒ²è¿½åŠ å¤±æ•—: {e}")
            
            raise Exception(error_msg)
        
        # HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆv2.0.1æ”¹å–„ï¼‰
        if resp.status_code == 401:
            error_msg = "èªè¨¼ã‚¨ãƒ©ãƒ¼ï¼ˆ401ï¼‰: Bearer TokenãŒç„¡åŠ¹ã¾ãŸã¯æœŸé™åˆ‡ã‚Œã§ã™ã€‚å†ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„ã€‚"
            logger.error(error_msg)
            
            # APIè¨˜éŒ²ã‚’è¿½åŠ ï¼ˆ401ï¼‰
            try:
                from classes.basic.core.api_recording_wrapper import record_api_call_for_self_info
                record_api_call_for_self_info(url, headers, 401, elapsed_ms, False, error_msg)
            except Exception as e:
                logger.debug(f"APIè¨˜éŒ²è¿½åŠ å¤±æ•—: {e}")
            
            raise Exception(error_msg)
        elif resp.status_code == 403:
            error_msg = "ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦ï¼ˆ403ï¼‰: ã“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—ã®æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
            logger.error(error_msg)
            
            # APIè¨˜éŒ²ã‚’è¿½åŠ ï¼ˆ403ï¼‰
            try:
                from classes.basic.core.api_recording_wrapper import record_api_call_for_self_info
                record_api_call_for_self_info(url, headers, 403, elapsed_ms, False, error_msg)
            except Exception as e:
                logger.debug(f"APIè¨˜éŒ²è¿½åŠ å¤±æ•—: {e}")
            
            raise Exception(error_msg)
        elif resp.status_code != 200:
            error_msg = f"ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—å¤±æ•—: HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ {resp.status_code}"
            logger.error(error_msg)
            
            # APIè¨˜éŒ²ã‚’è¿½åŠ ï¼ˆãã®ä»–ã‚¨ãƒ©ãƒ¼ï¼‰
            try:
                from classes.basic.core.api_recording_wrapper import record_api_call_for_self_info
                record_api_call_for_self_info(url, headers, resp.status_code, elapsed_ms, False, error_msg)
            except Exception as e:
                logger.debug(f"APIè¨˜éŒ²è¿½åŠ å¤±æ•—: {e}")
            
            raise Exception(error_msg)
        
        # JSONãƒ‘ãƒ¼ã‚¹
        try:
            data = resp.json()
        except Exception as json_error:
            error_msg = f"ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã®JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—: {json_error}"
            logger.error(error_msg)
            
            # APIè¨˜éŒ²ã‚’è¿½åŠ ï¼ˆãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ï¼‰
            try:
                from classes.basic.core.api_recording_wrapper import record_api_call_for_self_info
                record_api_call_for_self_info(url, headers, resp.status_code, elapsed_ms, False, f"JSONè§£æžã‚¨ãƒ©ãƒ¼: {json_error}")
            except Exception as e:
                logger.debug(f"APIè¨˜éŒ²è¿½åŠ å¤±æ•—: {e}")
            
            raise Exception(error_msg)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        output_dir = output_dir or OUTPUT_RDE_DATA_DIR
        os.makedirs(output_dir, exist_ok=True)
        save_path = os.path.join(output_dir, "self.json")
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"self.jsonå–å¾—ãƒ»ä¿å­˜å®Œäº†: {save_path}")
        
        # APIè¨˜éŒ²ã‚’è¿½åŠ ï¼ˆæˆåŠŸï¼‰
        try:
            from classes.basic.core.api_recording_wrapper import record_api_call_for_self_info
            record_api_call_for_self_info(url, headers, 200, elapsed_ms, True)
        except Exception as e:
            logger.debug(f"APIè¨˜éŒ²è¿½åŠ å¤±æ•—: {e}")
        
        return True
        
    except Exception as e:
        logger.error(f"self.jsonå–å¾—ãƒ»ä¿å­˜å¤±æ•—: {e}")
        # v2.0.1: ã‚¨ãƒ©ãƒ¼ã‚’å‘¼ã³å‡ºã—å…ƒã«ä¼æ’­ã•ã›ã‚‹
        raise


def fetch_all_data_entrys_info(bearer_token, output_dir=None, progress_callback=None, parallel_threshold: int = 50, max_workers: int = 10):
    """
    dataset.jsonå†…ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDã§fetch_data_entry_info_from_apiã‚’å‘¼ã³å‡ºã™
    
    æ”¹å–„ç‰ˆ: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç·æ•°ã‚’äº‹å‰è¨ˆç®—ã—ã€ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°é »åº¦ã‚’å‘ä¸Š
    v2.1.0: ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾å¿œï¼ˆ50ä»¶ä»¥ä¸Šã§è‡ªå‹•ä¸¦åˆ—åŒ–ï¼‰
    
    Args:
        bearer_token: èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        progress_callback: ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•° (current, total, message) -> bool
        parallel_threshold: ä¸¦åˆ—åŒ–é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50ä»¶ï¼‰
        max_workers: æœ€å¤§ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰
    """
    try:
        from net.http_helpers import parallel_download
        
        output_dir = output_dir or OUTPUT_RDE_DATA_DIR
        os.makedirs(output_dir, exist_ok=True)
        dataset_json = os.path.join(output_dir, "dataset.json")
        
        if not os.path.exists(dataset_json):
            logger.error(f"dataset.jsonãŒå­˜åœ¨ã—ã¾ã›ã‚“: {dataset_json}")
            return
        
        if progress_callback:
            if not progress_callback(0, 0, f"ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±å–å¾—æº–å‚™ä¸­... (ä¸¦åˆ—: {max_workers})"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
        with open(dataset_json, "r", encoding="utf-8") as f:
            data = json.load(f)
            
        datasets = data.get("data", [])
        total_datasets = len(datasets)
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±å–å¾—é–‹å§‹: {total_datasets}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†")
        
        if progress_callback:
            if not progress_callback(0, total_datasets, f"ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªå–å¾—é–‹å§‹: ç·æ•°={total_datasets}ä»¶ (ä¸¦åˆ—: {max_workers})"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        # ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã‚’ä½œæˆï¼ˆä¸¦åˆ—å®Ÿè¡Œç”¨ï¼‰
        tasks = [(bearer_token, ds.get("id")) for ds in datasets if ds.get("id")]
        
        # ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œï¼ˆ50ä»¶ä»¥ä¸Šã§è‡ªå‹•ä¸¦åˆ—åŒ–ï¼‰
        def worker(token, ds_id):
            """ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°"""
            try:
                fetch_data_entry_info_from_api(token, ds_id)
                return "success"
            except Exception as e:
                logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªå‡¦ç†å¤±æ•—: ds_id={ds_id}, error={e}")
                return f"failed: {e}"
        
        # ä»¶æ•°ãƒ™ãƒ¼ã‚¹ã§é€²æ—ã‚’é€šçŸ¥ï¼ˆQProgressDialogå´ã§ current/total ã¨ ETA ã‚’è¡¨ç¤ºï¼‰
        def adjusted_progress_callback(current, total, message):
            if progress_callback:
                return progress_callback(int(current), int(total), f"ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªå–å¾—ä¸­: {message}")
            return True
        
        result = parallel_download(
            tasks=tasks,
            worker_function=worker,
            max_workers=max_workers,
            progress_callback=adjusted_progress_callback,
            threshold=parallel_threshold,
            progress_mode="count",
        )
        
        result_msg = (f"ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±å–å¾—å®Œäº†: "
                     f"æˆåŠŸ={result['success_count']}, "
                     f"å¤±æ•—={result['failed_count']}, "
                     f"ã‚¹ã‚­ãƒƒãƒ—={result['skipped_count']}, "
                     f"ç·æ•°={result['total']}")
        logger.info(result_msg)
        
        if progress_callback:
            progress_callback(100, 100, result_msg)
        
        if result['cancelled']:
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        return result_msg
        
    except Exception as e:
        error_msg = f"fetch_all_data_entrys_infoå‡¦ç†å¤±æ•—: {e}"
        logger.error(error_msg)
        if progress_callback:
            progress_callback(100, 100, f"ã‚¨ãƒ©ãƒ¼: {error_msg}")
        raise




def fetch_data_entry_info_from_api(bearer_token, dataset_id, output_dir=None):
    """
    æŒ‡å®šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDã®ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±ã‚’APIã‹ã‚‰å–å¾—ã—ã€dataEntry.jsonã¨ã—ã¦ä¿å­˜
    v1.18.4: Bearer Tokenè‡ªå‹•é¸æŠžå¯¾å¿œ
    """
    url = f"https://rde-api.nims.go.jp/data?filter%5Bdataset.id%5D={dataset_id}&sort=-created&page%5Boffset%5D=0&page%5Blimit%5D=24&include=owner%2Csample%2CthumbnailFile%2Cfiles"
    target_dir = output_dir or DATA_ENTRY_DIR
    save_path = os.path.join(target_dir, f"{dataset_id}.json")
    
    if os.path.exists(save_path):
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªãƒ•ã‚¡ã‚¤ãƒ«æ—¢å­˜ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {dataset_id}.json")
        return
        
    headers = {
        "Accept": "application/vnd.api+json",
        "Host": "rde-api.nims.go.jp",
        "Origin": "https://rde.nims.go.jp",
        "Referer": "https://rde.nims.go.jp/"
    }
    
    try:
        logger.debug(f"ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªå–å¾—é–‹å§‹: dataset_id={dataset_id}")
        # v1.18.4: bearer_token=Noneã§è‡ªå‹•é¸æŠžã•ã›ã‚‹
        resp = api_request("GET", url, bearer_token=None, headers=headers, timeout=10)
        if resp is None:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªå–å¾—å¤±æ•—: dataset_id={dataset_id}")
            return
        resp.raise_for_status()
        data = resp.json()
        
        os.makedirs(target_dir, exist_ok=True)
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªå–å¾—ãƒ»ä¿å­˜å®Œäº†: {dataset_id}.json -> {save_path}")
        
    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªå–å¾—ãƒ»ä¿å­˜å¤±æ•—: dataset_id={dataset_id}, error={e}")
        raise


def fetch_invoice_info_from_api(bearer_token, entry_id, output_dir=None):
    """æŒ‡å®šã‚¨ãƒ³ãƒˆãƒªIDã®ã‚¤ãƒ³ãƒœã‚¤ã‚¹æƒ…å ±ã‚’APIã‹ã‚‰å–å¾—ã—ã€invoice.jsonã¨ã—ã¦ä¿å­˜"""
    url = f"https://rde-api.nims.go.jp/invoices/{entry_id}?include=submittedBy%2CdataOwner%2Cinstrument"
    target_dir = output_dir or INVOICE_DIR
    save_path = os.path.join(target_dir, f"{entry_id}.json")
    
    if os.path.exists(save_path):
        logger.info(f"ã‚¤ãƒ³ãƒœã‚¤ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æ—¢å­˜ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {entry_id}.json")
        return
        
    headers = _make_headers(bearer_token, host="rde-api.nims.go.jp", origin="https://rde.nims.go.jp", referer="https://rde.nims.go.jp/")
    
    try:
        logger.debug(f"ã‚¤ãƒ³ãƒœã‚¤ã‚¹å–å¾—é–‹å§‹: entry_id={entry_id}")
        resp = api_request("GET", url, bearer_token=bearer_token, headers=headers, timeout=10)
        if resp is None:
            logger.error(f"ã‚¤ãƒ³ãƒœã‚¤ã‚¹å–å¾—å¤±æ•—: entry_id={entry_id}")
            return
        resp.raise_for_status()
        data = resp.json()
        
        os.makedirs(target_dir, exist_ok=True)
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
        logger.info(f"ã‚¤ãƒ³ãƒœã‚¤ã‚¹å–å¾—ãƒ»ä¿å­˜å®Œäº†: {entry_id}.json -> {save_path}")
        
    except Exception as e:
        logger.error(f"ã‚¤ãƒ³ãƒœã‚¤ã‚¹å–å¾—ãƒ»ä¿å­˜å¤±æ•—: entry_id={entry_id}, error={e}")
        raise


def fetch_all_invoices_info(bearer_token, output_dir=None, progress_callback=None, max_workers: int = 10):
    """
    dataEntry.jsonå†…ã®å…¨ã‚¨ãƒ³ãƒˆãƒªIDã§fetch_invoice_info_from_apiã‚’å‘¼ã³å‡ºã™
    
    æ”¹å–„ç‰ˆ: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°ã¨ã‚¿ã‚¤ãƒ«æ•°ã‹ã‚‰ç·äºˆå®šå–å¾—æ•°ã‚’äº‹å‰è¨ˆç®—ã—ã€
    ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°é »åº¦ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã¦å‡¦ç†ã®é€²è¡ŒçŠ¶æ³ã‚’æ˜Žç¢ºåŒ–
    v2.1.0: ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾å¿œï¼ˆ50ä»¶ä»¥ä¸Šã§è‡ªå‹•ä¸¦åˆ—åŒ–ï¼‰
    
    Args:
        bearer_token: èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        progress_callback: ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•° (current, total, message) -> bool
    """
    try:
        from net.http_helpers import parallel_download
        
        resolved_root = output_dir or OUTPUT_RDE_DATA_DIR
        dataentry_dir = os.path.join(resolved_root, "dataEntry")
        invoice_dir = os.path.join(resolved_root, "invoice")
        
        if not os.path.exists(dataentry_dir):
            logger.error(f"dataEntryãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {dataentry_dir}")
            return
        
        # === äº‹å‰ã‚«ã‚¦ãƒ³ãƒˆï¼šç·äºˆå®šå–å¾—æ•°ã‚’è¨ˆç®— ===
        if progress_callback:
            if not progress_callback(0, 100, f"ã‚¤ãƒ³ãƒœã‚¤ã‚¹ç·æ•°ã‚’è¨ˆç®—ä¸­... (ä¸¦åˆ—: {max_workers})"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        dataentry_files = glob.glob(os.path.join(dataentry_dir, "*.json"))
        
        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ç·ã‚¨ãƒ³ãƒˆãƒªæ•°ã‚’è¨ˆç®—
        entry_list = []  # [entry_id, ...]
        
        logger.info(f"ã‚¤ãƒ³ãƒœã‚¤ã‚¹æƒ…å ±å–å¾—é–‹å§‹: {len(dataentry_files)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æžä¸­")
        
        for file_path in dataentry_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                
                entries = data.get("data", [])
                for entry in entries:
                    entry_id = entry.get("id")
                    if entry_id:
                        entry_list.append(entry_id)
                        
            except Exception as e:
                logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: file={file_path}, error={e}")
        
        total_entries = len(entry_list)
        logger.info(f"ã‚¤ãƒ³ãƒœã‚¤ã‚¹å–å¾—è¨ˆç”»: ç·æ•°={total_entries}ä»¶")
        
        if progress_callback:
            msg = f"ã‚¤ãƒ³ãƒœã‚¤ã‚¹å–å¾—é–‹å§‹ (ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(dataentry_files)}ä»¶, ã‚¿ã‚¤ãƒ«ç·æ•°: {total_entries}ä»¶, ä¸¦åˆ—: {max_workers})"
            if not progress_callback(5, 100, msg):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        # ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã‚’ä½œæˆï¼ˆä¸¦åˆ—å®Ÿè¡Œç”¨ï¼‰
        tasks = [(bearer_token, entry_id) for entry_id in entry_list]
        
        # ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œï¼ˆ50ä»¶ä»¥ä¸Šã§è‡ªå‹•ä¸¦åˆ—åŒ–ï¼‰
        def worker(token, entry_id):
            """ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°"""
            try:
                fetch_invoice_info_from_api(token, entry_id, invoice_dir)
                return "success"
            except Exception as e:
                logger.error(f"ã‚¤ãƒ³ãƒœã‚¤ã‚¹å‡¦ç†å¤±æ•—: entry_id={entry_id}, error={e}")
                return f"failed: {e}"
        
        # ä»¶æ•°ãƒ™ãƒ¼ã‚¹ã§é€²æ—ã‚’é€šçŸ¥ï¼ˆQProgressDialogå´ã§ current/total ã¨ ETA ã‚’è¡¨ç¤ºï¼‰
        def adjusted_progress_callback(current, total, message):
            if progress_callback:
                return progress_callback(int(current), int(total), f"ã‚¤ãƒ³ãƒœã‚¤ã‚¹å–å¾—ä¸­: {message}")
            return True
        
        result = parallel_download(
            tasks=tasks,
            worker_function=worker,
            max_workers=max_workers,
            progress_callback=adjusted_progress_callback,
            threshold=50,
            progress_mode="count",
        )
        
        # === å®Œäº†å‡¦ç† ===
        result_msg = (f"ã‚¤ãƒ³ãƒœã‚¤ã‚¹æƒ…å ±å–å¾—å®Œäº†: "
                     f"æˆåŠŸ={result['success_count']}, "
                     f"å¤±æ•—={result['failed_count']}, "
                     f"ã‚¹ã‚­ãƒƒãƒ—={result['skipped_count']}, "
                     f"ç·æ•°={result['total']}")
        logger.info(result_msg)
        
        if progress_callback:
            progress_callback(100, 100, result_msg)
        
        if result['cancelled']:
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        return result_msg
        
    except Exception as e:
        error_msg = f"fetch_all_invoices_infoå‡¦ç†å¤±æ•—: {e}"
        logger.error(error_msg)
        if progress_callback:
            progress_callback(100, 100, f"ã‚¨ãƒ©ãƒ¼: {error_msg}")
        raise


def fetch_dataset_info_respectively_from_api(bearer_token, dataset_id, output_dir=None):
    """æŒ‡å®šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDã®ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±ã‚’APIã‹ã‚‰å–å¾—ã—ã€{dataset_id}.jsonã¨ã—ã¦ä¿å­˜"""
    url = f"https://rde-api.nims.go.jp/datasets/{dataset_id}?updateViews=true&include=releases%2Capplicant%2Cprogram%2Cmanager%2CrelatedDatasets%2Ctemplate%2Cinstruments%2Clicense%2CsharingGroups&fields%5Brelease%5D=id%2CreleaseNumber%2Cversion%2Cdoi%2Cnote%2CreleaseTime&fields%5Buser%5D=id%2CuserName%2CorganizationName%2CisDeleted&fields%5Bgroup%5D=id%2Cname&fields%5BdatasetTemplate%5D=id%2CnameJa%2CnameEn%2Cversion%2CdatasetType%2CisPrivate%2CworkflowEnabled&fields%5Binstrument%5D=id%2CnameJa%2CnameEn%2Cstatus&fields%5Blicense%5D=id%2Curl%2CfullName"

    headers = _make_headers(bearer_token, host="rde-api.nims.go.jp", origin="https://rde.nims.go.jp", referer="https://rde.nims.go.jp/")
    
    try:
        logger.debug(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©³ç´°å–å¾—é–‹å§‹: dataset_id={dataset_id}")
        resp = api_request("GET", url, bearer_token=bearer_token, headers=headers, timeout=10)  # refactored to use api_request_helper
        if resp is None:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©³ç´°å–å¾—å¤±æ•—: dataset_id={dataset_id}")
            return
        resp.raise_for_status()
        data = resp.json()
        
        target_dir = output_dir or get_dynamic_file_path("output/rde/data/datasets")
        os.makedirs(target_dir, exist_ok=True)
        save_path = os.path.join(target_dir, f"{dataset_id}.json")
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©³ç´°å–å¾—ãƒ»ä¿å­˜å®Œäº†: {dataset_id}.json -> {save_path}")
        
    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©³ç´°å–å¾—ãƒ»ä¿å­˜å¤±æ•—: dataset_id={dataset_id}, error={e}")
        raise

# --- APIå–å¾—ç³» ---
def fetch_all_dataset_info(
    bearer_token,
    output_dir=None,
    onlySelf=False,
    searchWords=None,
    searchWordsBatch: Optional[List[str]] = None,
    progress_callback: Optional[Callable[[int, int, str], bool]] = None,
    parallel_threshold: int = 20,
    max_workers: int = 8,
):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’APIã‹ã‚‰å–å¾—ã—ã€dataset.jsonã¨ã—ã¦ä¿å­˜ã—ã¤ã¤é€²æ—ã‚’é€šçŸ¥ã™ã‚‹"""
    user_name = get_self_username_from_json()

    # ãƒ‘ã‚¹åŒºåˆ‡ã‚Šã‚’çµ±ä¸€
    output_dir = os.path.normpath(output_dir or OUTPUT_RDE_DATA_DIR)
    detail_dir = os.path.join(output_dir, "datasets")

    search_query = None
    if onlySelf is True:
        if searchWords and len(searchWords) > 0:
            logger.debug("searchWords: %s", searchWords)
            search_query = searchWords
        else:
            logger.debug("UserName: %s", user_name)
            search_query = user_name

    sanitized_batch: List[str] = []
    if searchWordsBatch:
        seen = set()
        for word in searchWordsBatch:
            normalized = (word or "").strip()
            if normalized and normalized not in seen:
                sanitized_batch.append(normalized)
                seen.add(normalized)

    search_targets: List[Optional[str]] = []
    if sanitized_batch:
        search_targets.extend(sanitized_batch)
    elif search_query is not None:
        search_targets.append(search_query)
    else:
        search_targets.append(None)

    headers = _make_headers(bearer_token, host="rde-api.nims.go.jp", origin="https://rde.nims.go.jp", referer="https://rde.nims.go.jp/")
    dataset_payload: Dict = {}
    emit_progress = progress_callback if progress_callback else lambda *_args, **_kwargs: True
    try:
        start_detail = f"æ¤œç´¢ã‚»ãƒƒãƒˆ: {len(search_targets)}ä»¶, " if len(search_targets) > 1 else ""
        start_message = f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§å–å¾—ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™ ({start_detail}ä¸¦åˆ—: è‡ªå‹•)"
        if not emit_progress(0, 100, start_message):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        target_payloads: List[Dict] = []
        for idx, target in enumerate(search_targets, start=1):
            if target:
                label = target
            else:
                label = "ãƒ¦ãƒ¼ã‚¶ãƒ¼å" if onlySelf else "å…¨ä»¶"
            logger.info(
                "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§: æ¤œç´¢%02d/%02dã‚’å®Ÿè¡Œä¸­ (æ¡ä»¶=%s)",
                idx,
                len(search_targets),
                label,
            )
            chunk_payload = _download_dataset_list_in_chunks(
                bearer_token=bearer_token,
                headers=headers,
                search_words=target,
            )
            target_payloads.append(chunk_payload)

        if len(target_payloads) == 1:
            dataset_payload = target_payloads[0]
        else:
            dataset_payload = _merge_dataset_search_payloads(target_payloads)

        meta = dict(dataset_payload.get("meta") or {})
        if sanitized_batch:
            meta["searchWordsBatch"] = sanitized_batch
        elif search_query:
            meta["searchWords"] = search_query
        dataset_payload["meta"] = meta

        os.makedirs(output_dir, exist_ok=True)
        save_path = os.path.join(output_dir, "dataset.json")
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(dataset_payload, f, ensure_ascii=False, indent=2)

        logger.info("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±(dataset.json)å–å¾—ãƒ»ä¿å­˜å®Œäº†")

    except Exception as e:
        logger.error("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±å–å¾—ãƒ»ä¿å­˜å¤±æ•—: %s (searchTargets=%s)", e, search_targets)
        raise

    datasets = dataset_payload.get("data", [])
    total_datasets = len(datasets)
    if not emit_progress(5, 100, f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§å–å¾—å®Œäº† (è¨ˆç”»: {total_datasets}ä»¶, ä¸¦åˆ—: è‡ªå‹•)"):
        return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

    datasets_with_meta = []
    for ds in datasets:
        ds_id = ds.get("id")
        attr = ds.get("attributes", {})
        modified_at = attr.get("modified", "")
        modified_dt = parse_datetime(modified_at) if modified_at else None
        datasets_with_meta.append((ds_id, modified_dt))

    # å–å¾—ãŒå¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä»¶æ•°ã‚’å…ˆã«æ•°ãˆã‚‹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨æ™‚ã‚‚ç·æ•°è¡¨ç¤ºç”¨ï¼‰
    fetch_targets = []
    for ds_id, modified_dt in datasets_with_meta:
        if not ds_id or not modified_dt:
            continue

        detail_path = os.path.join(detail_dir, f"{ds_id}.json")
        file_mtime = datetime.fromtimestamp(os.path.getmtime(detail_path), timezone.utc) if os.path.exists(detail_path) else None
        needs_fetch = file_mtime is None or file_mtime < modified_dt
        fetch_targets.append((ds_id, modified_dt, detail_path, needs_fetch))

    total_targets = len(fetch_targets)
    total_fetch_targets = sum(1 for _ds_id, _modified_dt, _path, need in fetch_targets if need)
    parallel_enabled = total_fetch_targets >= parallel_threshold and total_fetch_targets > 0

    if not emit_progress(
        6,
        100,
        f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©³ç´°å–å¾—æº–å‚™ (è¨ˆç”»: {total_targets}ä»¶, å–å¾—å¯¾è±¡: {total_fetch_targets}ä»¶, ä¸¦åˆ—: {'æœ‰åŠ¹' if parallel_enabled else 'ç„¡åŠ¹'})",
    ):
        return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

    fetched_count = 0

    if parallel_enabled:
        tasks = [
            (bearer_token, ds_id, detail_dir)
            for ds_id, _modified_dt, _path, needs_fetch in fetch_targets
            if needs_fetch and ds_id
        ]

        def detail_worker(token, ds_id, out_dir):
            try:
                fetch_dataset_info_respectively_from_api(token, ds_id, output_dir=out_dir)
                return "success"
            except Exception as worker_error:
                logger.error("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©³ç´°å–å¾—å¤±æ•— (ä¸¦åˆ—): %s", worker_error)
                return "failed"

        def detail_progress(current, total, message):
            # é€²æ—ã¯ä»¶æ•°ãƒ™ãƒ¼ã‚¹ã§é€šçŸ¥ï¼ˆshow_progress_dialog å´ã§ current/total/ETA ã‚’è¡¨ç¤ºï¼‰
            return emit_progress(int(current), int(total), f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©³ç´°å–å¾—ä¸­ (ä¸¦åˆ—: æœ‰åŠ¹) {message}")

        try:
            from net.http_helpers import parallel_download

            result = parallel_download(
                tasks=tasks,
                worker_function=detail_worker,
                max_workers=max_workers,
                progress_callback=detail_progress,
                threshold=1,
                progress_mode="count",
            )
        except Exception as parallel_error:
            logger.error("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©³ç´°ä¸¦åˆ—å–å¾—ã§ã‚¨ãƒ©ãƒ¼: %s", parallel_error)
            result = {"success_count": 0, "failed_count": total_fetch_targets, "cancelled": False}

        if result.get("cancelled"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        fetched_count = result.get("success_count", 0)
        if not emit_progress(95, 100, f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©³ç´°å–å¾—å®Œäº† (ä¸¦åˆ—: æœ‰åŠ¹, æˆåŠŸ: {fetched_count}/{total_fetch_targets})"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
    else:
        processed_count = 0
        for ds_id, modified_dt, detail_path, needs_fetch in fetch_targets:
            processed_count += 1
            if not ds_id:
                continue

            try:
                if needs_fetch:
                    fetch_dataset_info_respectively_from_api(bearer_token, ds_id, output_dir=detail_dir)
                    fetched_count += 1
                else:
                    logger.info("%s.jsonã¯æœ€æ–°ã§ã™ã€‚å†å–å¾—ã¯è¡Œã„ã¾ã›ã‚“ã€‚", ds_id)
            except Exception as e:
                logger.error("ds_id=%s ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: %s", ds_id, e)

            denominator = total_targets if total_targets else 1
            progress_percent = 5 + int((processed_count / denominator) * 90)
            status_message = (
                f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©³ç´°å‡¦ç† {processed_count}/{denominator}"
                f" (å–å¾—å¯¾è±¡: {fetched_count}/{total_fetch_targets}, ä¸¦åˆ—: ç„¡åŠ¹)"
            )
            if not emit_progress(progress_percent, 100, status_message):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

    final_parallel = "æœ‰åŠ¹" if parallel_enabled else "ç„¡åŠ¹"
    if not emit_progress(
        100,
        100,
        f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†å®Œäº† (è¨ˆç”»: {total_datasets}ä»¶, å®Ÿè¡Œ: {fetched_count}ä»¶, ä¸¦åˆ—: {final_parallel})",
    ):
        return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"


def fetch_instrument_type_info_from_api(bearer_token, save_path):
    """
    è£…ç½®ã‚¿ã‚¤ãƒ—æƒ…å ±ã‚’APIã‹ã‚‰å–å¾—ã—ã€instrumentType.jsonã¨ã—ã¦ä¿å­˜
    v1.18.4: Bearer Tokenè‡ªå‹•é¸æŠžå¯¾å¿œ
    """
    import time
    start_time = time.time()
    
    url = "https://rde-instrument-api.nims.go.jp/typeTerms?programId=4bbf62be-f270-4a46-9682-38cd064607ba"
    headers = {
        "Accept": "application/vnd.api+json",
        "Host": "rde-instrument-api.nims.go.jp",
        "Origin": "https://rde-entry-arim.nims.go.jp",
        "Referer": "https://rde-entry-arim.nims.go.jp/"
    }
    try:
        # v1.18.4: bearer_token=Noneã§è‡ªå‹•é¸æŠžã•ã›ã‚‹
        resp = api_request("GET", url, bearer_token=None, headers=headers, timeout=10)
        elapsed_ms = (time.time() - start_time) * 1000
        
        if resp is None:
            # APIè¨˜éŒ²ã‚’è¿½åŠ ï¼ˆå¤±æ•—ï¼‰
            try:
                from classes.basic.core.api_recording_wrapper import record_api_call_for_instrument_type
                record_api_call_for_instrument_type(url, headers, 0, elapsed_ms, False, "APIãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—")
            except Exception as e:
                logger.debug(f"APIè¨˜éŒ²è¿½åŠ å¤±æ•—: {e}")
            
            logger.error("è£…ç½®ã‚¿ã‚¤ãƒ—æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼")
            return
        
        resp.raise_for_status()
        data = resp.json()
        save_json(data, *save_path)
        logger.info("è£…ç½®ã‚¿ã‚¤ãƒ—æƒ…å ±ã®å–å¾—ãƒ»ä¿å­˜ã«æˆåŠŸã—ã¾ã—ãŸ: %s", os.path.join(*save_path))
        
        # APIè¨˜éŒ²ã‚’è¿½åŠ ï¼ˆæˆåŠŸï¼‰
        try:
            from classes.basic.core.api_recording_wrapper import record_api_call_for_instrument_type
            record_api_call_for_instrument_type(url, headers, 200, elapsed_ms, True)
        except Exception as e:
            logger.debug(f"APIè¨˜éŒ²è¿½åŠ å¤±æ•—: {e}")
    except Exception as e:
        elapsed_ms = (time.time() - start_time) * 1000
        
        # APIè¨˜éŒ²ã‚’è¿½åŠ ï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰
        try:
            from classes.basic.core.api_recording_wrapper import record_api_call_for_instrument_type
            record_api_call_for_instrument_type(url, headers, 500, elapsed_ms, False, str(e))
        except Exception as e2:
            logger.debug(f"APIè¨˜éŒ²è¿½åŠ å¤±æ•—: {e2}")
        
        logger.error("è£…ç½®ã‚¿ã‚¤ãƒ—æƒ…å ±ã®å–å¾—ãƒ»ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: %s", e)

def fetch_organization_info_from_api(bearer_token, save_path):
    """
    çµ„ç¹”æƒ…å ±ã‚’APIã‹ã‚‰å–å¾—ã—ã€organization.jsonã¨ã—ã¦ä¿å­˜
    v1.18.4: Bearer Tokenè‡ªå‹•é¸æŠžå¯¾å¿œ
    """
    import time
    start_time = time.time()
    
    url = "https://rde-instrument-api.nims.go.jp/organizations"
    headers = {
        "Accept": "application/vnd.api+json",
        "Host": "rde-instrument-api.nims.go.jp",
        "Origin": "https://rde-entry-arim.nims.go.jp",
        "Referer": "https://rde-entry-arim.nims.go.jp/"
    }
    try:
        # v1.18.4: bearer_token=Noneã§è‡ªå‹•é¸æŠžã•ã›ã‚‹
        resp = api_request("GET", url, bearer_token=None, headers=headers, timeout=10)
        elapsed_ms = (time.time() - start_time) * 1000
        
        if resp is None:
            # APIè¨˜éŒ²ã‚’è¿½åŠ ï¼ˆå¤±æ•—ï¼‰
            try:
                from classes.basic.core.api_recording_wrapper import record_api_call_for_organization
                record_api_call_for_organization(url, headers, 0, elapsed_ms, False, "APIãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—")
            except Exception as e:
                logger.debug(f"APIè¨˜éŒ²è¿½åŠ å¤±æ•—: {e}")
            
            logger.error("çµ„ç¹”æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼")
            return
        
        resp.raise_for_status()
        data = resp.json()
        save_json(data, *save_path)
        logger.info("çµ„ç¹”æƒ…å ±ã®å–å¾—ãƒ»ä¿å­˜ã«æˆåŠŸã—ã¾ã—ãŸ: %s", os.path.join(*save_path))
        
        # APIè¨˜éŒ²ã‚’è¿½åŠ ï¼ˆæˆåŠŸï¼‰
        try:
            from classes.basic.core.api_recording_wrapper import record_api_call_for_organization
            record_api_call_for_organization(url, headers, 200, elapsed_ms, True)
        except Exception as e:
            logger.debug(f"APIè¨˜éŒ²è¿½åŠ å¤±æ•—: {e}")
    except Exception as e:
        elapsed_ms = (time.time() - start_time) * 1000
        
        # APIè¨˜éŒ²ã‚’è¿½åŠ ï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰
        try:
            from classes.basic.core.api_recording_wrapper import record_api_call_for_organization
            record_api_call_for_organization(url, headers, 500, elapsed_ms, False, str(e))
        except Exception as e2:
            logger.debug(f"APIè¨˜éŒ²è¿½åŠ å¤±æ•—: {e2}")
        
        logger.error("çµ„ç¹”æƒ…å ±ã®å–å¾—ãƒ»ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: %s", e)


def fetch_template_info_from_api(bearer_token, output_dir=None, progress_callback=None):
    """
    ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæƒ…å ±ã‚’APIã‹ã‚‰å–å¾—ã—ã€template.jsonã¨ã—ã¦ä¿å­˜
    v1.18.4: Bearer Tokenè‡ªå‹•é¸æŠžå¯¾å¿œ
    """
    headers = {
        "Accept": "application/vnd.api+json",
        "Host": "rde-api.nims.go.jp",
        "Origin": "https://rde.nims.go.jp",
        "Referer": "https://rde.nims.go.jp/"
    }
    try:
        target_dir = output_dir or OUTPUT_RDE_DATA_DIR
        program_id = _resolve_program_id_for_templates(output_dir=target_dir)
        team_candidates = _iterate_template_team_ids(output_dir=target_dir)

        selected_payload: Optional[Dict] = None
        selected_team_id: Optional[str] = None
        last_payload: Optional[Dict] = None

        for idx, team_id in enumerate(team_candidates, 1):
            logger.info("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå–å¾—: teamIdå€™è£œ(%d/%d)=%s ã‚’è©¦è¡Œã—ã¾ã™", idx, len(team_candidates), team_id)

            if progress_callback:
                _progress_ok(
                    progress_callback,
                    0,
                    0,
                    f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæƒ…å ±: teamIdå€™è£œ({idx}/{len(team_candidates)})={team_id} ã‚’å–å¾—ä¸­...",
                )

            base_params = {
                "programId": program_id,
                "teamId": team_id,
                "sort": "id",
                "include": "instruments",
                "fields[instrument]": "nameJa,nameEn",
            }
            chunk_dir = _prepare_template_chunk_directory()

            def _reuse_chunk_dir(chunk_dir=chunk_dir):
                return chunk_dir

            try:
                payload = _download_paginated_resource(
                    base_url=TEMPLATE_API_BASE_URL,
                    base_params=base_params,
                    headers=headers,
                    bearer_token=None,
                    page_size=TEMPLATE_PAGE_SIZE,
                    timeout=TEMPLATE_REQUEST_TIMEOUT,
                    record_callback=lambda url, hdrs, status_code, elapsed_ms, success, error=None: record_api_call_for_template(
                        url,
                        hdrs,
                        status_code,
                        elapsed_ms,
                        success=success,
                        error=error,
                    ),
                    progress_callback=progress_callback,
                    chunk_label="ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæƒ…å ±",
                    chunk_dir_factory=_reuse_chunk_dir,
                    chunk_file_template=TEMPLATE_CHUNK_FILE_TEMPLATE,
                )
            except GroupFetchCancelled:
                raise
            except Exception as per_team_error:
                logger.warning(
                    "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå–å¾—: teamId=%s ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ¬¡ã®å€™è£œã‚’è©¦è¡Œã—ã¾ã™: %s",
                    team_id,
                    per_team_error,
                )
                if progress_callback:
                    _progress_ok(
                        progress_callback,
                        0,
                        0,
                        f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæƒ…å ±: teamIdå€™è£œ({idx}/{len(team_candidates)}) å¤±æ•—ã€‚æ¬¡ã®å€™è£œã¸...",
                    )
                continue

            last_payload = payload
            if _template_payload_is_preferred(payload):
                selected_payload = payload
                selected_team_id = team_id
                logger.info("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå–å¾—: teamId=%s ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æŽ¡ç”¨ã—ã¾ã—ãŸ", team_id)
                break

            logger.info("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå–å¾—: teamId=%s ã§ã¯æœ‰æ„ãªãƒ‡ãƒ¼ã‚¿ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚æ¬¡ã®å€™è£œã‚’è©¦è¡Œã—ã¾ã™ã€‚", team_id)

        if selected_payload is None:
            if last_payload is None:
                raise RuntimeError("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæƒ…å ±: å…¨teamIdå€™è£œã§å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")

            selected_payload = last_payload
            selected_team_id = team_candidates[-1] if team_candidates else DEFAULT_TEAM_ID
            logger.info(
                "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå–å¾—: æœ‰æ„ãªãƒ‡ãƒ¼ã‚¿ãŒå¾—ã‚‰ã‚Œãªã‹ã£ãŸãŸã‚æœ€å¾Œã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æŽ¡ç”¨ã—ã¾ã™ (teamId=%s)",
                selected_team_id,
            )

        os.makedirs(target_dir, exist_ok=True)
        with open(os.path.join(target_dir, "template.json"), "w", encoding="utf-8") as f:
            json.dump(selected_payload, f, ensure_ascii=False, indent=2)
        logger.info(
            "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ(template.json)ã®å–å¾—ãƒ»ä¿å­˜ã«æˆåŠŸã—ã¾ã—ãŸ (teamId=%s, å€™è£œæ•°=%d)ã€‚",
            selected_team_id,
            len(team_candidates),
        )
    except Exception as e:
        logger.error("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å–å¾—ãƒ»ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: %s", e)

def fetch_instruments_info_from_api(bearer_token, output_dir=None, progress_callback=None):
    """
    è¨­å‚™ãƒªã‚¹ãƒˆæƒ…å ±ã‚’APIã‹ã‚‰å–å¾—ã—ã€instruments.jsonã¨ã—ã¦ä¿å­˜
    v1.18.4: Bearer Tokenè‡ªå‹•é¸æŠžå¯¾å¿œ
    """
    headers = {
        "Accept": "application/vnd.api+json",
        "Host": "rde-instrument-api.nims.go.jp",
        "Origin": "https://rde.nims.go.jp",
        "Referer": "https://rde.nims.go.jp/"
    }
    try:
        if progress_callback:
            _progress_ok(progress_callback, 0, 0, "è¨­å‚™æƒ…å ±å–å¾—ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...")
        base_params = {
            "programId": DEFAULT_PROGRAM_ID,
            "sort": "id",
        }
        data = _download_paginated_resource(
            base_url=INSTRUMENT_API_BASE_URL,
            base_params=base_params,
            headers=headers,
            bearer_token=None,
            page_size=INSTRUMENT_PAGE_SIZE,
            timeout=INSTRUMENT_REQUEST_TIMEOUT,
            record_callback=lambda url, hdrs, status_code, elapsed_ms, success, error=None: record_api_call_for_instruments(
                url,
                hdrs,
                status_code,
                elapsed_ms,
                success=success,
                error=error,
            ),
            progress_callback=progress_callback,
            chunk_label="è¨­å‚™æƒ…å ±",
            chunk_dir_factory=_prepare_instrument_chunk_directory,
            chunk_file_template=INSTRUMENT_CHUNK_FILE_TEMPLATE,
        )
        target_dir = output_dir or OUTPUT_RDE_DATA_DIR
        os.makedirs(target_dir, exist_ok=True)
        with open(os.path.join(target_dir, "instruments.json"), "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info("è¨­å‚™æƒ…å ±(instruments.json)ã®å–å¾—ãƒ»ä¿å­˜ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
    except Exception as e:
        logger.error("è¨­å‚™æƒ…å ±ã®å–å¾—ãƒ»ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: %s", e)

def fetch_licenses_info_from_api(bearer_token, output_dir=None, progress_callback=None):
    """
    åˆ©ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒžã‚¹ã‚¿æƒ…å ±ã‚’APIã‹ã‚‰å–å¾—ã—ã€licenses.jsonã¨ã—ã¦ä¿å­˜
    v1.18.4: Bearer Tokenè‡ªå‹•é¸æŠžå¯¾å¿œ
    """
    import time
    start_time = time.time()
    
    url = "https://rde-api.nims.go.jp/licenses"
    headers = {
        "Accept": "application/vnd.api+json",
        "Host": "rde-api.nims.go.jp",
        "Origin": "https://rde.nims.go.jp",
        "Referer": "https://rde.nims.go.jp/"
    }
    try:
        if progress_callback:
            _progress_ok(progress_callback, 0, 1, "åˆ©ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±å–å¾—ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...")
        # v1.18.4: bearer_token=Noneã§è‡ªå‹•é¸æŠžã•ã›ã‚‹
        resp = api_request("GET", url, bearer_token=None, headers=headers, timeout=10)
        elapsed_ms = (time.time() - start_time) * 1000
        
        if resp is None:
            try:
                from classes.basic.core.api_recording_wrapper import record_api_call_for_licenses
                record_api_call_for_licenses(url, headers, 0, elapsed_ms, False, "APIãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—")
            except Exception as e:
                logger.debug(f"APIè¨˜éŒ²è¿½åŠ å¤±æ•—: {e}")
            logger.error("åˆ©ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼")
            return
        resp.raise_for_status()
        data = resp.json()
        target_dir = output_dir or OUTPUT_RDE_DATA_DIR
        os.makedirs(target_dir, exist_ok=True)
        with open(os.path.join(target_dir, "licenses.json"), "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info("åˆ©ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±(licenses.json)ã®å–å¾—ãƒ»ä¿å­˜ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        logger.info(f"åˆ©ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±å–å¾—å®Œäº†: {len(data.get('data', []))}ä»¶ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹")

        if progress_callback:
            _progress_ok(progress_callback, 1, 1, "åˆ©ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸ")
        try:
            from classes.basic.core.api_recording_wrapper import record_api_call_for_licenses
            record_api_call_for_licenses(url, headers, 200, elapsed_ms, True)
        except Exception as e:
            logger.debug(f"APIè¨˜éŒ²è¿½åŠ å¤±æ•—: {e}")
    except Exception as e:
        elapsed_ms = (time.time() - start_time) * 1000
        try:
            from classes.basic.core.api_recording_wrapper import record_api_call_for_licenses
            record_api_call_for_licenses(url, headers, 500, elapsed_ms, False, str(e))
        except Exception as e2:
            logger.debug(f"APIè¨˜éŒ²è¿½åŠ å¤±æ•—: {e2}")
        logger.error("åˆ©ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±ã®å–å¾—ãƒ»ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: %s", e)
        logger.error(f"åˆ©ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±å–å¾—å¤±æ•—: {e}")


# --- ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±å–å¾—ãƒ»WebViewãƒ»infoç”Ÿæˆ ---
def fetch_group_info_from_api(url, headers, save_path, bearer_token=None):
    import time
    start_time = time.time()
    
    try:
        resp = api_request("GET", url, bearer_token=bearer_token, headers=headers, timeout=10)  # refactored to use api_request_helper
        elapsed_ms = (time.time() - start_time) * 1000
        
        if resp is None:
            # APIè¨˜éŒ²ã‚’è¿½åŠ ï¼ˆå¤±æ•—ï¼‰
            try:
                from classes.basic.core.api_recording_wrapper import record_api_call_for_group
                group_type = "subgroup" if "project_group_id" in url else ("detail" if "groupDetail" in str(save_path) else "root")
                record_api_call_for_group(url, headers, 0, elapsed_ms, group_type, False, "APIãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—")
            except Exception as e:
                logger.debug(f"APIè¨˜éŒ²è¿½åŠ å¤±æ•—: {e}")
            
            raise Exception("ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±å–å¾—å¤±æ•—: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼")
        
        resp.raise_for_status()
        data = resp.json()
        save_json(data, *save_path)
        
        # APIè¨˜éŒ²ã‚’è¿½åŠ ï¼ˆæˆåŠŸï¼‰
        try:
            from classes.basic.core.api_recording_wrapper import record_api_call_for_group
            group_type = "subgroup" if "project_group_id" in url else ("detail" if "groupDetail" in str(save_path) else "root")
            record_api_call_for_group(url, headers, 200, elapsed_ms, group_type, True)
        except Exception as e:
            logger.debug(f"APIè¨˜éŒ²è¿½åŠ å¤±æ•—: {e}")
        
        return data
    except Exception as e:
        elapsed_ms = (time.time() - start_time) * 1000
        
        # APIè¨˜éŒ²ã‚’è¿½åŠ ï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰
        try:
            from classes.basic.core.api_recording_wrapper import record_api_call_for_group
            group_type = "subgroup" if "project_group_id" in url else ("detail" if "groupDetail" in str(save_path) else "root")
            record_api_call_for_group(url, headers, 500, elapsed_ms, group_type, False, str(e))
        except Exception as e2:
            logger.debug(f"APIè¨˜éŒ²è¿½åŠ å¤±æ•—: {e2}")
        
        raise


@dataclass
class GroupFetchResult:
    group_data: Dict
    program_details: Dict[str, Dict]
    project_details: Dict[str, Dict]
    project_groups_by_program: Dict[str, List[Dict]]
    selected_program_id: Optional[str]
    selected_project_id: Optional[str]
    selected_program_data: Optional[Dict]
    selected_project_data: Optional[Dict]
    subgroup_summary: Dict[str, Dict[str, int]]


class GroupFetchCancelled(Exception):
    """ã‚°ãƒ«ãƒ¼ãƒ—éšŽå±¤å–å¾—å‡¦ç†ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼æ“ä½œã§ä¸­æ–­ã•ã‚ŒãŸã“ã¨ã‚’ç¤ºã™å†…éƒ¨ä¾‹å¤–"""


def _extract_group_items(payload: Dict) -> List[Dict]:
    return [item for item in payload.get("included", []) if item.get("type") == "group"]


def run_group_hierarchy_pipeline(
    bearer_token: str,
    parent_widget=None,
    preferred_program_id: Optional[str] = None,
    progress_callback: Optional[Callable[[int, int, str], bool]] = None,
    headers: Optional[Dict[str, str]] = None,
    force_project_dialog: bool = False,
    force_program_dialog: bool = False,
    force_download: bool = False,
    skip_dialog: bool = False,
    max_workers: int = 10,
) -> GroupFetchResult:
    """rootâ†’programâ†’projectâ†’subgroup ã®å–å¾—ãƒ•ãƒ­ãƒ¼ã‚’å…±é€šå®Ÿè£…ã§å®Ÿè¡Œã™ã‚‹
    
    v2.1.22è¿½åŠ :
    - force_downloadå¼•æ•°ã‚’è¿½åŠ ï¼ˆsubGroupså€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®å¼·åˆ¶å–å¾—å¯¾å¿œï¼‰
    """
    from classes.basic.ui.group_selection_dialog import show_group_selection_if_needed

    def emit_progress(percent: int, total_or_message: str, message: str = None):
        """Progress emitter supporting both 2 and 3 argument calls.
        - emit_progress(percent, message) 
        - emit_progress(percent, total, message)
        """
        actual_message = message if message is not None else total_or_message
        if not _progress_ok(progress_callback, percent, 100, actual_message):
            raise GroupFetchCancelled("ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")

    headers = headers or _make_headers(
        bearer_token,
        host="rde-api.nims.go.jp",
        origin="https://rde.nims.go.jp",
        referer="https://rde.nims.go.jp/",
    )

    emit_progress(5, "ãƒ«ãƒ¼ãƒˆã‚°ãƒ«ãƒ¼ãƒ—å–å¾—ä¸­...")
    group_url = "https://rde-api.nims.go.jp/groups/root?include=children%2Cmembers"
    group_json_path = [OUTPUT_DIR, "rde", "data", "group.json"]
    group_data = fetch_group_info_from_api(group_url, headers, group_json_path)

    program_groups = _extract_group_items(group_data)
    if not program_groups:
        raise Exception("Rootãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«å‚ç…§å¯èƒ½ãªãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    program_ids = {item.get("id") for item in program_groups if item.get("id")}
    selected_program_id = preferred_program_id if preferred_program_id in program_ids else None
    if not selected_program_id:
        selection = show_group_selection_if_needed(
            program_groups,
            parent_widget,
            context_name="ãƒ—ãƒ­ã‚°ãƒ©ãƒ ï¼ˆRoot Groupï¼‰",
            force_dialog=force_program_dialog and not skip_dialog,
            preferred_group_id=preferred_program_id,
            remember_context=PROGRAM_SELECTION_CONTEXT,
            auto_select_saved=True if skip_dialog else not force_program_dialog,
        )
        if not selection:
            raise GroupFetchCancelled("ãƒ—ãƒ­ã‚°ãƒ©ãƒ é¸æŠžãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
        selected_program_id = selection["id"]

    emit_progress(15, "ãƒ—ãƒ­ã‚°ãƒ©ãƒ è©³ç´°å–å¾—ä¸­...")
    program_details: Dict[str, Dict] = {}
    selected_program_data: Optional[Dict] = None

    # é€Ÿåº¦æœ€é©åŒ–: ãƒ—ãƒ­ã‚°ãƒ©ãƒ è©³ç´°ã¯é¸æŠžæ¸ˆã¿ã®1ä»¶ã®ã¿å–å¾—
    selected_program_name = "åç§°ä¸æ˜Ž"
    for program in program_groups:
        if program.get("id") == selected_program_id:
            selected_program_name = program.get("attributes", {}).get("name", "åç§°ä¸æ˜Ž")
            break

    detail_url = f"https://rde-api.nims.go.jp/groups/{selected_program_id}?include=children%2Cmembers"
    save_path = [GROUP_PROJECT_DIR, f"{selected_program_id}.json"]
    emit_progress(25, f"ãƒ—ãƒ­ã‚°ãƒ©ãƒ å–å¾—: {selected_program_name[:30]}...")
    selected_program_data = fetch_group_info_from_api(detail_url, headers, save_path)
    if not selected_program_data:
        raise Exception("ãƒ—ãƒ­ã‚°ãƒ©ãƒ è©³ç´°ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
    program_details[selected_program_id] = selected_program_data

    save_json(selected_program_data, GROUP_DETAIL_JSON_PATH)


    project_groups_by_program: Dict[str, List[Dict]] = {}
    program_projects = _extract_group_items(selected_program_data)
    project_groups_by_program[selected_program_id] = program_projects

    if not program_projects:
        raise Exception("é¸æŠžã•ã‚ŒãŸãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«ç´ã¥ããƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    # é€Ÿåº¦æœ€é©åŒ–: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè©³ç´°ã¯é¸æŠžå¾Œã«1ä»¶ã®ã¿å–å¾—
    selection = show_group_selection_if_needed(
        program_projects,
        parent_widget,
        context_name="ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆDetailï¼‰",
        force_dialog=force_project_dialog and not skip_dialog,
        remember_context=PROJECT_SELECTION_CONTEXT,
        auto_select_saved=True if skip_dialog else True,
    )
    if not selection:
        raise GroupFetchCancelled("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé¸æŠžãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
    selected_project_id = selection["id"]

    emit_progress(35, "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè©³ç´°å–å¾—ä¸­...")
    project_details: Dict[str, Dict] = {}
    project_meta: Dict[str, Dict[str, str]] = {}
    selected_project_name = selection.get("attributes", {}).get("name", "åç§°ä¸æ˜Ž")
    detail_url = f"https://rde-api.nims.go.jp/groups/{selected_project_id}?include=children%2Cmembers"
    emit_progress(50, f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå–å¾—: {selected_project_name[:30]}...")
    selected_project_data = fetch_group_info_from_api(
        detail_url,
        headers,
        [GROUP_ORGNIZATION_DIR, f"{selected_project_id}.json"],
    )
    project_details[selected_project_id] = selected_project_data
    project_meta[selected_project_id] = {"name": selected_project_name, "program_id": selected_program_id}

    save_json(selected_project_data, SUBGROUP_JSON_PATH)

    # relationships(parent/children) ã«å¯¾ã™ã‚‹è¿½åŠ è©³ç´°å–å¾—ï¼ˆancestorsä»˜ãï¼‰
    emit_progress(58, "é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—è©³ç´°å–å¾—æº–å‚™ä¸­...")
    fetch_relationship_group_details(
        bearer_token=bearer_token,
        sub_group_data=selected_project_data,
        headers=headers,
        progress_callback=emit_progress,
        base_progress=58,
        progress_range=7,
        destination_dir=SUBGROUP_REL_DETAILS_DIR,
        force_download=force_download,
        max_workers=max_workers,
    )

    emit_progress(60, "ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—è©³ç´°å–å¾—ä¸­...")
    subgroup_summary: Dict[str, Dict[str, int]] = {}
    project_id = selected_project_id
    project_data = selected_project_data
    project_name = project_meta.get(project_id, {}).get("name", "åç§°ä¸æ˜Ž")
    emit_progress(70, f"ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—å±•é–‹: {project_name[:30]}...")
    success, fail, errors = fetch_all_subgroups(
        bearer_token=bearer_token,
        sub_group_data=project_data,
        headers=headers,
        progress_callback=emit_progress,
        base_progress=65,
        progress_range=30,
        destination_dir=SUBGROUP_DETAILS_DIR,
        legacy_dir=LEGACY_SUBGROUP_DETAILS_DIR,
        project_group_id=project_id,
        project_group_name=project_name,
        force_download=force_download,
        max_workers=max_workers,
    )

    subgroup_summary[project_id] = {
        "success": success,
        "fail": fail,
        "errors": len(errors),
        "relationship_success": 0,
        "relationship_fail": 0,
        "relationship_skipped": 0,
    }

    emit_progress(100, "ã‚°ãƒ«ãƒ¼ãƒ—éšŽå±¤å–å¾—å®Œäº†")

    return GroupFetchResult(
        group_data=group_data,
        program_details=program_details,
        project_details=project_details,
        project_groups_by_program=project_groups_by_program,
        selected_program_id=selected_program_id,
        selected_project_id=selected_project_id,
        selected_program_data=selected_program_data,
        selected_project_data=selected_project_data,
        subgroup_summary=subgroup_summary,
    )
def parse_group_id_from_data_old(data, preferred_program_id=None):
    """
    includedé…åˆ—ã‹ã‚‰ã‚°ãƒ«ãƒ¼ãƒ—IDã‚’æŠ½å‡º
    
    Args:
        data: group.jsonç­‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿
        preferred_program_id: å„ªå…ˆã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ID (Noneæ™‚ã¯æœ€åˆã®groupã‚’è¿”ã™)
    
    Returns:
        str: ã‚°ãƒ«ãƒ¼ãƒ—ID
    """
    included = data.get("included", [])
    
    # å„ªå…ˆIDãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯æ¤œç´¢
    if preferred_program_id:
        for item in included:
            if (item.get("type") == "group" and 
                item.get("id") == preferred_program_id):
                return item["id"]
        
        # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯è­¦å‘Š
        logger.warning(f"æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ã‚°ãƒ©ãƒ ID '{preferred_program_id[:20]}...' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æœ€åˆã®groupã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æœ€åˆã®groupã‚’è¿”ã™
    for item in included:
        if item.get("type") == "group" and "id" in item:
            return item["id"]
    
    return None


def fetch_all_subgroups(
    bearer_token: str,
    sub_group_data: dict,
    headers: dict,
    progress_callback=None,
    base_progress: int = 70,
    progress_range: int = 30,
    destination_dir: Optional[str] = None,
    legacy_dir: Optional[str] = None,
    project_group_id: Optional[str] = None,
    project_group_name: Optional[str] = None,
    force_download: bool = False,
    max_workers: int = 10,
):
    """
    è¤‡æ•°ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ã®å€‹åˆ¥è©³ç´°ã‚’ä¸€æ‹¬å–å¾—ã—ã¦ä¿å­˜ï¼ˆv2.1.19æ”¹ä¿®ï¼‰
    
    subGroup.jsonã®includedé…åˆ—ã‹ã‚‰å…¨ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—IDã‚’æŠ½å‡ºã—ã€
    å„ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ã®è©³ç´°æƒ…å ±ã‚’å€‹åˆ¥ã«APIã§å–å¾—ã—ã¦
    output/rde/data/subGroups/{subgroup_id}.json ã«ä¿å­˜ã—ã¾ã™ï¼ˆlegacyäº’æ›ã§subgroups/ã«ã‚‚ä¿å­˜å¯èƒ½ï¼‰ã€‚
    
    v2.1.21æ”¹ä¿®:
    - force_downloadå¼•æ•°ã‚’è¿½åŠ ã€‚Falseæ™‚ã¯æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªï¼‰
    
    Args:
        bearer_token: èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³
        sub_group_data: subGroup.jsonã®ãƒ‡ãƒ¼ã‚¿
        headers: HTTPãƒ˜ãƒƒãƒ€
        progress_callback: ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
        base_progress: ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®é–‹å§‹ä½ç½®ï¼ˆ%ï¼‰
        progress_range: ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®ç¯„å›²ï¼ˆ%ï¼‰
        force_download: Trueæ™‚ã¯æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ãã€Falseæ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—
    
    Returns:
        tuple: (æˆåŠŸæ•°, å¤±æ•—æ•°, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ)
    """
    import time
    from pathlib import Path
    
    resolved_dir = destination_dir
    if not resolved_dir:
        try:
            resolved_dir = get_dynamic_file_path("subgroups")
        except Exception:
            resolved_dir = SUBGROUP_DETAILS_DIR

    target_dir = Path(resolved_dir)
    target_dir.mkdir(parents=True, exist_ok=True)

    legacy_target = None
    if legacy_dir:
        legacy_target = Path(legacy_dir)
        legacy_target.mkdir(parents=True, exist_ok=True)
    
    # includedé…åˆ—ã‹ã‚‰ type="group" ã‹ã¤ groupType="TEAM" ã‚’æŠ½å‡ºï¼ˆv2.1.17ä¿®æ­£ï¼šTEAMã¯ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç¤ºã™ï¼‰
    included = sub_group_data.get("included", [])
    subgroups = [
        item for item in included 
        if item.get("type") == "group" 
        and item.get("attributes", {}).get("groupType") == "TEAM"
    ]

    # included ã«ç„¡ã„å ´åˆã¯ data.relationships.children ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§åˆ©ç”¨ï¼ˆè¦æ±‚ä»•æ§˜: relationshipsãƒ™ãƒ¼ã‚¹ã§å…¨å–å¾—ï¼‰
    if not subgroups:
        rel_ids = _collect_relationship_group_ids(sub_group_data)
        if rel_ids:
            logger.info("includedã«TEAMãŒç„¡ã„ãŸã‚relationshipsã‹ã‚‰ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—IDã‚’è£œå®Œã—ã¾ã™: %sä»¶", len(rel_ids))
            subgroups = [{"id": gid, "attributes": {"name": gid}, "from_relationships": True} for gid in rel_ids]
        else:
            logger.info("ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return (0, 0, [])
    
    logger.info(f"\n[ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—å€‹åˆ¥å–å¾—ãƒ«ãƒ¼ãƒ—é–‹å§‹] v2.1.24")
    logger.info(f"  ðŸ”„ ãƒ«ãƒ¼ãƒ—å‡¦ç†å¯¾è±¡: {len(subgroups)}ä»¶ã®ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—")
    logger.info(f"  ðŸ”§ force_download: {force_download}")
    logger.info(f"  ðŸ’¾ ä¿å­˜å…ˆ: {target_dir}\n")
    
    success_count = 0
    fail_count = 0
    skipped_count = 0
    error_messages = []

    # é€Ÿåº¦æœ€é©åŒ–: æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é™¤å¤–ã—ã€ä»¶æ•°ãŒå¤šã„å ´åˆã¯ä¸¦åˆ—åŒ–
    download_targets: list[tuple[str, str]] = []
    for subgroup in subgroups:
        subgroup_id = subgroup.get("id", "")
        subgroup_name = subgroup.get("attributes", {}).get("name", "åç§°ä¸æ˜Ž")
        if not subgroup_id:
            continue
        save_path = target_dir / f"{subgroup_id}.json"
        if save_path.exists() and not force_download:
            skipped_count += 1
            continue
        download_targets.append((subgroup_id, subgroup_name))

    def _download_one(subgroup_id: str, subgroup_name: str) -> dict:
        save_path = target_dir / f"{subgroup_id}.json"
        if save_path.exists() and not force_download:
            return {"status": "skipped"}

        subgroup_url = f"https://rde-api.nims.go.jp/groups/{subgroup_id}?include=children%2Cmembers"
        start_time = time.time()
        try:
            resp = api_request("GET", subgroup_url, bearer_token=bearer_token, headers=headers, timeout=10)
            elapsed_ms = (time.time() - start_time) * 1000

            if resp is None:
                try:
                    from classes.basic.core.api_recording_wrapper import record_api_call_for_subgroup_detail
                    record_api_call_for_subgroup_detail(
                        subgroup_url,
                        headers,
                        0,
                        elapsed_ms,
                        subgroup_id,
                        subgroup_name,
                        step_index=1,
                        success=False,
                        error="APIãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—",
                    )
                except Exception:
                    pass
                return {"status": "failed", "error": "APIãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—"}

            resp.raise_for_status()
            subgroup_detail = resp.json()

            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(subgroup_detail, f, ensure_ascii=False, indent=2)
            if legacy_target:
                legacy_path = legacy_target / f"{subgroup_id}.json"
                with open(legacy_path, "w", encoding="utf-8") as f:
                    json.dump(subgroup_detail, f, ensure_ascii=False, indent=2)

            try:
                from classes.basic.core.api_recording_wrapper import record_api_call_for_subgroup_detail
                record_api_call_for_subgroup_detail(
                    subgroup_url,
                    headers,
                    200,
                    elapsed_ms,
                    subgroup_id,
                    subgroup_name,
                    step_index=1,
                    success=True,
                )
            except Exception:
                pass

            return {"status": "success"}
        except Exception as e:
            elapsed_ms = (time.time() - start_time) * 1000
            try:
                from classes.basic.core.api_recording_wrapper import record_api_call_for_subgroup_detail
                record_api_call_for_subgroup_detail(
                    subgroup_url,
                    headers,
                    500,
                    elapsed_ms,
                    subgroup_id,
                    subgroup_name,
                    step_index=1,
                    success=False,
                    error=str(e),
                )
            except Exception:
                pass
            return {"status": "failed", "error": str(e)}

    if download_targets:
        from net.http_helpers import parallel_download

        def _pd_progress(progress_percent: int, _total: int, message: str) -> bool:
            mapped_progress = base_progress + int((progress_percent / 100.0) * max(progress_range, 1))
            msg = f"ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—å–å¾—ä¸­... {message}"
            return _progress_ok(progress_callback, mapped_progress, 100, msg)

        result = parallel_download(
            tasks=download_targets,
            worker_function=_download_one,
            max_workers=max(1, int(max_workers)),
            progress_callback=_pd_progress if progress_callback else None,
            threshold=10,
        )

        success_count += int(result.get("success_count", 0))
        fail_count += int(result.get("failed_count", 0))
        skipped_count += int(result.get("skipped_count", 0))

        for item in result.get("errors", []) or []:
            task = item.get("task")
            err = item.get("error")
            if isinstance(task, (list, tuple)) and len(task) >= 2:
                error_messages.append(f"ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ— {task[1]}: {err}")
            else:
                error_messages.append(f"ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—å–å¾—å¤±æ•—: {err}")
    
    # çµæžœã‚µãƒžãƒªãƒ¼
    logger.info(f"\n[ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—å€‹åˆ¥å–å¾—ãƒ«ãƒ¼ãƒ—å®Œäº†] v2.1.24")
    logger.info(f"  âœ… æˆåŠŸ: {success_count}ä»¶")
    logger.info(f"  âŒ å¤±æ•—: {fail_count}ä»¶")
    logger.info(f"  â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: {skipped_count}ä»¶")
    logger.info(f"  ðŸ“Š åˆè¨ˆ: {success_count + fail_count + skipped_count}ä»¶")
    
    if error_messages:
        logger.warning(f"  å¤±æ•—ã—ãŸã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆæœ€åˆ3ä»¶ï¼‰:")
        for err in error_messages[:3]:
            logger.warning(f"    - {err}")
        if len(error_messages) > 3:
            logger.warning(f"    ... ä»– {len(error_messages) - 3}ä»¶")
    
    logger.info("[ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—å€‹åˆ¥å–å¾—ãƒ«ãƒ¼ãƒ—çµ‚äº†]\n")
    
    return (success_count, fail_count, error_messages)


def _collect_relationship_group_ids(sub_group_data: dict) -> list[str]:
    """Extract unique group IDs from parent/children relationships."""
    relationship_ids: list[str] = []

    def _append_id(data_obj: dict | None) -> None:
        if not isinstance(data_obj, dict):
            return
        gid = data_obj.get("id")
        if isinstance(gid, str) and gid and gid not in relationship_ids:
            relationship_ids.append(gid)

    relationships = sub_group_data.get("data", {}).get("relationships", {})
    if not isinstance(relationships, dict):
        return relationship_ids

    _append_id(relationships.get("parent", {}).get("data"))

    children = relationships.get("children", {}).get("data", [])
    if isinstance(children, list):
        for child in children:
            _append_id(child)

    return relationship_ids


def fetch_relationship_group_details(
    bearer_token: str,
    sub_group_data: dict,
    headers: dict,
    progress_callback=None,
    base_progress: int = 85,
    progress_range: int = 10,
    destination_dir: Optional[str] = None,
    force_download: bool = False,
    max_workers: int = 10,
):
    """Fetch additional group details for relationship IDs in subGroup.json.

    The API call uses include=ancestors,members to keep ancestor context and
    membership information. Each response is stored under
    output/rde/data/subGroupsAncestors/{group_id}.json. Existing files are
    preserved unless force_download is True.
    """
    import time
    from pathlib import Path

    resolved_dir = destination_dir or SUBGROUP_REL_DETAILS_DIR

    target_dir = Path(resolved_dir)
    target_dir.mkdir(parents=True, exist_ok=True)

    group_ids = _collect_relationship_group_ids(sub_group_data)
    if not group_ids:
        logger.info("é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—IDãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚è¿½åŠ å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        return (0, 0, 0)

    logger.info("[é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—è©³ç´°å–å¾—é–‹å§‹] å¯¾è±¡: %sä»¶", len(group_ids))

    success_count = 0
    fail_count = 0
    skipped_count = 0

    download_ids: list[str] = []
    for group_id in group_ids:
        save_path = target_dir / f"{group_id}.json"
        if save_path.exists() and not force_download:
            skipped_count += 1
            continue
        download_ids.append(group_id)

    def _download_one(group_id: str) -> dict:
        save_path = target_dir / f"{group_id}.json"
        if save_path.exists() and not force_download:
            return {"status": "skipped"}

        detail_url = f"https://rde-api.nims.go.jp/groups/{group_id}?include=ancestors%2Cmembers"
        start_time = time.time()
        try:
            resp = api_request("GET", detail_url, bearer_token=bearer_token, headers=headers, timeout=10)
            if resp is None:
                return {"status": "failed", "error": "APIãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—"}
            resp.raise_for_status()
            payload = resp.json()
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(payload, f, ensure_ascii=False, indent=2)
            _ = (time.time() - start_time) * 1000
            return {"status": "success"}
        except Exception as e:
            return {"status": "failed", "error": str(e)}

    if download_ids:
        from net.http_helpers import parallel_download

        def _pd_progress(progress_percent: int, _total: int, message: str) -> bool:
            mapped_progress = base_progress + int((progress_percent / 100.0) * max(progress_range, 1))
            msg = f"é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—å–å¾—ä¸­... {message}"
            return _progress_ok(progress_callback, mapped_progress, 100, msg)

        result = parallel_download(
            tasks=[(gid,) for gid in download_ids],
            worker_function=_download_one,
            max_workers=max(1, int(max_workers)),
            progress_callback=_pd_progress if progress_callback else None,
            threshold=10,
        )

        success_count += int(result.get("success_count", 0))
        fail_count += int(result.get("failed_count", 0))
        skipped_count += int(result.get("skipped_count", 0))

    logger.info(
        "[é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—è©³ç´°å–å¾—å®Œäº†] æˆåŠŸ=%s, å¤±æ•—=%s, ã‚¹ã‚­ãƒƒãƒ—=%s",
        success_count,
        fail_count,
        skipped_count,
    )

    return (success_count, fail_count, skipped_count)


def parse_group_id_from_data(data, preferred_program_id=None):
    """
    includedé…åˆ—ã‹ã‚‰ã‚°ãƒ«ãƒ¼ãƒ—IDã‚’æŠ½å‡º
    
    Args:
        data: group.jsonç­‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿
        preferred_program_id: å„ªå…ˆã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ID (Noneæ™‚ã¯æœ€åˆã®groupã‚’è¿”ã™)
    
    Returns:
        str: ã‚°ãƒ«ãƒ¼ãƒ—ID
    """
    included = data.get("included", [])
    
    # å„ªå…ˆIDãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯æ¤œç´¢
    if preferred_program_id:
        for item in included:
            if (item.get("type") == "group" and 
                item.get("id") == preferred_program_id):
                return item["id"]
        
        # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯è­¦å‘Š
        logger.warning(f"æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ã‚°ãƒ©ãƒ ID '{preferred_program_id[:20]}...' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æœ€åˆã®groupã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æœ€åˆã®groupã‚’è¿”ã™
    for item in included:
        if item.get("type") == "group" and "id" in item:
            return item["id"]
    
    return None


def move_webview_to_group(webview, project_group_id):
    import traceback
    logger.debug("move_webview_to_group: called with webview=%s, project_group_id=%s", webview, project_group_id)
    logger.debug("type(webview)=%s, type(project_group_id)=%s", type(webview), type(project_group_id))
    logger.debug("move_webview_to_group")
    try:
        logger.debug("webview: %s", webview)
        logger.debug("project_group_id: %s", project_group_id)
        # webviewã®åž‹ãƒ»çŠ¶æ…‹ã‚’è©³ç´°ã«å‡ºåŠ›
        logger.debug("type(webview): %s", type(webview))
        logger.debug("dir(webview): %s", dir(webview))
        logger.debug("webview is None: %s", webview is None)
        try:
            logger.debug("webview.isWidgetType: %s", getattr(webview, 'isWidgetType', lambda: 'N/A')())
        except Exception as e:
            logger.debug("webview.isWidgetType error: %s", e)
        try:
            logger.debug("webview.isVisible: %s", getattr(webview, 'isVisible', lambda: 'N/A')())
        except Exception as e:
            logger.debug("webview.isVisible error: %s", e)
        try:
            logger.debug("webview.isEnabled: %s", getattr(webview, 'isEnabled', lambda: 'N/A')())
        except Exception as e:
            logger.debug("webview.isEnabled error: %s", e)
        try:
            logger.debug("webview.metaObject: %s", getattr(webview, 'metaObject', lambda: 'N/A')())
        except Exception as e:
            logger.debug("webview.metaObject error: %s", e)
        if webview is None:
            logger.error("webview is None")
            return
        # setUrlå‰ã«webviewã‚’æœ‰åŠ¹åŒ–
        try:
            if hasattr(webview, 'setEnabled'):
                webview.setEnabled(True)
                logger.debug("setEnabled(True) å®Ÿè¡Œ")
        except Exception as e:
            logger.error("setEnabledä¾‹å¤–: %s", e)
        # setUrlã‚’UIã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
        url = f"https://rde.nims.go.jp/rde/datasets/groups/{project_group_id}"
        logger.debug("setUrlå‰: %s", url)
        try:
            from qt_compat.core import QTimer
            def do_set_url(wv, u):
                try:
                    if wv is None:
                        logger.error("do_set_url: webview is None")
                        return
                    wv.setUrl(u)
                    logger.debug("setUrlå¾Œ: æ­£å¸¸ã«setUrlå‘¼ã³å‡ºã—å®Œäº†")
                except Exception as e:
                    import traceback
                    logger.error("setUrlä¾‹å¤–: %s", e)
                    traceback.print_exc()
            QTimer.singleShot(0, lambda: do_set_url(webview, url))
        except Exception as e:
            import traceback
            logger.error("setUrlãƒ©ãƒƒãƒ—ä¾‹å¤–: %s", e)
            traceback.print_exc()
    except Exception as e:
        import traceback
        logger.error("move_webview_to_groupä¾‹å¤–: %s", e)
        traceback.print_exc()
    else:
        logger.error("move_webview_to_group: webview is None")

def extract_users_and_subgroups(sub_data):
    users = []
    subgroups = []
    for item in sub_data.get('included', []):
        if item.get('type') == 'user':
            attrs = item.get('attributes', {})
            users.append({
                'userId': item.get('id'),
                'userName': attrs.get('userName'),
                'email': attrs.get('emailAddress'),
                'familyName': attrs.get('familyName'),
                'givenName': attrs.get('givenName'),
                'organizationName': attrs.get('organizationName'),
            })
        elif item.get('type') == 'group':
            attrs = item.get('attributes', {})
            subgroups.append({
                'groupId': item.get('id'),
                'name': attrs.get('name'),
                'groupType': attrs.get('groupType'),
                'description': attrs.get('description'),
            })
    return users, subgroups

def show_fetch_confirmation_dialog(parent, onlySelf, searchWords, searchWordsList: Optional[List[str]] = None):
    """
    åŸºæœ¬æƒ…å ±å–å¾—ã®ç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’è¡¨ç¤º
    """
    from qt_compat.widgets import QMessageBox, QPushButton, QDialog, QVBoxLayout, QTextEdit
    import json
    
    # å–å¾—å¯¾è±¡ã®æƒ…å ±ã‚’ç”Ÿæˆ
    fetch_mode = "æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰" if onlySelf else "å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–å¾—ãƒ¢ãƒ¼ãƒ‰"
    keyword_lines = None
    if searchWordsList:
        trimmed = [word for word in searchWordsList if word]
        if trimmed:
            limit = 10
            preview = trimmed[:limit]
            keyword_lines = "\n".join(f"â€¢ {word}" for word in preview)
            if len(trimmed) > limit:
                keyword_lines += f"\nâ€¢ ... (ä»–{len(trimmed) - limit}ä»¶)"
            search_text = f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰({len(trimmed)}ä»¶):\n{keyword_lines}"
        elif searchWords:
            search_text = f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {searchWords}"
        else:
            search_text = "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: è‡ªåˆ†ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å"
    elif searchWords:
        search_text = f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {searchWords}"
    else:
        search_text = "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: è‡ªåˆ†ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å"
    
    # äºˆæƒ³ã•ã‚Œã‚‹å‡¦ç†å†…å®¹
    expected_actions = {
        "å…±é€šå–å¾—é …ç›®": [
            "ãƒ¦ãƒ¼ã‚¶ãƒ¼è‡ªèº«æƒ…å ± (self.json)",
            "ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ± (group.json)",
            "ã‚°ãƒ«ãƒ¼ãƒ—è©³ç´°æƒ…å ± (groupDetail.json)",
            "ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ± (subGroup.json)",
            "çµ„ç¹”æƒ…å ± (organization.json)",
            "è£…ç½®ã‚¿ã‚¤ãƒ—æƒ…å ± (instrumentType.json)",
            "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæƒ…å ± (template.json)",
            "è¨­å‚™æƒ…å ± (instruments.json)",
            "çµ±åˆæƒ…å ± (info.json)"
        ],
        "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–¢é€£": [
            "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§ (dataset.json)",
            "å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©³ç´° (datasets/*.json)",
            "ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªæƒ…å ± (dataEntry/*.json)",
            "ã‚¤ãƒ³ãƒœã‚¤ã‚¹æƒ…å ± (invoice/*.json)"
        ]
    }
    
    if onlySelf:
        warning_text = "æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰: æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã¿ã‚’å–å¾—ã—ã¾ã™ã€‚"
        time_estimate = "æŽ¨å®šå‡¦ç†æ™‚é–“: 1-5åˆ†ç¨‹åº¦"
    else:
        warning_text = "å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–å¾—ãƒ¢ãƒ¼ãƒ‰: å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨å€‹åˆ¥JSONã‚’å–å¾—ã—ã¾ã™ã€‚\nâš ï¸ å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«ã‚ˆã‚Šé•·æ™‚é–“ã‚’è¦ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
        time_estimate = "æŽ¨å®šå‡¦ç†æ™‚é–“: 5-30åˆ†ç¨‹åº¦ï¼ˆãƒ‡ãƒ¼ã‚¿é‡ã«ã‚ˆã‚Šå¤‰å‹•ï¼‰"
    
    # è©³ç´°æƒ…å ±ç”¨ã®ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ä½œæˆ
    payload_info = {
        "å–å¾—ãƒ¢ãƒ¼ãƒ‰": fetch_mode,
        "æ¤œç´¢æ¡ä»¶": search_text if onlySelf else "å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
        "å–å¾—é …ç›®": expected_actions,
        "è­¦å‘Š": warning_text,
        "å‡¦ç†æ™‚é–“": time_estimate,
        "å‡ºåŠ›å…ˆ": OUTPUT_RDE_DATA_DIR,
        "APIå‘¼ã³å‡ºã—å…ˆ": [
            "https://rde-user-api.nims.go.jp/users/self",
            "https://rde-api.nims.go.jp/groups/root",
            "https://rde-api.nims.go.jp/datasets",
            "https://rde-instrument-api.nims.go.jp/organizations",
            "https://rde-instrument-api.nims.go.jp/typeTerms",
            "https://rde-api.nims.go.jp/datasetTemplates",
            "https://rde-instrument-api.nims.go.jp/instruments",
            "https://rde-api.nims.go.jp/invoices"
        ]
    }

    if searchWordsList:
        payload_info["æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è¦§"] = searchWordsList
    
    # ç¢ºèªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    confirmation_text = f"""æœ¬å½“ã«åŸºæœ¬æƒ…å ±å–å¾—ã‚’å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ

ãƒ¢ãƒ¼ãƒ‰: {fetch_mode}
{search_text if onlySelf else 'å¯¾è±¡: å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ'}

{warning_text}
{time_estimate}

ã“ã®æ“ä½œã«ã‚ˆã‚Šä»¥ä¸‹ã®æƒ…å ±ãŒå–å¾—ã•ã‚Œã¾ã™ï¼š
â€¢ å…±é€šæƒ…å ±: 9ç¨®é¡žã®JSON
â€¢ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–¢é€£: å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ + ã‚¨ãƒ³ãƒˆãƒªæƒ…å ± + ã‚¤ãƒ³ãƒœã‚¤ã‚¹æƒ…å ±

å‡¦ç†ä¸­ã¯ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒå¿œç­”ã—ãªããªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚"""

    msg_box = QMessageBox(parent)
    msg_box.setWindowTitle("åŸºæœ¬æƒ…å ±å–å¾—ã®ç¢ºèª")
    msg_box.setIcon(QMessageBox.Question)
    msg_box.setText(confirmation_text)
    yes_btn = msg_box.addButton(QMessageBox.Yes)
    no_btn = msg_box.addButton(QMessageBox.No)
    detail_btn = QPushButton("è©³ç´°è¡¨ç¤º")
    msg_box.addButton(detail_btn, QMessageBox.ActionRole)
    msg_box.setDefaultButton(no_btn)
    msg_box.setStyleSheet("QLabel{font-family: 'Consolas'; font-size: 10pt;}")

    def show_detail():
        dlg = QDialog(parent)
        dlg.setWindowTitle("å–å¾—æƒ…å ± è©³ç´°è¡¨ç¤º")
        layout = QVBoxLayout(dlg)
        text_edit = QTextEdit(dlg)
        text_edit.setReadOnly(True)
        text_edit.setPlainText(json.dumps(payload_info, ensure_ascii=False, indent=2))
        text_edit.setMinimumSize(600, 400)
        layout.addWidget(text_edit)
        dlg.setLayout(layout)
        dlg.exec()
    
    detail_btn.clicked.connect(show_detail)
    
    reply = msg_box.exec()
    return msg_box.clickedButton() == yes_btn

# === æ®µéšŽåˆ¥å®Ÿè¡Œé–¢æ•° ===

@stage_error_handler("ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—")
def fetch_user_info_stage(bearer_token=None, progress_callback=None, parent_widget=None):
    """æ®µéšŽ1: ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—"""
    if progress_callback:
        if not progress_callback(10, 100, "ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—ä¸­..."):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
    
    if not fetch_self_info_from_api(bearer_token, parent_widget=parent_widget):
        return "ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"
    
    if progress_callback:
        if not progress_callback(100, 100, "ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—å®Œäº†"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
    
    return "ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸ"

@stage_error_handler("ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±å–å¾—")
def fetch_group_info_stage(
    bearer_token,
    progress_callback=None,
    program_id=None,
    parent_widget=None,
    force_program_dialog: bool = False,
    force_download: bool = False,
    force_refresh_subgroup: bool = False,
    skip_dialog: bool = False,
    max_workers: int = 10,
):
    """
    æ®µéšŽ2: ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±å–å¾—ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—ãƒ»è©³ç´°ãƒ»ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰
    
    v2.1.16è¿½åŠ :
    - program_idå¼•æ•°ã‚’è¿½åŠ ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—é¸æŠžæ©Ÿèƒ½å¯¾å¿œï¼‰
    
    v2.1.17è¿½åŠ :
    - parent_widgetå¼•æ•°ã‚’è¿½åŠ ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—é¸æŠžãƒ€ã‚¤ã‚¢ãƒ­ã‚°è¡¨ç¤ºç”¨ï¼‰
    - group.json/groupDetail.jsonå–å¾—å¾Œã«ã‚°ãƒ«ãƒ¼ãƒ—é¸æŠžãƒ€ã‚¤ã‚¢ãƒ­ã‚°çµ±åˆ
    - è¤‡æ•°ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ã®å€‹åˆ¥è©³ç´°å–å¾—æ©Ÿèƒ½ï¼ˆoutput/rde/data/subGroups/ï¼‰

    v2.1.20è¿½åŠ :
    - force_program_dialogå¼•æ•°ã‚’è¿½åŠ ï¼ˆUX-GROUP-SEL-ALL-FLOWSå¯¾å¿œã€‚ãƒ—ãƒ­ã‚°ãƒ©ãƒ é¸æŠžã‚’å¿…ãšè¡¨ç¤ºï¼‰
    
    v2.1.22è¿½åŠ :
    - subGroups/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«æ¬ ææ¤œå‡ºæ©Ÿèƒ½
    
    v2.2.10è¿½åŠ :
    - skip_dialogå¼•æ•°ã‚’è¿½åŠ ï¼ˆã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—è‡ªå‹•æ›´æ–°æ™‚ã«ç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’æŠ‘æ­¢ï¼‰
    """
    try:
        force_project_dialog = os.environ.get('FORCE_PROJECT_GROUP_DIALOG', '0') == '1'
        
        # ãƒ­ã‚°å‡ºåŠ›ï¼šå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ç¢ºèª
        if skip_dialog:
            logger.info("[è‡ªå‹•æ›´æ–°ãƒ¢ãƒ¼ãƒ‰] ã‚°ãƒ«ãƒ¼ãƒ—é¸æŠžãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’æŠ‘æ­¢ã—ã€ä¿å­˜æ¸ˆã¿é¸æŠžã‚’è‡ªå‹•é©ç”¨ã—ã¾ã™")
        else:
            logger.info("[é€šå¸¸ãƒ¢ãƒ¼ãƒ‰] ã‚°ãƒ«ãƒ¼ãƒ—é¸æŠžãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’è¡¨ç¤ºã—ã¾ã™")

        def stage_progress(percent: int, total: int, message: str):
            if progress_callback:
                return progress_callback(percent, total, message)
            return True

        # 3ã¤ã®ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ« + subGroups/ãƒ•ã‚©ãƒ«ãƒ€ã®å®Œå…¨æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        group_files_ready = all(
            Path(path).exists() for path in (GROUP_JSON_PATH, GROUP_DETAIL_JSON_PATH, SUBGROUP_JSON_PATH)
        )
        subgroups_complete = _subgroups_folder_complete() if group_files_ready else False

        reuse_allowed = not force_download and not force_refresh_subgroup
        if reuse_allowed and group_files_ready and subgroups_complete:
            logger.info("ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±: æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã¯å®Œå…¨ã€‚å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            # æ—¢å­˜ã®subGroup.jsonã‹ã‚‰é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—è©³ç´°ã‚’è£œå®Œï¼ˆancestorsä»˜ä¸Žï¼‰
            try:
                with open(SUBGROUP_JSON_PATH, "r", encoding="utf-8") as f:
                    existing_subgroup = json.load(f)
            except Exception as e:
                logger.warning("æ—¢å­˜subGroup.jsonã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: %s", e)
                existing_subgroup = None

            if existing_subgroup:
                headers = _make_headers(
                    bearer_token,
                    host="rde-api.nims.go.jp",
                    origin="https://rde.nims.go.jp",
                    referer="https://rde.nims.go.jp/",
                )
                fetch_relationship_group_details(
                    bearer_token=bearer_token,
                    sub_group_data=existing_subgroup,
                    headers=headers,
                    progress_callback=progress_callback,
                    base_progress=85,
                    progress_range=10,
                    destination_dir=SUBGROUP_REL_DETAILS_DIR,
                    force_download=False,
                    max_workers=max_workers,
                )

            if progress_callback:
                progress_callback(100, 100, "æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†åˆ©ç”¨ã—ã¾ã—ãŸ")
            return "ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±: æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†åˆ©ç”¨ã—ã¾ã—ãŸ"

        result = run_group_hierarchy_pipeline(
            bearer_token=bearer_token,
            parent_widget=parent_widget,
            preferred_program_id=program_id,
            progress_callback=stage_progress,
            force_project_dialog=force_project_dialog,
            force_program_dialog=force_program_dialog,
            force_download=force_download,
            skip_dialog=skip_dialog,
            max_workers=max_workers,
        )

        total_success = sum(item.get("success", 0) for item in result.subgroup_summary.values())
        total_fail = sum(item.get("fail", 0) for item in result.subgroup_summary.values())
        result_msg = (
            f"ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸï¼ˆã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—: æˆåŠŸ {total_success}ä»¶, å¤±æ•— {total_fail}ä»¶ï¼‰"
        )
        logger.info(result_msg)
        
        # ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±å–å¾—å¾Œã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
        _clear_dataset_entry_cache()
        
        if progress_callback:
            progress_callback(100, 100, "ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±å–å¾—å®Œäº†")
        return result_msg

    except GroupFetchCancelled:
        logger.info("ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±å–å¾—ã¯ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
        return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
    except Exception as e:
        error_msg = f"ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±å–å¾—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
        logger.error(error_msg)
        logger.error(traceback.format_exc())
        return error_msg


@stage_error_handler("çµ„ç¹”ãƒ»è£…ç½®æƒ…å ±å–å¾—")
def fetch_organization_stage(bearer_token, progress_callback=None):
    """æ®µéšŽ3: çµ„ç¹”ãƒ»è£…ç½®æƒ…å ±å–å¾—"""
    if progress_callback:
        if not progress_callback(20, 100, "çµ„ç¹”æƒ…å ±å–å¾—ä¸­..."):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
    
    org_json_path = [OUTPUT_DIR, "rde", "data", "organization.json"]
    fetch_organization_info_from_api(bearer_token, org_json_path)
    
    if progress_callback:
        if not progress_callback(70, 100, "è£…ç½®ã‚¿ã‚¤ãƒ—æƒ…å ±å–å¾—ä¸­..."):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
    
    instrument_type_json_path = [OUTPUT_DIR, "rde", "data", "instrumentType.json"]
    fetch_instrument_type_info_from_api(bearer_token, instrument_type_json_path)
    
    if progress_callback:
        if not progress_callback(100, 100, "çµ„ç¹”ãƒ»è£…ç½®æƒ…å ±å–å¾—å®Œäº†"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
    
    return "çµ„ç¹”ãƒ»è£…ç½®æƒ…å ±å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸ"

@stage_error_handler("ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å–å¾—")
def fetch_sample_info_stage(bearer_token, progress_callback=None, max_workers: int = 10):
    """
    æ®µéšŽ4: ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å–å¾—
    v2.1.1: ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾å¿œï¼ˆ50ä»¶ä»¥ä¸Šã§è‡ªå‹•ä¸¦åˆ—åŒ–ï¼‰
    """
    if progress_callback:
        if not progress_callback(10, 100, "ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±ç¢ºèªä¸­..."):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
    
    # ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±ã‹ã‚‰å¯¾è±¡ã‚°ãƒ«ãƒ¼ãƒ—IDã‚’å–å¾—
    if not os.path.exists(SUBGROUP_JSON_PATH):
        return "ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±ã‚’å–å¾—ã—ã¦ãã ã•ã„ã€‚"
    
    with open(SUBGROUP_JSON_PATH, "r", encoding="utf-8") as f:
        sub_group_data = json.load(f)
        
        sub_group_included = sub_group_data.get("included", [])
        sample_dir = os.path.join(OUTPUT_DIR, "rde", "data", "samples")
        os.makedirs(sample_dir, exist_ok=True)
        
        total_samples = len(sub_group_included)
        
        if progress_callback:
            if not progress_callback(15, 100, f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å–å¾—æº–å‚™ä¸­... ({total_samples}ä»¶)"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        # Material APIç”¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ˜Žç¤ºçš„ã«å–å¾—
        from config.common import load_bearer_token
        material_token = load_bearer_token('rde-material.nims.go.jp')
        
        # ä¸¦åˆ—åŒ–ç”¨ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆä½œæˆ
        tasks = [
            (material_token, included.get("id", ""), sample_dir)
            for included in sub_group_included
            if included.get("id")
        ]
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ©ãƒƒãƒ‘ãƒ¼
        def sample_progress_callback(current, total, message):
            """ã‚µãƒ³ãƒ—ãƒ«å–å¾—é€²æ—ã‚’é€šçŸ¥ï¼ˆ20-90%ã«ãƒžãƒƒãƒ”ãƒ³ã‚°ï¼‰"""
            if progress_callback:
                # parallel_download()ã‹ã‚‰ã¯(progress_percent, 100, message)ã§å‘¼ã°ã‚Œã‚‹
                # currentã¯0-100ã®ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆå€¤ãªã®ã§ã€20-90%ç¯„å›²ã«ãƒžãƒƒãƒ”ãƒ³ã‚°
                mapped_percent = 20 + int((current / 100.0) * 70)
                return progress_callback(mapped_percent, 100, message)
            return True
        
        # ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œï¼ˆ50ä»¶ä»¥ä¸Šã§è‡ªå‹•ä¸¦åˆ—åŒ–ï¼‰
        from net.http_helpers import parallel_download
        
        result = parallel_download(
            tasks=tasks,
            worker_function=_fetch_single_sample_worker,
            max_workers=max_workers,
            progress_callback=sample_progress_callback,
            threshold=50  # 50ã‚µãƒ³ãƒ—ãƒ«ä»¥ä¸Šã§ä¸¦åˆ—åŒ–
        )
        
        # çµæžœã®é›†è¨ˆ
        success_count = result.get("success_count", 0)
        skipped_count = result.get("skipped_count", 0)
        failed_count = result.get("failed_count", 0)
        cancelled = result.get("cancelled", False)
        errors = result.get("errors", [])
        
        if cancelled:
            logger.warning(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å–å¾—ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ: {success_count}ä»¶æˆåŠŸ, {skipped_count}ä»¶ã‚¹ã‚­ãƒƒãƒ—")
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°å‡ºåŠ›
        if errors:
            logger.error(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å–å¾—ã§ã‚¨ãƒ©ãƒ¼ãŒ{len(errors)}ä»¶ç™ºç”Ÿ:")
            for err in errors[:10]:  # æœ€åˆã®10ä»¶ã®ã¿
                logger.error(f"  - {err}")
    
    if progress_callback:
        if not progress_callback(100, 100, "ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å–å¾—å®Œäº†"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
    
    return f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚æˆåŠŸ: {success_count}ä»¶, ã‚¹ã‚­ãƒƒãƒ—: {skipped_count}ä»¶, å¤±æ•—: {failed_count}ä»¶"

def _fetch_single_sample_worker(material_token, group_id_sample, sample_dir):
    """
    ä¸¦åˆ—å‡¦ç†ç”¨ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°: å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ã®å–å¾—
    
    Args:
        material_token: Material APIèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³
        group_id_sample: ã‚µãƒ³ãƒ—ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ID
        sample_dir: ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        
    Returns:
        str: "success"/"skipped"/"failed"
    """
    try:
        if not group_id_sample:
            return "skipped"
        
        sample_json_path = os.path.join(sample_dir, f"{group_id_sample}.json")
        
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if os.path.exists(sample_json_path):
            logger.debug(f"æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—: {sample_json_path}")
            return "skipped"
        
        url = f"https://rde-material-api.nims.go.jp/samples?groupId={group_id_sample}&page%5Blimit%5D=1000&page%5Boffset%5D=0&fields%5Bsample%5D=names%2Cdescription%2Ccomposition"
        
        headers_sample = _make_headers(
            material_token, 
            host="rde-material-api.nims.go.jp", 
            origin="https://rde-entry-arim.nims.go.jp", 
            referer="https://rde-entry-arim.nims.go.jp/"
        )
        
        resp = api_request("GET", url, bearer_token=material_token, headers=headers_sample, timeout=10)
        
        if resp is None:
            logger.warning(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å–å¾—å¤±æ•— (ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—): {group_id_sample}")
            return "failed"
        
        if resp.status_code == 404:
            logger.debug(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {group_id_sample}")
            return "skipped"
            
        if resp.status_code != 200:
            logger.warning(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å–å¾—å¤±æ•— (HTTP {resp.status_code}): {group_id_sample}")
            return "failed"
        
        resp.raise_for_status()
        data = resp.json()
        
        with open(sample_json_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ä¿å­˜å®Œäº†: {sample_json_path}")
        return "success"
        
    except Exception as e:
        logger.error(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±({group_id_sample})ã®å–å¾—ã«å¤±æ•—: {e}")
        return "failed"


def _fetch_single_sample_worker_force(material_token, group_id_sample, sample_dir, force_download=False):
    """force_downloadå¯¾å¿œç‰ˆã®ã‚µãƒ³ãƒ—ãƒ«å–å¾—ãƒ¯ãƒ¼ã‚«ãƒ¼"""
    try:
        if not group_id_sample:
            return "skipped"

        sample_json_path = os.path.join(sample_dir, f"{group_id_sample}.json")

        if force_download and os.path.exists(sample_json_path):
            try:
                os.remove(sample_json_path)
            except Exception as remove_error:
                logger.debug(f"æ—¢å­˜ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤å¤±æ•—ã‚’ç„¡è¦–: {remove_error}")
        elif not force_download and os.path.exists(sample_json_path):
            logger.debug(f"æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—: {sample_json_path}")
            return "skipped"

        url = (
            "https://rde-material-api.nims.go.jp/samples?"
            f"groupId={group_id_sample}&page%5Blimit%5D=1000&page%5Boffset%5D=0&fields%5Bsample%5D=names%2Cdescription%2Ccomposition"
        )
        headers_sample = _make_headers(
            material_token,
            host="rde-material-api.nims.go.jp",
            origin="https://rde-entry-arim.nims.go.jp",
            referer="https://rde-entry-arim.nims.go.jp/",
        )

        resp = api_request("GET", url, bearer_token=material_token, headers=headers_sample, timeout=10)
        if resp is None:
            logger.warning(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å–å¾—å¤±æ•— (ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—): {group_id_sample}")
            return "failed"

        if resp.status_code == 404:
            logger.debug(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {group_id_sample}")
            return "skipped"

        if resp.status_code != 200:
            logger.warning(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å–å¾—å¤±æ•— (HTTP {resp.status_code}): {group_id_sample}")
            return "failed"

        resp.raise_for_status()
        data = resp.json()

        with open(sample_json_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        logger.info(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ä¿å­˜å®Œäº†: {sample_json_path}")
        return "success"

    except Exception as e:
        logger.error(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±({group_id_sample})ã®å–å¾—ãƒ»ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return f"failed: {e}"

@stage_error_handler("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±å–å¾—")
def fetch_dataset_info_stage(
    bearer_token,
    onlySelf=False,
    searchWords=None,
    searchWordsBatch: Optional[List[str]] = None,
    progress_callback=None,
    max_workers: int = 10,
):
    """æ®µéšŽ5: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±å–å¾—"""
    result = fetch_all_dataset_info(
        bearer_token,
        output_dir=os.path.join(OUTPUT_DIR, "rde", "data"),
        onlySelf=onlySelf,
        searchWords=searchWords,
        searchWordsBatch=searchWordsBatch,
        progress_callback=progress_callback,
        max_workers=max_workers,
    )

    return result or "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸ"

@stage_error_handler("ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±å–å¾—")
def fetch_data_entry_stage(bearer_token, progress_callback=None, max_workers: int = 10):
    """æ®µéšŽ6: ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±å–å¾—"""
    result = fetch_all_data_entrys_info(
        bearer_token,
        output_dir=os.path.join(OUTPUT_DIR, "rde", "data"),
        progress_callback=progress_callback,
        max_workers=max_workers,
    )

    return result or "ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸ"

@stage_error_handler("ã‚¤ãƒ³ãƒœã‚¤ã‚¹æƒ…å ±å–å¾—")
def fetch_invoice_stage(bearer_token, progress_callback=None, max_workers: int = 10):
    """æ®µéšŽ7: ã‚¤ãƒ³ãƒœã‚¤ã‚¹æƒ…å ±å–å¾—"""
    result = fetch_all_invoices_info(
        bearer_token,
        output_dir=os.path.join(OUTPUT_DIR, "rde", "data"),
        progress_callback=progress_callback,
        max_workers=max_workers,
    )

    return result or "ã‚¤ãƒ³ãƒœã‚¤ã‚¹æƒ…å ±å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸ"

@stage_error_handler("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ»è¨­å‚™ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±å–å¾—")
def fetch_template_instrument_stage(bearer_token, progress_callback=None, max_workers: int = 10):
    """æ®µéšŽ7: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ»è¨­å‚™ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±å–å¾—"""
    if progress_callback:
        if not progress_callback(15, 100, "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæƒ…å ±å–å¾—ä¸­..."):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

    def _map_percent(current: int, total: int, start: int, span: int) -> int:
        try:
            c = int(current)
        except Exception:
            c = 0
        try:
            t = int(total)
        except Exception:
            t = 0

        if t <= 0:
            return int(start)
        if c < 0:
            c = 0
        if c > t:
            c = t
        return int(start + int((c / max(t, 1)) * span))

    def template_progress(current, total, message):
        if not progress_callback:
            return True
        mapped = _map_percent(current, total, 15, 35)  # 15% â†’ 50%
        return _progress_ok(progress_callback, mapped, 100, str(message))

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒ³ã‚°é€²æ—ã‚’ 15â†’50% ã«ãƒžãƒƒãƒ—ï¼‰
    fetch_template_info_from_api(bearer_token, progress_callback=template_progress)
    
    # é€Ÿåº¦æœ€é©åŒ–: instruments/licenses ã¯ç‹¬ç«‹ãªã®ã§ä¸¦åˆ—å®Ÿè¡Œ
    if progress_callback:
        if not progress_callback(50, 100, "è¨­å‚™ãƒ»åˆ©ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±å–å¾—ä¸­..."):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

    resolved_workers = 1
    try:
        resolved_workers = max(1, int(max_workers))
    except Exception:
        resolved_workers = 1

    if resolved_workers <= 1:
        def instruments_progress(current, total, message):
            if not progress_callback:
                return True
            mapped = _map_percent(current, total, 50, 35)  # 50% â†’ 85%
            return _progress_ok(progress_callback, mapped, 100, str(message))

        fetch_instruments_info_from_api(bearer_token, progress_callback=instruments_progress)
        if progress_callback:
            if not progress_callback(85, 100, "åˆ©ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±å–å¾—ä¸­..."):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        fetch_licenses_info_from_api(bearer_token)
    else:
        from concurrent.futures import ThreadPoolExecutor, wait, FIRST_COMPLETED
        import threading
        import time

        state_lock = threading.Lock()
        # é€²æ—çŠ¶æ…‹ï¼ˆå„ã‚¿ã‚¹ã‚¯ã¯è‡ªåˆ†ã® state ã‚’æ›´æ–°ã™ã‚‹ã ã‘ã€‚UIã¸ã®åæ˜ ã¯ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ãŒã¾ã¨ã‚ã¦è¡Œã†ï¼‰
        states = {
            "instruments": {"current": 0, "total": 0, "message": "è¨­å‚™æƒ…å ±å–å¾—ä¸­..."},
            "licenses": {"current": 0, "total": 1, "message": "åˆ©ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±å–å¾—ä¸­..."},
        }

        def make_state_updater(name: str):
            def _update(current, total, message):
                with state_lock:
                    try:
                        states[name]["current"] = int(current)
                    except Exception:
                        states[name]["current"] = 0
                    try:
                        states[name]["total"] = int(total)
                    except Exception:
                        states[name]["total"] = 0
                    states[name]["message"] = str(message)
                return True

            return _update

        def instruments_job():
            fetch_instruments_info_from_api(bearer_token, progress_callback=make_state_updater("instruments"))

        def licenses_job():
            # licenses ã¯å˜ç™ºãªã®ã§æ“¬ä¼¼çš„ã« 0/1 â†’ 1/1 ã‚’æ›´æ–°
            updater = make_state_updater("licenses")
            updater(0, 1, "åˆ©ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±å–å¾—ä¸­...")
            fetch_licenses_info_from_api(bearer_token)
            updater(1, 1, "åˆ©ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±å–å¾—å®Œäº†")

        with ThreadPoolExecutor(max_workers=min(resolved_workers, 2)) as executor:
            future_to_name = {
                executor.submit(instruments_job): "instruments",
                executor.submit(licenses_job): "licenses",
            }

            remaining = set(future_to_name.keys())
            completed = 0
            total = len(remaining)

            # ãƒãƒ¼ãƒªãƒ³ã‚°ã—ãªãŒã‚‰é€²æ—è¡¨ç¤ºã‚’æ›´æ–°
            while remaining:
                done, not_done = wait(remaining, timeout=0.2, return_when=FIRST_COMPLETED)

                # é€²æ—ã®åˆæˆï¼ˆå¹³å‡é€²æ—çŽ‡ï¼‰
                if progress_callback:
                    with state_lock:
                        snapshots = {k: dict(v) for k, v in states.items()}

                    fractions = []
                    for name, st in snapshots.items():
                        t = int(st.get("total") or 0)
                        c = int(st.get("current") or 0)
                        if t > 0:
                            c = min(max(c, 0), t)
                            fractions.append(float(c) / float(t))
                        else:
                            fractions.append(0.0)

                    overall = sum(fractions) / max(len(fractions), 1)
                    mapped = 50 + int(overall * 35)  # 50% â†’ 85%
                    msg = (
                        f"è¨­å‚™: {snapshots['instruments']['current']}/{snapshots['instruments']['total'] or '?'} | "
                        f"ãƒ©ã‚¤ã‚»ãƒ³ã‚¹: {snapshots['licenses']['current']}/{snapshots['licenses']['total'] or '?'}"
                    )
                    if not _progress_ok(progress_callback, mapped, 100, msg):
                        return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

                # å®Œäº† future ã‚’å‡¦ç†
                for future in done:
                    name = future_to_name[future]
                    # ä¾‹å¤–ã¯ã“ã“ã§å†é€å‡ºã—ã¦ stage_error_handler ã«æ‹¾ã‚ã›ã‚‹
                    future.result()
                    completed += 1
                    remaining.remove(future)

                    if progress_callback:
                        mapped = 50 + int((completed / max(total, 1)) * 35)
                        if not progress_callback(mapped, 100, f"å–å¾—å®Œäº†: {name} ({completed}/{total})"):
                            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
    
    if progress_callback:
        if not progress_callback(100, 100, "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ»è¨­å‚™ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±å–å¾—å®Œäº†"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
    
    return "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ»è¨­å‚™ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸ"

@stage_error_handler("invoiceSchemaæƒ…å ±å–å¾—")
def fetch_invoice_schema_stage(bearer_token, progress_callback=None, max_workers: int = 10):
    """æ®µéšŽ8: invoiceSchemaæƒ…å ±å–å¾—"""
    if progress_callback:
        if not progress_callback(10, 100, "invoiceSchemaæƒ…å ±å–å¾—ä¸­..."):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
    
    output_dir = OUTPUT_RDE_DATA_DIR
    result = fetch_invoice_schemas(bearer_token, output_dir, progress_callback, max_workers=max_workers)
    
    if progress_callback:
        if not progress_callback(100, 100, "invoiceSchemaæƒ…å ±å–å¾—å®Œäº†"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
    
    return result

def finalize_basic_info_stage(webview=None, progress_callback=None):
    """æ®µéšŽ8: çµ±åˆæƒ…å ±ç”Ÿæˆãƒ»WebViewé·ç§»"""
    try:
        if progress_callback:
            if not progress_callback(20, 100, "çµ±åˆæƒ…å ±ç”Ÿæˆä¸­..."):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        # ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±ã‹ã‚‰çµ±åˆæƒ…å ±ã‚’ç”Ÿæˆ
        if os.path.exists(SUBGROUP_JSON_PATH):
            with open(SUBGROUP_JSON_PATH, "r", encoding="utf-8") as f:
                sub_group_data = json.load(f)
            
            # ã‚°ãƒ«ãƒ¼ãƒ—IDã®è§£æžï¼ˆv2.1.16: program_idå„ªå…ˆå¯¾å¿œï¼‰
            group_path = os.path.join(OUTPUT_DIR, "rde", "data", "group.json")
            group_detail_path = os.path.join(OUTPUT_DIR, "rde", "data", "groupDetail.json")
            
            group_id = None
            project_group_id = None
            
            if os.path.exists(group_path):
                with open(group_path, "r", encoding="utf-8") as f:
                    group_data = json.load(f)
                group_id = parse_group_id_from_data(group_data)
            
            if os.path.exists(group_detail_path):
                with open(group_detail_path, "r", encoding="utf-8") as f:
                    detail_data = json.load(f)
                project_group_id = parse_group_id_from_data(detail_data)
            
            if progress_callback:
                if not progress_callback(60, 100, "WebViewé·ç§»ä¸­..."):
                    return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
            # WebViewé·ç§»
            if webview and project_group_id:
                move_webview_to_group(webview, project_group_id)
            
            if progress_callback:
                if not progress_callback(80, 100, "çµ±åˆæƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆä¸­..."):
                    return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
            # info.jsonç”Ÿæˆ
            users, subgroups = extract_users_and_subgroups(sub_group_data)
            info = {
                'group_id': group_id,
                'project_group_id': project_group_id,
                'users': users,
                'subgroups': subgroups
            }
            info_json_path = [OUTPUT_DIR, 'rde', 'data', 'info.json']
            save_json(info, *info_json_path)
        
        if progress_callback:
            if not progress_callback(100, 100, "çµ±åˆæƒ…å ±ç”Ÿæˆå®Œäº†"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        return "çµ±åˆæƒ…å ±ç”Ÿæˆãƒ»WebViewé·ç§»ãŒå®Œäº†ã—ã¾ã—ãŸ"
    except Exception as e:
        error_msg = f"çµ±åˆæƒ…å ±ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
        logger.error(error_msg)
        return error_msg

def auto_refresh_subgroup_json(bearer_token, progress_callback=None, force_refresh_subgroup: bool = False):
    """
    ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ä½œæˆæˆåŠŸå¾Œã«subGroup.jsonã‚’è‡ªå‹•å†å–å¾—ã™ã‚‹
    
    v2.1.17æ›´æ–°:
    - parent_widget=None ã‚’æ¸¡ã™ï¼ˆè‡ªå‹•æ›´æ–°ã®ãŸã‚ãƒ€ã‚¤ã‚¢ãƒ­ã‚°è¡¨ç¤ºãªã—ï¼‰
    """
    try:
        if progress_callback:
            if not progress_callback(20, 100, "ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±è‡ªå‹•æ›´æ–°ä¸­..."):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        logger.info("ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ä½œæˆæˆåŠŸ - subGroup.jsonè‡ªå‹•æ›´æ–°é–‹å§‹")
        result = fetch_group_info_stage(
            bearer_token,
            progress_callback,
            program_id=None,
            parent_widget=None,
            force_program_dialog=False,
            force_download=False,
            force_refresh_subgroup=force_refresh_subgroup,
            skip_dialog=True,
        )
        
        if progress_callback:
            if not progress_callback(100, 100, "ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±è‡ªå‹•æ›´æ–°å®Œäº†"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        logger.info("ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±è‡ªå‹•æ›´æ–°å®Œäº†")
        return result
    except Exception as e:
        error_msg = f"ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±è‡ªå‹•æ›´æ–°ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
        logger.error(error_msg)
        return error_msg

def auto_refresh_dataset_json(bearer_token, progress_callback=None):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–‹è¨­æˆåŠŸå¾Œã«dataset.jsonã‚’è‡ªå‹•å†å–å¾—ã™ã‚‹
    """
    try:
        if progress_callback:
            if not progress_callback(20, 100, "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§è‡ªå‹•æ›´æ–°ä¸­..."):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        logger.info("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–‹è¨­æˆåŠŸ - dataset.jsonè‡ªå‹•æ›´æ–°é–‹å§‹")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§ã®ã¿æ›´æ–°ï¼ˆå€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©³ç´°ã¯é™¤ãï¼‰
        fetch_dataset_list_only(bearer_token, output_dir=os.path.join(OUTPUT_DIR, "rde", "data"))
        
        if progress_callback:
            if not progress_callback(100, 100, "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§è‡ªå‹•æ›´æ–°å®Œäº†"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        logger.info("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§è‡ªå‹•æ›´æ–°å®Œäº†")
        return "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§ã®è‡ªå‹•æ›´æ–°ãŒå®Œäº†ã—ã¾ã—ãŸ"
    except Exception as e:
        error_msg = f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§è‡ªå‹•æ›´æ–°ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
        logger.error(error_msg)
        return error_msg

# === æ®µéšŽé¸æŠžç”¨ã®å®Ÿè¡Œé–¢æ•°ãƒžãƒƒãƒ— ===
STAGE_FUNCTIONS = {
    "ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±": fetch_user_info_stage,
    "ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±": fetch_group_info_stage,
    "çµ„ç¹”ãƒ»è£…ç½®æƒ…å ±": fetch_organization_stage,
    "ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±": fetch_sample_info_stage,
    "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±": fetch_dataset_info_stage,
    "ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±": fetch_data_entry_stage,
    "ã‚¤ãƒ³ãƒœã‚¤ã‚¹æƒ…å ±": fetch_invoice_stage,
    "invoiceSchemaæƒ…å ±": fetch_invoice_schema_stage,
    "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ»è¨­å‚™ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±": fetch_template_instrument_stage,
    "çµ±åˆæƒ…å ±ç”Ÿæˆ": finalize_basic_info_stage,
    "--- è»½é‡å–å¾— ---": None,  # ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿
    "ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ï¼ˆè»½é‡ï¼‰": lambda bearer_token, **kwargs: fetch_sample_info_from_subgroup_ids_only(bearer_token),
    "--- è‡ªå‹•æ›´æ–° ---": None,  # ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿
    "subGroup.jsonè‡ªå‹•æ›´æ–°": auto_refresh_subgroup_json,
    "dataset.jsonè‡ªå‹•æ›´æ–°": auto_refresh_dataset_json
}

def execute_individual_stage(
    stage_name,
    bearer_token,
    webview=None,
    onlySelf=False,
    searchWords=None,
    searchWordsBatch: Optional[List[str]] = None,
    progress_callback=None,
    parent_widget=None,
    force_program_dialog: bool = False,
    force_download: bool = False,
    parallel_max_workers: Optional[int] = None,
):
    """æŒ‡å®šã•ã‚ŒãŸæ®µéšŽã‚’å€‹åˆ¥å®Ÿè¡Œã™ã‚‹"""
    if stage_name not in STAGE_FUNCTIONS:
        return f"ä¸æ­£ãªæ®µéšŽåã§ã™: {stage_name}"
    
    # ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ã®å ´åˆã¯å®Ÿè¡Œã—ãªã„
    if STAGE_FUNCTIONS[stage_name] is None:
        return f"ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ã‚¢ã‚¤ãƒ†ãƒ ã¯å®Ÿè¡Œã§ãã¾ã›ã‚“: {stage_name}"
    
    logger.info(f"å€‹åˆ¥æ®µéšŽå®Ÿè¡Œé–‹å§‹: {stage_name}")
    
    try:
        func = STAGE_FUNCTIONS[stage_name]
        
        resolved_workers: Optional[int] = None
        try:
            if parallel_max_workers is not None:
                resolved_workers = int(parallel_max_workers)
                if resolved_workers < 1:
                    resolved_workers = None
        except Exception:
            resolved_workers = None

        # é–¢æ•°ã®ã‚·ã‚°ãƒãƒãƒ£ã«å¿œã˜ã¦å¼•æ•°ã‚’èª¿æ•´
        if stage_name == "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±":
            result = func(
                bearer_token,
                onlySelf=onlySelf,
                searchWords=searchWords,
                 searchWordsBatch=searchWordsBatch,
                progress_callback=progress_callback,
                max_workers=resolved_workers or 10,
            )
        elif stage_name == "çµ±åˆæƒ…å ±ç”Ÿæˆ":
            result = func(webview=webview, progress_callback=progress_callback)
        elif stage_name == "ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±":
            result = func(bearer_token, progress_callback=progress_callback, parent_widget=parent_widget)
        elif stage_name == "ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±":
            result = func(
                bearer_token,
                progress_callback=progress_callback,
                program_id=None,
                parent_widget=parent_widget,
                force_program_dialog=force_program_dialog,
                force_download=force_download,
                max_workers=resolved_workers or 10,
            )
        elif stage_name in ["subGroup.jsonè‡ªå‹•æ›´æ–°", "dataset.jsonè‡ªå‹•æ›´æ–°"]:
            # è‡ªå‹•æ›´æ–°é–¢æ•°ã¯ bearer_token ã¨ progress_callback ã®ã¿
            result = func(bearer_token, progress_callback=progress_callback)
        else:
            if stage_name in {"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±", "ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±", "ã‚¤ãƒ³ãƒœã‚¤ã‚¹æƒ…å ±", "invoiceSchemaæƒ…å ±", "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ»è¨­å‚™ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±"}:
                result = func(
                    bearer_token,
                    progress_callback=progress_callback,
                    max_workers=resolved_workers or 10,
                )
            else:
                result = func(bearer_token, progress_callback=progress_callback)
        
        logger.info(f"å€‹åˆ¥æ®µéšŽå®Ÿè¡Œå®Œäº†: {stage_name}")
        return result
    except Exception as e:
        error_msg = f"å€‹åˆ¥æ®µéšŽå®Ÿè¡Œã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ({stage_name}): {e}"
        logger.error(error_msg)
        traceback.print_exc()
        return error_msg

def fetch_basic_info_logic(
    bearer_token,
    parent=None,
    webview=None,
    onlySelf=False,
    searchWords=None,
    searchWordsBatch: Optional[List[str]] = None,
    skip_confirmation=False,
    progress_callback=None,
    program_id=None,
    force_download: bool = False,
    parallel_max_workers: Optional[int] = None,
):
    """
    åŸºæœ¬æƒ…å ±å–å¾—ãƒ»ä¿å­˜ãƒ»WebViewé·ç§»ï¼ˆé–‹ç™ºç”¨ï¼‰
    
    v2.0.1æ”¹å–„:
    - äº‹å‰ãƒˆãƒ¼ã‚¯ãƒ³æ¤œè¨¼ã®è¿½åŠ 
    - èªè¨¼ã‚¨ãƒ©ãƒ¼æ™‚ã®å†ãƒ­ã‚°ã‚¤ãƒ³ä¿ƒé€²
    - ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ˜Žç¢ºåŒ–
    
    v2.1.16è¿½åŠ :
    - program_idå¼•æ•°ã‚’è¿½åŠ ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—é¸æŠžæ©Ÿèƒ½å¯¾å¿œï¼‰

    v2.1.20è¿½åŠ :
    - force_downloadå¼•æ•°ã‚’è¿½åŠ ã€‚Falseæ™‚ã¯æ—¢å­˜JSONã‚’å„ªå…ˆåˆ©ç”¨ã—ã€æ¬ æåˆ†ã®ã¿å–å¾—
    """
    import traceback
    import json
    from pathlib import Path
    from core.bearer_token_manager import BearerTokenManager
    from qt_compat.widgets import QMessageBox

    try:
        resolved_workers = int(parallel_max_workers) if parallel_max_workers is not None else None
        if resolved_workers is not None and resolved_workers < 1:
            resolved_workers = None
    except Exception:
        resolved_workers = None

    # æ—¢å­˜ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤(å¤šãã®ç®‡æ‰€ã§10)ã‚’ç¶­æŒã—ã¤ã¤ã€UIã‹ã‚‰ä¸Šæ›¸ãå¯èƒ½ã«ã™ã‚‹
    parallel_workers = resolved_workers or 10
    
    # ===== 1. ãƒˆãƒ¼ã‚¯ãƒ³æ¤œè¨¼ï¼ˆv2.0.1æ–°è¦è¿½åŠ ï¼‰ =====
    logger.info("åŸºæœ¬æƒ…å ±å–å¾—é–‹å§‹: ãƒˆãƒ¼ã‚¯ãƒ³æ¤œè¨¼")
    
    # bearer_tokenãŒæ¸¡ã•ã‚Œã¦ã„ãªã„ã€ã¾ãŸã¯ç©ºã®å ´åˆã¯BearerTokenManagerã‹ã‚‰å–å¾—
    if not bearer_token or bearer_token.strip() == "":
        logger.warning("bearer_tokenãŒæœªæŒ‡å®šã®ãŸã‚ã€BearerTokenManagerã‹ã‚‰å–å¾—ã—ã¾ã™")
        bearer_token = BearerTokenManager.get_valid_token()
    else:
        # æ¸¡ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼
        logger.debug("æ¸¡ã•ã‚ŒãŸbearer_tokenã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼ã—ã¾ã™")
        if not BearerTokenManager.validate_token(bearer_token):
            logger.warning("æ¸¡ã•ã‚ŒãŸbearer_tokenãŒç„¡åŠ¹ã§ã™")
            bearer_token = None
    
    # ãƒˆãƒ¼ã‚¯ãƒ³ãŒå–å¾—ã§ããªã„ã€ã¾ãŸã¯ç„¡åŠ¹ãªå ´åˆ
    if not bearer_token:
        error_msg = "èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ãŒç„¡åŠ¹ã¾ãŸã¯æœŸé™åˆ‡ã‚Œã§ã™ã€‚"
        logger.error(error_msg)
        
        # å†ãƒ­ã‚°ã‚¤ãƒ³ä¿ƒé€²ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’è¡¨ç¤º
        if parent and BearerTokenManager.request_relogin_if_invalid(parent):
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå†ãƒ­ã‚°ã‚¤ãƒ³ã‚’é¸æŠžã—ãŸå ´åˆ
            # ãƒ­ã‚°ã‚¤ãƒ³ã‚¿ãƒ–ã¸ã®åˆ‡ã‚Šæ›¿ãˆã‚’è©¦ã¿ã‚‹
            try:
                if hasattr(parent, 'tabs'):
                    # ãƒ¡ã‚¤ãƒ³ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ã‚¿ãƒ–ã‚’æ¤œç´¢
                    for i in range(parent.tabs.count()):
                        if parent.tabs.tabText(i) == "ãƒ­ã‚°ã‚¤ãƒ³":
                            parent.tabs.setCurrentIndex(i)
                            logger.info("ãƒ­ã‚°ã‚¤ãƒ³ã‚¿ãƒ–ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ")
                            break
                
                # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
                QMessageBox.information(
                    parent,
                    "å†ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦",
                    "ãƒ­ã‚°ã‚¤ãƒ³ã‚¿ãƒ–ã§RDEã‚·ã‚¹ãƒ†ãƒ ã«å†ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„ã€‚\n"
                    "ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†å¾Œã€å†åº¦åŸºæœ¬æƒ…å ±å–å¾—ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
                )
            except Exception as e:
                logger.error(f"ãƒ­ã‚°ã‚¤ãƒ³ã‚¿ãƒ–åˆ‡ã‚Šæ›¿ãˆã‚¨ãƒ©ãƒ¼: {e}")
        
        return error_msg
    
    logger.info(f"ãƒˆãƒ¼ã‚¯ãƒ³æ¤œè¨¼æˆåŠŸ: {bearer_token[:20]}...")
    
    # ===== 2. ç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°è¡¨ç¤º =====
    if not skip_confirmation:
        preview_words = list(searchWordsBatch) if searchWordsBatch else None
        if not show_fetch_confirmation_dialog(parent, onlySelf, searchWords, searchWordsList=preview_words):
            logger.info("åŸºæœ¬æƒ…å ±å–å¾—å‡¦ç†ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚Šã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
    
    logger.info("åŸºæœ¬æƒ…å ±å–å¾—å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")

    def _exists(path: str) -> bool:
        return Path(path).exists()

    def _folder_has_files(folder_path: str, expected_count: Optional[int] = None) -> tuple[bool, int]:
        """ãƒ•ã‚©ãƒ«ãƒ€å†…ã®JSONãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ãƒã‚§ãƒƒã‚¯ã€‚existsã¯å¸¸ã«ç¢ºèªã€‚
        
        Returns:
            (has_any_files, actual_count): ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ã€å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°
        """
        folder = Path(folder_path)
        # ãƒ•ã‚©ãƒ«ãƒ€ã®å­˜åœ¨ç¢ºèªã¯ã‚¹ã‚­ãƒƒãƒ—ã—ãªã„ï¼ˆv2.1.21ï¼‰
        if not folder.exists():
            logger.debug(f"ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {folder_path}")
            return False, 0
        
        # *.json ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        json_files = list(folder.glob("*.json"))
        actual_count = len(json_files)
        
        # expected_countãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯æ¬ æåˆ¤å®š
        if expected_count is not None and actual_count < expected_count:
            logger.info(f"ãƒ•ã‚©ãƒ«ãƒ€å†…ã«æ¬ æãƒ•ã‚¡ã‚¤ãƒ«ã‚ã‚Š: {folder_path} (æœŸå¾…: {expected_count}ä»¶, å®Ÿéš›: {actual_count}ä»¶)")
            return True, actual_count  # æ¬ æãŒã‚ã£ã¦ã‚‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒ1ã¤ã§ã‚‚ã‚ã‚Œã°True
        
        return actual_count > 0, actual_count
    
    try:
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ç®¡ç†
        stages = [
            ("ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—", 5),
            ("ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±å–å¾—", 8), 
            ("ã‚°ãƒ«ãƒ¼ãƒ—è©³ç´°æƒ…å ±å–å¾—", 8),
            ("ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±å–å¾—", 8),
            ("ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å–å¾—", 12),
            ("çµ„ç¹”ãƒ»è£…ç½®æƒ…å ±å–å¾—", 8),
            ("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±å–å¾—", 16),
            ("ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±å–å¾—", 12),
            ("ã‚¤ãƒ³ãƒœã‚¤ã‚¹æƒ…å ±å–å¾—", 10),
            ("invoiceSchemaæƒ…å ±å–å¾—", 10),
            ("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ»è¨­å‚™ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±å–å¾—", 8),
            ("çµ±åˆæƒ…å ±ç”Ÿæˆãƒ»WebViewé·ç§»", 5)
        ]
        
        current_progress = 0
        
        def update_stage_progress(stage_index, stage_progress=100, sub_message=""):
            nonlocal current_progress
            if stage_index > 0:
                # å‰ã®æ®µéšŽã¾ã§å®Œäº†
                current_progress = sum(stage[1] for stage in stages[:stage_index])
            
            # ç¾åœ¨ã®æ®µéšŽã®é€²æ—ã‚’åŠ ç®—
            stage_weight = stages[stage_index][1]
            stage_contribution = (stage_progress / 100) * stage_weight
            total_progress = current_progress + stage_contribution
            
            stage_name = stages[stage_index][0]
            message = f"{stage_name}: {sub_message}" if sub_message else stage_name
            
            if progress_callback:
                return progress_callback(int(total_progress), 100, message)
            return True

        # 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼è‡ªèº«æƒ…å ±å–å¾—
        if not update_stage_progress(0, 0, "é–‹å§‹"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        logger.debug("fetch_self_info_from_api")
        try:
            if force_download or not _exists(SELF_JSON_PATH):
                fetch_self_info_from_api(bearer_token, parent_widget=parent)
            else:
                logger.info("ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±: æ—¢å­˜ã® self.json ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        except Exception as fetch_error:
            logger.error(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {fetch_error}")
            return "ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"
        
        if not update_stage_progress(0, 100, "å®Œäº†"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        group_id = None
        project_group_id = None
        sub_group_data = None

        # 2-4. ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±å–å¾—ï¼ˆçµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼‰
        if not update_stage_progress(1, 0, "é–‹å§‹"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        if not update_stage_progress(2, 0, "æº–å‚™ä¸­"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        if not update_stage_progress(3, 0, "æº–å‚™ä¸­"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        force_project_dialog = os.environ.get('FORCE_PROJECT_GROUP_DIALOG', '0') == '1'

        def pipeline_progress_adapter(current_percent, total, message):
            percent = max(0, min(100, current_percent))
            if percent <= 34:
                mapped = min(100, int((percent / 34) * 100))
                return update_stage_progress(1, mapped, message)
            if percent <= 67:
                mapped = min(100, int(((percent - 34) / 33) * 100))
                return update_stage_progress(2, mapped, message)
            mapped = min(100, int(((percent - 67) / 33) * 100))
            return update_stage_progress(3, mapped, message)
        group_files_ready = all(
            _exists(path) for path in (GROUP_JSON_PATH, GROUP_DETAIL_JSON_PATH, SUBGROUP_JSON_PATH)
        )
        subgroups_complete = _subgroups_folder_complete() if group_files_ready else False
        if group_files_ready and not subgroups_complete and not force_download:
            logger.info("ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—è©³ç´°ã«æ¬ æãŒã‚ã‚‹ãŸã‚ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±ã‚’å†å–å¾—ã—ã¾ã™")
        use_cache = (not force_download) and group_files_ready and subgroups_complete
        group_pipeline = None

        if use_cache:
            try:
                with open(GROUP_DETAIL_JSON_PATH, "r", encoding="utf-8") as f:
                    cached_program_data = json.load(f)
                with open(SUBGROUP_JSON_PATH, "r", encoding="utf-8") as f:
                    cached_project_data = json.load(f)
                group_id = cached_program_data.get("data", {}).get("id")
                project_group_id = cached_project_data.get("data", {}).get("id")
                sub_group_data = cached_project_data
                if not group_id or not project_group_id:
                    raise ValueError("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«å¿…è¦ãªã‚°ãƒ«ãƒ¼ãƒ—IDãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
                logger.info("ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±: æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†åˆ©ç”¨ã—ã¾ã—ãŸ")
            except Exception as cache_error:
                logger.warning("ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£JSONã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸãŸã‚å†å–å¾—ã‚’å®Ÿè¡Œã—ã¾ã™: %s", cache_error)
                use_cache = False

        if not use_cache:
            try:
                group_pipeline = run_group_hierarchy_pipeline(
                    bearer_token=bearer_token,
                    parent_widget=parent,
                    preferred_program_id=program_id,
                    progress_callback=pipeline_progress_adapter,
                    force_project_dialog=force_project_dialog,
                )
            except GroupFetchCancelled:
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

            group_id = group_pipeline.selected_program_id
            project_group_id = group_pipeline.selected_project_id
            sub_group_data = group_pipeline.selected_project_data

            if not sub_group_data or not project_group_id:
                logger.error("ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’æ­£å¸¸ã«å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return "ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ"

            total_success = sum(item.get("success", 0) for item in group_pipeline.subgroup_summary.values())
            total_fail = sum(item.get("fail", 0) for item in group_pipeline.subgroup_summary.values())
            logger.info(
                "ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±å–å¾—å®Œäº†ï¼ˆã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—: æˆåŠŸ %sä»¶, å¤±æ•— %sä»¶ï¼‰",
                total_success,
                total_fail,
            )

        for stage_idx in (1, 2, 3):
            sub_message = "ã‚­ãƒ£ãƒƒã‚·ãƒ¥å†åˆ©ç”¨" if use_cache else "å®Œäº†"
            if not update_stage_progress(stage_idx, 100, sub_message):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        # 5. ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å–å¾—
        if not update_stage_progress(4, 0, "ã‚µãƒ³ãƒ—ãƒ«å–å¾—æº–å‚™ä¸­"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
        logger.debug("fetch_sample_info_from_api")
        sub_group_included = []
        if sub_group_data and isinstance(sub_group_data, dict):
            sub_group_included = sub_group_data.get("included", [])
            
        sample_dir = os.path.join(OUTPUT_DIR, "rde", "data", "samples")
        os.makedirs(sample_dir, exist_ok=True)
        
        total_samples = len(sub_group_included)
        if not update_stage_progress(4, 0, f"ã‚µãƒ³ãƒ—ãƒ«å–å¾—æº–å‚™: è¨ˆç”» {total_samples}ä»¶ (ä¸¦åˆ—é–¾å€¤: 50ä»¶)"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        # ã‚µãƒ³ãƒ—ãƒ« ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆv2.1.21: æ¬ æåˆ¤å®šï¼‰
        sample_has_files, sample_actual_count = _folder_has_files(sample_dir, expected_count=total_samples)
        
        skip_sample_fetch = not force_download and sample_has_files and sample_actual_count == total_samples
        
        if skip_sample_fetch:
            logger.info(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±: æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€({sample_actual_count}ä»¶)ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            if not update_stage_progress(4, 100, f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Œäº† (è¨ˆç”»: {total_samples}ä»¶)"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        else:
            processed_samples = 0
            skipped_samples = 0
            failed_samples = 0

            if total_samples >= 50:
                from config.common import load_bearer_token
                from net.http_helpers import parallel_download

                material_token = load_bearer_token('rde-material.nims.go.jp')
                tasks = [
                    (material_token, included.get("id", ""), sample_dir, force_download)
                    for included in sub_group_included
                    if included.get("id")
                ]

                def sample_parallel_progress(current, total, message):
                    mapped = 5 + int((current / 100.0) * 90)
                    mapped = min(95, max(5, mapped))
                    text = f"ä¸¦åˆ—ã‚µãƒ³ãƒ—ãƒ«å–å¾—ä¸­ (è¨ˆç”»: {total_samples}ä»¶, {message})"
                    return update_stage_progress(4, mapped, text)

                result = parallel_download(
                    tasks=tasks,
                    worker_function=_fetch_single_sample_worker_force,
                    max_workers=parallel_workers,
                    progress_callback=sample_parallel_progress,
                    threshold=1,
                )

                if result.get("cancelled"):
                    logger.warning("ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å–å¾—ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚Šã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
                    return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

                processed_samples = result.get("success_count", 0)
                skipped_samples = result.get("skipped_count", 0)
                failed_samples = result.get("failed_count", 0)
            else:
                from config.common import load_bearer_token

                material_token = load_bearer_token('rde-material.nims.go.jp')
                for idx, included in enumerate(sub_group_included):
                    current_index = idx + 1
                    sample_progress = int((current_index / total_samples) * 100) if total_samples > 0 else 100
                    group_id_sample = included.get("id", "")
                    sample_json_path = os.path.join(sample_dir, f"{group_id_sample}.json")
                    
                    if not force_download and os.path.exists(sample_json_path):
                        skipped_samples += 1
                        if not update_stage_progress(4, sample_progress, f"ã‚µãƒ³ãƒ—ãƒ«ç¢ºèª {current_index}/{total_samples} - ã‚¹ã‚­ãƒƒãƒ—æ¸ˆã¿: {skipped_samples}"):
                            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
                        logger.debug(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±({group_id_sample})ã¯æ—¢ã«å­˜åœ¨ã™ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ: {sample_json_path}")
                        continue
                        
                    if not update_stage_progress(4, sample_progress, f"ã‚µãƒ³ãƒ—ãƒ«å–å¾—ä¸­ {current_index}/{total_samples} - å®Œäº†: {processed_samples}"):
                        return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
                    
                    url = f"https://rde-material-api.nims.go.jp/samples?groupId={group_id_sample}&page%5Blimit%5D=1000&page%5Boffset%5D=0&fields%5Bsample%5D=names%2Cdescription%2Ccomposition"
                    try:
                        headers_sample = _make_headers(material_token, host="rde-material-api.nims.go.jp", origin="https://rde-entry-arim.nims.go.jp", referer="https://rde-entry-arim.nims.go.jp/")
                        resp = api_request("GET", url, bearer_token=material_token, headers=headers_sample, timeout=10)
                        if resp is None:
                            logger.error(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±({group_id_sample})ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼")
                            failed_samples += 1
                            continue
                        resp.raise_for_status()
                        data = resp.json()
                        with open(sample_json_path, "w", encoding="utf-8") as f:
                            json.dump(data, f, ensure_ascii=False, indent=2)
                        processed_samples += 1
                        logger.info(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±({group_id_sample})ã®å–å¾—ãƒ»ä¿å­˜ã«æˆåŠŸã—ã¾ã—ãŸ: {sample_json_path}")
                    except Exception as e:
                        failed_samples += 1
                        logger.error(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±({group_id_sample})ã®å–å¾—ãƒ»ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

            logger.info(
                "ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å–å¾—å®Œäº†: å‡¦ç†æ¸ˆã¿ %sä»¶, ã‚¹ã‚­ãƒƒãƒ—æ¸ˆã¿ %sä»¶, å¤±æ•— %sä»¶ (è¨ˆç”» %sä»¶)",
                processed_samples,
                skipped_samples,
                failed_samples,
                total_samples,
            )
            final_message = (
                f"å®Œäº† (è¨ˆç”»: {total_samples}ä»¶, æˆåŠŸ: {processed_samples}, ã‚¹ã‚­ãƒƒãƒ—: {skipped_samples}, å¤±æ•—: {failed_samples}, "
                f"ä¸¦åˆ—: {'æœ‰åŠ¹' if total_samples >= 50 else 'ç„¡åŠ¹'})"
            )
            if not update_stage_progress(4, 100, final_message):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        # 6. çµ„ç¹”ãƒ»è£…ç½®æƒ…å ±å–å¾—
        total_org_tasks = 2
        if not update_stage_progress(5, 0, f"çµ„ç¹”ãƒ»è£…ç½®æƒ…å ±å–å¾—æº–å‚™ (è¨ˆç”»: {total_org_tasks}ä»¶, ä¸¦åˆ—: ãªã—)"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
        logger.debug("fetch_organization_info_from_api")
        org_json_path = [OUTPUT_DIR, "rde", "data", "organization.json"]
        if force_download or not _exists(ORGANIZATION_JSON_PATH):
            fetch_organization_info_from_api(bearer_token, org_json_path)
            if not update_stage_progress(5, 50, "çµ„ç¹”æƒ…å ±å–å¾—å®Œäº† (1/2)"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        else:
            logger.info("çµ„ç¹”æƒ…å ±: æ—¢å­˜ã® organization.json ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            if not update_stage_progress(5, 50, "çµ„ç¹”æƒ…å ±ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Œäº† (1/2)"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        logger.debug("fetch_instrument_type_info_from_api")
        instrument_type_json_path = [OUTPUT_DIR, "rde", "data", "instrumentType.json"]
        if force_download or not _exists(INSTRUMENT_TYPE_JSON_PATH):
            fetch_instrument_type_info_from_api(bearer_token, instrument_type_json_path)
            if not update_stage_progress(5, 100, "è£…ç½®ã‚¿ã‚¤ãƒ—æƒ…å ±å–å¾—å®Œäº† (2/2)"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        else:
            logger.info("è£…ç½®ã‚¿ã‚¤ãƒ—æƒ…å ±: æ—¢å­˜ã® instrumentType.json ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            if not update_stage_progress(5, 100, "è£…ç½®ã‚¿ã‚¤ãƒ—æƒ…å ±ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Œäº† (2/2)"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        # 7. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±å–å¾—
        if not update_stage_progress(6, 0, "é–‹å§‹"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
        logger.debug("fetch_all_dataset_info")

        def dataset_progress_adapter(current, total, message):
            return update_stage_progress(6, current, message)

        if force_download or not _exists(DATASET_JSON_PATH):
            dataset_result = fetch_all_dataset_info(
                bearer_token,
                output_dir=os.path.join(OUTPUT_DIR, "rde", "data"),
                onlySelf=onlySelf,
                searchWords=searchWords,
                searchWordsBatch=searchWordsBatch,
                progress_callback=dataset_progress_adapter,
                max_workers=parallel_workers,
            )
            if dataset_result == "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ":
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        else:
            cache_message = "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§: æ—¢å­˜ã® dataset.json ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™"
            logger.info(cache_message)
            if not update_stage_progress(6, 100, f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Œäº† (è¨ˆç”»: ä¸æ˜Žä»¶, ä¸¦åˆ—: ãªã—)"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        if not update_stage_progress(6, 100, "å®Œäº†"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        # 8. ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±å–å¾—
        if not update_stage_progress(7, 0, "ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±å–å¾—æº–å‚™ä¸­"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
        logger.debug("fetch_all_data_entrys_info")
        
        # dataEntry ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆv2.1.21: æ¬ æåˆ¤å®šï¼‰
        dataentry_dir = os.path.join(OUTPUT_DIR, "rde", "data", "dataEntry")
        dataentry_has_files, dataentry_count = _folder_has_files(dataentry_dir)
        
        skip_dataentry_fetch = not force_download and dataentry_has_files
        
        if skip_dataentry_fetch:
            logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±: æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€({dataentry_count}ä»¶)ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            if not update_stage_progress(7, 100, "ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Œäº†"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        else:
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½œæˆï¼ˆã‚¹ãƒ†ãƒ¼ã‚¸7ã®0-100%ã‚’ãƒžãƒƒãƒ”ãƒ³ã‚°ï¼‰
            def dataentry_progress_callback(current, total, message):
                return update_stage_progress(7, current, message)
            
            result = fetch_all_data_entrys_info(
                bearer_token,
                progress_callback=dataentry_progress_callback,
                max_workers=parallel_workers,
            )
            if result == "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ":
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        if not update_stage_progress(7, 100, "ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±å–å¾—å®Œäº†"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        # 9. ã‚¤ãƒ³ãƒœã‚¤ã‚¹æƒ…å ±å–å¾—
        if not update_stage_progress(8, 0, "ã‚¤ãƒ³ãƒœã‚¤ã‚¹æƒ…å ±å–å¾—æº–å‚™ä¸­"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
        logger.debug("fetch_all_invoices_info")
        
        # invoice ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆv2.1.21: æ¬ æåˆ¤å®šï¼‰
        invoice_dir = os.path.join(OUTPUT_DIR, "rde", "data", "invoice")
        invoice_has_files, invoice_count = _folder_has_files(invoice_dir)
        
        skip_invoice_fetch = not force_download and invoice_has_files
        
        if skip_invoice_fetch:
            logger.info(f"ã‚¤ãƒ³ãƒœã‚¤ã‚¹æƒ…å ±: æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€({invoice_count}ä»¶)ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            if not update_stage_progress(8, 100, "ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Œäº†"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        else:
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½œæˆï¼ˆã‚¹ãƒ†ãƒ¼ã‚¸8ã®0-100%ã‚’ãƒžãƒƒãƒ”ãƒ³ã‚°ï¼‰
            def invoice_progress_callback(current, total, message):
                return update_stage_progress(8, current, message)
            
            result = fetch_all_invoices_info(
                bearer_token,
                progress_callback=invoice_progress_callback,
                max_workers=parallel_workers,
            )
            if result == "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ":
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        if not update_stage_progress(8, 100, "ã‚¤ãƒ³ãƒœã‚¤ã‚¹æƒ…å ±å–å¾—å®Œäº†"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        # 10. invoiceSchemaæƒ…å ±å–å¾—
        if not update_stage_progress(9, 0, "invoiceSchemaæƒ…å ±å–å¾—ä¸­"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
        logger.debug("fetch_invoice_schemas")
        
        # invoiceSchemas ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆv2.1.21: æ¬ æåˆ¤å®šï¼‰
        invoiceschemas_dir = os.path.join(OUTPUT_DIR, "rde", "data", "invoiceSchemas")
        invoiceschemas_has_files, invoiceschemas_count = _folder_has_files(invoiceschemas_dir)
        
        skip_invoiceschema_fetch = not force_download and invoiceschemas_has_files
        
        if skip_invoiceschema_fetch:
            logger.info(f"invoiceSchemaæƒ…å ±: æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€({invoiceschemas_count}ä»¶)ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            if not update_stage_progress(9, 100, f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Œäº† (æ—¢å­˜: {invoiceschemas_count}ä»¶)"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        else:
            try:
                output_dir = os.path.join(OUTPUT_DIR, "rde", "data")

                def invoiceschema_progress_adapter(current, total, message):
                    return update_stage_progress(9, current, message)

                invoice_schema_result = fetch_invoice_schemas(
                    bearer_token,
                    output_dir,
                    progress_callback=invoiceschema_progress_adapter,
                    max_workers=parallel_workers,
                )
                if invoice_schema_result == "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ":
                    return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            except Exception as e:
                logger.warning(f"invoiceSchemaå–å¾—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸãŒå‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™: {e}")
        
            if not update_stage_progress(9, 100, "å®Œäº†"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        # 11. ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ»è¨­å‚™ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±å–å¾—
        if not update_stage_progress(10, 0, "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæƒ…å ±å–å¾—ä¸­"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
        logger.debug("fetch_template_info_from_api")
        if force_download or not _exists(TEMPLATE_JSON_PATH):
            fetch_template_info_from_api(bearer_token)
        else:
            logger.info("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæƒ…å ±: æ—¢å­˜ã® template.json ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        
        # devices/licenses ã¯ç‹¬ç«‹ãªã®ã§ã€å¿…è¦åˆ†ãŒã‚ã‚Œã°ä¸¦åˆ—åŒ–ã—ã¦çŸ­ç¸®
        if not update_stage_progress(10, 33, "è¨­å‚™ãƒ»åˆ©ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±å–å¾—ä¸­"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        need_instruments = force_download or not _exists(INSTRUMENTS_JSON_PATH)
        need_licenses = force_download or not _exists(LICENSES_JSON_PATH)

        if not need_instruments:
            logger.info("è¨­å‚™æƒ…å ±: æ—¢å­˜ã® instruments.json ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        if not need_licenses:
            logger.info("åˆ©ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±: æ—¢å­˜ã® licenses.json ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")

        tasks = []
        if need_instruments:
            tasks.append(("instruments", lambda: fetch_instruments_info_from_api(bearer_token)))
        if need_licenses:
            tasks.append(("licenses", lambda: fetch_licenses_info_from_api(bearer_token)))

        if len(tasks) <= 1 or parallel_workers <= 1:
            for name, fn in tasks:
                logger.debug("fetch_%s_info_from_api", name)
                fn()
        else:
            from concurrent.futures import ThreadPoolExecutor, as_completed

            with ThreadPoolExecutor(max_workers=min(int(parallel_workers), len(tasks))) as executor:
                future_to_name = {executor.submit(fn): name for name, fn in tasks}
                completed = 0
                total = len(future_to_name)
                for future in as_completed(future_to_name):
                    name = future_to_name[future]
                    future.result()
                    completed += 1
                    if not update_stage_progress(10, 33 + int((completed / max(total, 1)) * 33), f"å®Œäº†: {name} ({completed}/{total})"):
                        return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        if not update_stage_progress(10, 66, "å®Œäº†"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        if not update_stage_progress(10, 100, "å®Œäº†"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        # 12. çµ±åˆæƒ…å ±ç”Ÿæˆãƒ»WebViewé·ç§»
        if not update_stage_progress(11, 0, "çµ±åˆæƒ…å ±ç”Ÿæˆä¸­"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
        # WebViewé·ç§»ã¯UIã‚¹ãƒ¬ãƒƒãƒ‰ã§è¡Œã†
        logger.debug("move_webview_to_group")
        move_webview_to_group(webview, project_group_id)
        
        if not update_stage_progress(11, 50, "çµ±åˆæƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆä¸­"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        # info.jsonç”Ÿæˆ
        if sub_group_data:
            try:
                logger.debug("extract_users_and_subgroups")
                users, subgroups = extract_users_and_subgroups(sub_group_data)
                info = {
                    'group_id': group_id,
                    'project_group_id': project_group_id,
                    'users': users,
                    'subgroups': subgroups
                }
                info_json_path = [OUTPUT_DIR, 'rde', 'data', 'info.json']
                save_json(info, *info_json_path)
                logger.info("info.jsonï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ»ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±ï¼‰ã‚’æ›¸ãå‡ºã—ã¾ã—ãŸã€‚")
            except Exception as e:
                logger.error(f"subGroup.jsonã®è§£æžãƒ»è¡¨ç¤ºã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                traceback.print_exc()
                
        if not update_stage_progress(11, 100, "å®Œäº†"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
        result_msg = "åŸºæœ¬æƒ…å ±å–å¾—ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ"
        logger.info(result_msg)
        return result_msg
        
    except Exception as e:
        error_msg = f"åŸºæœ¬æƒ…å ±å–å¾—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
        logger.error(error_msg)
        traceback.print_exc()
        return error_msg

def fetch_sample_info_only(bearer_token, output_dir=None, progress_callback=None, max_workers: int = 10):
    """
    ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ã®ã¿ã‚’å¼·åˆ¶å–å¾—ãƒ»ä¿å­˜ï¼ˆæ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä¸Šæ›¸ãï¼‰
    v2.1.0: ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾å¿œï¼ˆ50ä»¶ä»¥ä¸Šã§è‡ªå‹•ä¸¦åˆ—åŒ–ï¼‰
    """
    from net.http_helpers import parallel_download
    
    if not bearer_token:
        error_msg = "Bearerãƒˆãƒ¼ã‚¯ãƒ³ãŒå–å¾—ã§ãã¾ã›ã‚“ã€‚ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        logger.error(error_msg)
        return error_msg
    
    logger.info("ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å¼·åˆ¶å–å¾—é–‹å§‹")
    
    try:
        if progress_callback:
            if not progress_callback(5, 100, "ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        # ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±ã‹ã‚‰å¯¾è±¡ã‚°ãƒ«ãƒ¼ãƒ—IDã‚’å–å¾—
        root_dir = output_dir or OUTPUT_RDE_DATA_DIR
        sub_group_path = os.path.join(root_dir, "subGroup.json")
        if not os.path.exists(sub_group_path):
            error_msg = "subGroup.jsonãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚å…ˆã«åŸºæœ¬æƒ…å ±å–å¾—ã¾ãŸã¯å…±é€šæƒ…å ±å–å¾—ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
            logger.error(error_msg)
            return error_msg
            
        with open(sub_group_path, "r", encoding="utf-8") as f:
            sub_group_data = json.load(f)
            
        sub_group_included = sub_group_data.get("included", [])
        if not sub_group_included:
            error_msg = "subGroup.jsonã«includedé…åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
            logger.error(error_msg)
            return error_msg
            
        if progress_callback:
            if not progress_callback(10, 100, f"å¯¾è±¡ã‚°ãƒ«ãƒ¼ãƒ—æ•°: {len(sub_group_included)}"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        sample_dir = os.path.join(root_dir, "samples")
        os.makedirs(sample_dir, exist_ok=True)
        
        total_samples = len(sub_group_included)
        
        # ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã‚’ä½œæˆï¼ˆä¸¦åˆ—å®Ÿè¡Œç”¨ï¼‰
        tasks = []
        for included in sub_group_included:
            group_id_sample = included.get("id", "")
            if group_id_sample:
                tasks.append((bearer_token, group_id_sample, sample_dir))
        
        # ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œï¼ˆ50ä»¶ä»¥ä¸Šã§è‡ªå‹•ä¸¦åˆ—åŒ–ï¼‰
        def worker(token, group_id, samp_dir):
            """ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°"""
            try:
                url = f"https://rde-material-api.nims.go.jp/samples?groupId={group_id}&page%5Blimit%5D=1000&page%5Boffset%5D=0&fields%5Bsample%5D=names%2Cdescription%2Ccomposition"
                sample_json_path = os.path.join(samp_dir, f"{group_id}.json")
                
                # Material APIç”¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ˜Žç¤ºçš„ã«å–å¾—
                from config.common import load_bearer_token
                material_token = load_bearer_token('rde-material.nims.go.jp')
                headers_sample = _make_headers(material_token, host="rde-material-api.nims.go.jp", origin="https://rde-entry-arim.nims.go.jp", referer="https://rde-entry-arim.nims.go.jp/")
                resp = api_request("GET", url, bearer_token=material_token, headers=headers_sample, timeout=10)
                if resp is None:
                    logger.error(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±({group_id})ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼")
                    return "failed: request error"
                
                resp.raise_for_status()
                data = resp.json()
                
                # å¼·åˆ¶ä¸Šæ›¸ãä¿å­˜
                with open(sample_json_path, "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                    
                logger.info(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±({group_id})ã®å¼·åˆ¶å–å¾—ãƒ»ä¿å­˜ã«æˆåŠŸã—ã¾ã—ãŸ: {sample_json_path}")
                return "success"
                
            except Exception as e:
                logger.error(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±({group_id})ã®å–å¾—ãƒ»ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                return f"failed: {e}"
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’èª¿æ•´ï¼ˆ10-95%ã®ç¯„å›²ã«ãƒžãƒƒãƒ”ãƒ³ã‚°ï¼‰
        def adjusted_progress_callback(current, total, message):
            if progress_callback:
                progress_percent = 10 + int((current / 100) * 85)  # 10-95%
                return progress_callback(progress_percent, 100, message)
            return True
        
        result = parallel_download(
            tasks=tasks,
            worker_function=worker,
            max_workers=max_workers,
            progress_callback=adjusted_progress_callback,
            threshold=50
        )
        
        if progress_callback:
            if not progress_callback(95, 100, "ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å–å¾—å®Œäº†å‡¦ç†ä¸­..."):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
                
        result_msg = (f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å¼·åˆ¶å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚"
                     f"æˆåŠŸ: {result['success_count']}ä»¶, "
                     f"å¤±æ•—: {result['failed_count']}ä»¶, "
                     f"ç·æ•°: {result['total']}ä»¶")
        logger.info(result_msg)
        
        if progress_callback:
            if not progress_callback(100, 100, "å®Œäº†"):
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        if result['cancelled']:
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
        
        return result_msg
        
    except Exception as e:
        error_msg = f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å–å¾—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
        logger.error(error_msg)
        traceback.print_exc()
        return error_msg

def fetch_sample_info_from_subgroup_ids_only(bearer_token, output_dir=None):
    r"""
    subGroup.jsonã®å„IDã«ã¤ã„ã¦output\rde\data\samples\{id}.jsonã®ã¿ã‚’è»½é‡å–å¾—
    ãƒ‡ãƒ¼ã‚¿ç™»éŒ²å¾Œã®è‡ªå‹•å–å¾—ç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸé–¢æ•°
    """
    if not bearer_token:
        error_msg = "Bearerãƒˆãƒ¼ã‚¯ãƒ³ãŒå–å¾—ã§ãã¾ã›ã‚“"
        logger.error(error_msg)
        return error_msg
    
    try:
        # ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±ã‹ã‚‰å¯¾è±¡ã‚°ãƒ«ãƒ¼ãƒ—IDã‚’å–å¾—
        root_dir = output_dir or OUTPUT_RDE_DATA_DIR
        sub_group_path = os.path.join(root_dir, "subGroup.json")
        if not os.path.exists(sub_group_path):
            error_msg = "subGroup.jsonãŒå­˜åœ¨ã—ã¾ã›ã‚“"
            logger.error(error_msg)
            return error_msg
            
        with open(sub_group_path, "r", encoding="utf-8") as f:
            sub_group_data = json.load(f)
            
        sub_group_included = sub_group_data.get("included", [])
        if not sub_group_included:
            error_msg = "subGroup.jsonã«includedé…åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            logger.error(error_msg)
            return error_msg
        
        sample_dir = os.path.join(root_dir, "samples")
        os.makedirs(sample_dir, exist_ok=True)
        
        total_samples = len(sub_group_included)
        processed_samples = 0
        failed_samples = 0
        
        logger.info(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±è»½é‡å–å¾—é–‹å§‹: {total_samples}ä»¶")
        
        for idx, included in enumerate(sub_group_included):
            group_id_sample = included.get("id", "")
            
            if not group_id_sample:
                logger.warning(f"ã‚°ãƒ«ãƒ¼ãƒ—ID ãŒç©ºã®ãŸã‚ã€ã‚µãƒ³ãƒ—ãƒ«{idx + 1}ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
                continue
                    
            url = f"https://rde-material-api.nims.go.jp/samples?groupId={group_id_sample}&page%5Blimit%5D=1000&page%5Boffset%5D=0&fields%5Bsample%5D=names%2Cdescription%2Ccomposition"
            sample_json_path = os.path.join(sample_dir, f"{group_id_sample}.json")
            
            try:
                # Material APIç”¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ˜Žç¤ºçš„ã«å–å¾—
                from config.common import load_bearer_token
                material_token = load_bearer_token('rde-material.nims.go.jp')
                headers_sample = _make_headers(material_token, host="rde-material-api.nims.go.jp", origin="https://rde-entry-arim.nims.go.jp", referer="https://rde-entry-arim.nims.go.jp/")
                resp = api_request("GET", url, bearer_token=material_token, headers=headers_sample, timeout=10)
                if resp is None:
                    failed_samples += 1
                    logger.error(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±({group_id_sample})ã®å–å¾—ã«å¤±æ•—: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼")
                    continue
                
                resp.raise_for_status()
                data = resp.json()
                
                # è»½é‡ä¿å­˜ï¼ˆä¸Šæ›¸ãï¼‰
                with open(sample_json_path, "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                    
                processed_samples += 1
                logger.debug(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±({group_id_sample})ã®è»½é‡å–å¾—å®Œäº†: {sample_json_path}")
                
            except Exception as e:
                failed_samples += 1
                logger.error(f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±({group_id_sample})ã®å–å¾—å¤±æ•—: {e}")
                
        result_msg = f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±è»½é‡å–å¾—å®Œäº†: æˆåŠŸ={processed_samples}ä»¶, å¤±æ•—={failed_samples}ä»¶, ç·æ•°={total_samples}ä»¶"
        logger.info(result_msg)
        return result_msg
        
    except Exception as e:
        error_msg = f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±è»½é‡å–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}"
        logger.error(error_msg)
        return error_msg

def fetch_sample_info_for_dataset_only(bearer_token, dataset_id, output_dir=None):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDã®å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆJSONã‹ã‚‰ã‚°ãƒ«ãƒ¼ãƒ—IDã‚’å–å¾—ã—ã€
    ãã®ã‚°ãƒ«ãƒ¼ãƒ—ã®ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ã®ã¿ã‚’å–å¾—ã™ã‚‹ï¼ˆãƒ‡ãƒ¼ã‚¿ç™»éŒ²å¾Œã®è‡ªå‹•å–å¾—ç”¨ï¼‰
    
    æ³¨æ„: bearer_tokenã¯Optionalï¼ˆNoneå¯ï¼‰ã€‚APIå‘¼ã³å‡ºã—æ™‚ã«è‡ªå‹•é¸æŠžã•ã‚Œã‚‹ã€‚
    """
    # æ³¨æ„: Bearer Tokenãƒã‚§ãƒƒã‚¯ã‚’å‰Šé™¤ï¼ˆAPIå‘¼ã³å‡ºã—æ™‚ã«è‡ªå‹•é¸æŠžã•ã‚Œã‚‹ï¼‰
        
    if not dataset_id:
        error_msg = "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
        logger.error(error_msg)
        return error_msg
    
    try:
        # å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆJSONã‹ã‚‰ã‚°ãƒ«ãƒ¼ãƒ—IDã‚’å–å¾—
        root_dir = output_dir or OUTPUT_RDE_DATA_DIR
        dataset_json_path = os.path.join(root_dir, "datasets", f"{dataset_id}.json")
        if not os.path.exists(dataset_json_path):
            error_msg = f"å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆJSONãŒå­˜åœ¨ã—ã¾ã›ã‚“: {dataset_json_path}"
            logger.error(error_msg)
            return error_msg
            
        with open(dataset_json_path, "r", encoding="utf-8") as f:
            dataset_data = json.load(f)
            
        # data.relationships.group.data.id ã‹ã‚‰ã‚°ãƒ«ãƒ¼ãƒ—IDã‚’å–å¾—
        group_data = dataset_data.get("data", {}).get("relationships", {}).get("group", {}).get("data", {})
        group_id = group_data.get("id", "")
        
        if not group_id:
            error_msg = f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ{dataset_id}ã‹ã‚‰ã‚°ãƒ«ãƒ¼ãƒ—IDã‚’å–å¾—ã§ãã¾ã›ã‚“"
            logger.error(error_msg)
            return error_msg
        
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ{dataset_id}ã®ã‚°ãƒ«ãƒ¼ãƒ—IDå–å¾—: {group_id}")
        
        # ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ã‚’å–å¾—
        sample_dir = os.path.join(root_dir, "samples")
        os.makedirs(sample_dir, exist_ok=True)
        
        url = f"https://rde-material-api.nims.go.jp/samples?groupId={group_id}&page%5Blimit%5D=1000&page%5Boffset%5D=0&fields%5Bsample%5D=names%2Cdescription%2Ccomposition"
        sample_json_path = os.path.join(sample_dir, f"{group_id}.json")
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®šï¼ˆAuthorizationã¯å‰Šé™¤ã€api_requestå†…ã§è‡ªå‹•é¸æŠžï¼‰
        headers_sample = {
            "Accept": "application/vnd.api+json",
            "Accept-Encoding": "gzip, deflate, br, zstd",
            "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
            "Connection": "keep-alive",
            "Host": "rde-material-api.nims.go.jp",
            "Origin": "https://rde-entry-arim.nims.go.jp",
            "Referer": "https://rde-entry-arim.nims.go.jp/",
            "Sec-Fetch-Dest": "empty",
            "Sec-Fetch-Mode": "cors",
            "Sec-Fetch-Site": "same-site",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
            "sec-ch-ua": '"Not)A;Brand";v="8", "Chromium";v="138", "Google Chrome";v="138"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
        }
        
        # bearer_token=Noneã§è‡ªå‹•é¸æŠžï¼ˆMaterial APIç”¨ãƒˆãƒ¼ã‚¯ãƒ³ãŒè‡ªå‹•çš„ã«é¸ã°ã‚Œã‚‹ï¼‰
        resp = api_request("GET", url, bearer_token=None, headers=headers_sample, timeout=10)
        if resp is None:
            error_msg = f"ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±({group_id})ã®å–å¾—ã«å¤±æ•—: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼"
            logger.error(error_msg)
            return error_msg
        
        resp.raise_for_status()
        data = resp.json()
        
        # ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ã‚’ä¿å­˜
        with open(sample_json_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
        result_msg = f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ{dataset_id}ã®ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å–å¾—å®Œäº†: ã‚°ãƒ«ãƒ¼ãƒ—{group_id} -> {sample_json_path}"
        logger.info(result_msg)
        return result_msg
        
    except Exception as e:
        error_msg = f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ{dataset_id}ã®ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±å–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}"
        logger.error(error_msg)
        return error_msg

def fetch_common_info_only_logic(
    bearer_token,
    parent=None,
    webview=None,
    progress_callback=None,
    program_id=None,
    force_download=False,
):
    """
    7ç¨®é¡žã®å…±é€šæƒ…å ±JSONã®ã¿ã‚’å–å¾—ãƒ»ä¿å­˜ï¼ˆå€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆJSONã¯å–å¾—ã—ãªã„ï¼‰
    
    v2.0.1æ”¹å–„:
    - äº‹å‰ãƒˆãƒ¼ã‚¯ãƒ³æ¤œè¨¼ã®è¿½åŠ 
    - èªè¨¼ã‚¨ãƒ©ãƒ¼æ™‚ã®å†ãƒ­ã‚°ã‚¤ãƒ³ä¿ƒé€²
    - ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ˜Žç¢ºåŒ–
    
    v2.1.16è¿½åŠ :
    - program_idå¼•æ•°ã‚’è¿½åŠ ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—é¸æŠžæ©Ÿèƒ½å¯¾å¿œï¼‰
    """
    import traceback
    from datetime import datetime
    from core.bearer_token_manager import BearerTokenManager
    from qt_compat.widgets import QMessageBox
    
    # ===== APIè¨˜éŒ²åˆæœŸåŒ–ï¼ˆv2.1.16æ–°è¦è¿½åŠ ï¼‰ =====
    try:
        from net.api_call_recorder import reset_global_recorder
        session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        reset_global_recorder(session_id=session_id)
        logger.debug(f"APIã‚³ãƒ¼ãƒ«è¨˜éŒ²ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ: session_id={session_id}")
    except Exception as e:
        logger.debug(f"APIè¨˜éŒ²åˆæœŸåŒ–å¤±æ•—ï¼ˆéžè‡´å‘½çš„ï¼‰: {e}")
    
    # ===== 1. ãƒˆãƒ¼ã‚¯ãƒ³æ¤œè¨¼ï¼ˆv2.0.1æ–°è¦è¿½åŠ ï¼‰ =====
    logger.info("å…±é€šæƒ…å ±å–å¾—é–‹å§‹: ãƒˆãƒ¼ã‚¯ãƒ³æ¤œè¨¼")
    
    # bearer_tokenãŒæ¸¡ã•ã‚Œã¦ã„ãªã„ã€ã¾ãŸã¯ç©ºã®å ´åˆã¯BearerTokenManagerã‹ã‚‰å–å¾—
    if not bearer_token or bearer_token.strip() == "":
        logger.warning("bearer_tokenãŒæœªæŒ‡å®šã®ãŸã‚ã€BearerTokenManagerã‹ã‚‰å–å¾—ã—ã¾ã™")
        bearer_token = BearerTokenManager.get_valid_token()
    else:
        # æ¸¡ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼
        logger.debug("æ¸¡ã•ã‚ŒãŸbearer_tokenã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼ã—ã¾ã™")
        if not BearerTokenManager.validate_token(bearer_token):
            logger.warning("æ¸¡ã•ã‚ŒãŸbearer_tokenãŒç„¡åŠ¹ã§ã™")
            bearer_token = None
    
    # ãƒˆãƒ¼ã‚¯ãƒ³ãŒå–å¾—ã§ããªã„ã€ã¾ãŸã¯ç„¡åŠ¹ãªå ´åˆ
    if not bearer_token:
        error_msg = "èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ãŒç„¡åŠ¹ã¾ãŸã¯æœŸé™åˆ‡ã‚Œã§ã™ã€‚"
        logger.error(error_msg)
        
        # å†ãƒ­ã‚°ã‚¤ãƒ³ä¿ƒé€²ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’è¡¨ç¤º
        if parent and BearerTokenManager.request_relogin_if_invalid(parent):
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå†ãƒ­ã‚°ã‚¤ãƒ³ã‚’é¸æŠžã—ãŸå ´åˆ
            # ãƒ­ã‚°ã‚¤ãƒ³ã‚¿ãƒ–ã¸ã®åˆ‡ã‚Šæ›¿ãˆã‚’è©¦ã¿ã‚‹
            try:
                if hasattr(parent, 'tabs'):
                    # ãƒ¡ã‚¤ãƒ³ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ã‚¿ãƒ–ã‚’æ¤œç´¢
                    for i in range(parent.tabs.count()):
                        if parent.tabs.tabText(i) == "ãƒ­ã‚°ã‚¤ãƒ³":
                            parent.tabs.setCurrentIndex(i)
                            logger.info("ãƒ­ã‚°ã‚¤ãƒ³ã‚¿ãƒ–ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ")
                            break
                
                # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
                QMessageBox.information(
                    parent,
                    "å†ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦",
                    "ãƒ­ã‚°ã‚¤ãƒ³ã‚¿ãƒ–ã§RDEã‚·ã‚¹ãƒ†ãƒ ã«å†ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„ã€‚\n"
                    "ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†å¾Œã€å†åº¦å…±é€šæƒ…å ±å–å¾—ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
                )
            except Exception as e:
                logger.error(f"ãƒ­ã‚°ã‚¤ãƒ³ã‚¿ãƒ–åˆ‡ã‚Šæ›¿ãˆã‚¨ãƒ©ãƒ¼: {e}")
        
        return error_msg
    
    logger.info(f"ãƒˆãƒ¼ã‚¯ãƒ³æ¤œè¨¼æˆåŠŸ: {bearer_token[:20]}...")
    logger.info("å…±é€šæƒ…å ±å–å¾—å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")

    group_id = None
    project_group_id = None
    sub_group_data = None
    group_stage_executed = False

    def _exists(path: str) -> bool:
        return Path(path).exists()

    def _folder_has_files(folder_path: str, expected_count: Optional[int] = None) -> tuple[bool, int]:
        """ãƒ•ã‚©ãƒ«ãƒ€å†…ã®JSONãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ãƒã‚§ãƒƒã‚¯ã€‚existsã¯å¸¸ã«ç¢ºèªã€‚
        
        Returns:
            (has_any_files, actual_count): ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ã€å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°
        """
        folder = Path(folder_path)
        # ãƒ•ã‚©ãƒ«ãƒ€ã®å­˜åœ¨ç¢ºèªã¯ã‚¹ã‚­ãƒƒãƒ—ã—ãªã„ï¼ˆv2.1.21ï¼‰
        if not folder.exists():
            logger.debug(f"ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {folder_path}")
            return False, 0
        
        # *.json ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        json_files = list(folder.glob("*.json"))
        actual_count = len(json_files)
        
        # expected_countãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯æ¬ æåˆ¤å®š
        if expected_count is not None and actual_count < expected_count:
            logger.info(f"ãƒ•ã‚©ãƒ«ãƒ€å†…ã«æ¬ æãƒ•ã‚¡ã‚¤ãƒ«ã‚ã‚Š: {folder_path} (æœŸå¾…: {expected_count}ä»¶, å®Ÿéš›: {actual_count}ä»¶)")
            return True, actual_count  # æ¬ æãŒã‚ã£ã¦ã‚‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒ1ã¤ã§ã‚‚ã‚ã‚Œã°True
        
        return actual_count > 0, actual_count
    
    try:
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ç®¡ç† - 7æ®µéšŽã®å…±é€šæƒ…å ±å–å¾—
        stages = [
            ("ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—", 15),
            ("ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±å–å¾—", 25), 
            ("çµ„ç¹”ãƒ»è£…ç½®æƒ…å ±å–å¾—", 20),
            ("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§å–å¾—", 15),
            ("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ»è¨­å‚™ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±å–å¾—", 15),
            ("çµ±åˆæƒ…å ±ç”Ÿæˆ", 10)
        ]
        
        current_progress = 0
        
        def update_stage_progress(stage_index, stage_progress=100, sub_message=""):
            nonlocal current_progress
            if stage_index > 0:
                # å‰ã®æ®µéšŽã¾ã§å®Œäº†
                current_progress = sum(stage[1] for stage in stages[:stage_index])
            
            # ç¾åœ¨ã®æ®µéšŽã®é€²æ—ã‚’åŠ ç®—
            stage_weight = stages[stage_index][1]
            stage_contribution = (stage_progress / 100) * stage_weight
            total_progress = current_progress + stage_contribution
            
            stage_name = stages[stage_index][0]
            message = f"{stage_name}: {sub_message}" if sub_message else stage_name
            
            if progress_callback:
                return progress_callback(int(total_progress), 100, message)
            return True

        # 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼è‡ªèº«æƒ…å ±å–å¾—
        if not update_stage_progress(0, 0, "é–‹å§‹"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        try:
            if force_download or not Path(SELF_JSON_PATH).exists():
                logger.debug("fetch_self_info_from_api")
                fetch_self_info_from_api(bearer_token, parent_widget=parent)
            else:
                logger.info("ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±: æ—¢å­˜ã® self.json ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        except Exception as fetch_error:
            logger.error(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {fetch_error}")
            return "ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"
        
        if not update_stage_progress(0, 100, "å®Œäº†"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        # 2. ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±å–å¾—ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—ã€ã‚°ãƒ«ãƒ¼ãƒ—è©³ç´°ã€ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰
        if not update_stage_progress(1, 0, "ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±å–å¾—é–‹å§‹"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        import os
        force_project_dialog = os.environ.get('FORCE_PROJECT_GROUP_DIALOG', '0') == '1'

        def pipeline_progress_callback(current, total, message):
            total = total or 100
            mapped = int((current / total) * 100)
            mapped = max(0, min(100, mapped))
            return update_stage_progress(1, mapped, message)

        group_files_ready = all(
            _exists(path) for path in (GROUP_JSON_PATH, GROUP_DETAIL_JSON_PATH, SUBGROUP_JSON_PATH)
        )
        subgroups_complete = _subgroups_folder_complete() if group_files_ready else False
        if group_files_ready and not subgroups_complete and not force_download:
            logger.info("ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—è©³ç´°ã«æ¬ æãŒã‚ã‚‹ãŸã‚ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±ã‚’å†å–å¾—ã—ã¾ã™")
        use_cache = (not force_download) and group_files_ready and subgroups_complete
        group_pipeline = None

        if use_cache:
            try:
                with open(GROUP_DETAIL_JSON_PATH, "r", encoding="utf-8") as f:
                    cached_program_data = json.load(f)
                with open(SUBGROUP_JSON_PATH, "r", encoding="utf-8") as f:
                    cached_project_data = json.load(f)
                group_id = cached_program_data.get("data", {}).get("id")
                project_group_id = cached_project_data.get("data", {}).get("id")
                sub_group_data = cached_project_data
                if not group_id or not project_group_id:
                    raise ValueError("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«å¿…è¦ãªã‚°ãƒ«ãƒ¼ãƒ—IDãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
                logger.info("ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±: æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†åˆ©ç”¨ã—ã¾ã—ãŸ")
            except Exception as cache_error:
                logger.warning("ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£JSONã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸãŸã‚å†å–å¾—ã‚’å®Ÿè¡Œã—ã¾ã™: %s", cache_error)
                use_cache = False

        if not use_cache:
            try:
                group_pipeline = run_group_hierarchy_pipeline(
                    bearer_token=bearer_token,
                    parent_widget=parent,
                    preferred_program_id=program_id,
                    progress_callback=pipeline_progress_callback,
                    force_project_dialog=force_project_dialog,
                    force_download=force_download,
                )
                group_stage_executed = True
            except GroupFetchCancelled:
                logger.info("å…±é€šæƒ…å ±å–å¾—: ã‚°ãƒ«ãƒ¼ãƒ—éšŽå±¤å–å¾—ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
                return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            except Exception as pipeline_error:
                logger.error("å…±é€šæƒ…å ±å–å¾—: ã‚°ãƒ«ãƒ¼ãƒ—éšŽå±¤å–å¾—ã«å¤±æ•—", exc_info=True)
                return f"ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {pipeline_error}"

            group_id = group_pipeline.selected_program_id
            project_group_id = group_pipeline.selected_project_id
            sub_group_data = group_pipeline.selected_project_data

        if not update_stage_progress(1, 100, "å®Œäº†" if not use_cache else "ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Œäº†"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        # 3. çµ„ç¹”ãƒ»è£…ç½®æƒ…å ±å–å¾—
        if not update_stage_progress(2, 0, "çµ„ç¹”æƒ…å ±å–å¾—ä¸­"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
        logger.debug("fetch_organization_info_from_api")
        org_json_path = [OUTPUT_DIR, "rde", "data", "organization.json"]
        if force_download or not _exists(ORGANIZATION_JSON_PATH):
            fetch_organization_info_from_api(bearer_token, org_json_path)
        else:
            logger.info("çµ„ç¹”æƒ…å ±: æ—¢å­˜ã® organization.json ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        
        if not update_stage_progress(2, 50, "è£…ç½®ã‚¿ã‚¤ãƒ—æƒ…å ±å–å¾—ä¸­"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
        logger.debug("fetch_instrument_type_info_from_api")
        instrument_type_json_path = [OUTPUT_DIR, "rde", "data", "instrumentType.json"]
        if force_download or not _exists(INSTRUMENT_TYPE_JSON_PATH):
            fetch_instrument_type_info_from_api(bearer_token, instrument_type_json_path)
        else:
            logger.info("è£…ç½®ã‚¿ã‚¤ãƒ—æƒ…å ±: æ—¢å­˜ã® instrumentType.json ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        
        if not update_stage_progress(2, 100, "å®Œäº†"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        # 4. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§å–å¾—ï¼ˆå€‹åˆ¥è©³ç´°ã¯å–å¾—ã—ãªã„ï¼‰
        if not update_stage_progress(3, 0, "é–‹å§‹"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
        logger.debug("fetch_dataset_list_only")
        if force_download or not _exists(DATASET_JSON_PATH):
            fetch_dataset_list_only(bearer_token, output_dir=os.path.join(OUTPUT_DIR, "rde", "data"))
        else:
            logger.info("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§: æ—¢å­˜ã® dataset.json ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        
        if not update_stage_progress(3, 100, "å®Œäº†"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        # 5. ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ»è¨­å‚™ãƒ»ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±å–å¾—
        if not update_stage_progress(4, 0, "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæƒ…å ±å–å¾—ä¸­"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
        logger.debug("fetch_template_info_from_api")
        if force_download or not _exists(TEMPLATE_JSON_PATH):
            fetch_template_info_from_api(bearer_token)
        else:
            logger.info("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæƒ…å ±: æ—¢å­˜ã® template.json ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        
        if not update_stage_progress(4, 33, "è¨­å‚™æƒ…å ±å–å¾—ä¸­"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
        logger.debug("fetch_instruments_info_from_api")
        if force_download or not _exists(INSTRUMENTS_JSON_PATH):
            fetch_instruments_info_from_api(bearer_token)
        else:
            logger.info("è¨­å‚™æƒ…å ±: æ—¢å­˜ã® instruments.json ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        
        if not update_stage_progress(4, 66, "åˆ©ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±å–å¾—ä¸­"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
        logger.debug("fetch_licenses_info_from_api")
        if force_download or not _exists(LICENSES_JSON_PATH):
            fetch_licenses_info_from_api(bearer_token)
        else:
            logger.info("åˆ©ç”¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±: æ—¢å­˜ã® licenses.json ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        
        if not update_stage_progress(4, 100, "å®Œäº†"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"

        # 6. çµ±åˆæƒ…å ±ç”Ÿæˆ
        if not update_stage_progress(5, 0, "é–‹å§‹"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
        # info.jsonç”Ÿæˆ
        should_generate_info = sub_group_data and (
            force_download or group_stage_executed or not _exists(INFO_JSON_PATH)
        )
        if should_generate_info:
            try:
                logger.debug("extract_users_and_subgroups")
                users, subgroups = extract_users_and_subgroups(sub_group_data)
                info = {
                    'group_id': group_id,
                    'project_group_id': project_group_id,
                    'users': users,
                    'subgroups': subgroups
                }
                info_json_path = [OUTPUT_DIR, 'rde', 'data', 'info.json']
                save_json(info, *info_json_path)
                logger.info("info.jsonï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ»ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±ï¼‰ã‚’æ›¸ãå‡ºã—ã¾ã—ãŸã€‚")
            except Exception as e:
                logger.error(f"subGroup.jsonã®è§£æžãƒ»è¡¨ç¤ºã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                traceback.print_exc()
        elif sub_group_data:
            logger.info("info.json: æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                
        if not update_stage_progress(5, 100, "å®Œäº†"):
            return "ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ"
            
        result_msg = "å…±é€šæƒ…å ±å–å¾—ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ"
        logger.info(result_msg)
        return result_msg
        
    except Exception as e:
        error_msg = f"å…±é€šæƒ…å ±å–å¾—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
        logger.error(error_msg)
        traceback.print_exc()
        return error_msg

def fetch_dataset_list_only(bearer_token, output_dir=None):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§ã®ã¿ã‚’å–å¾—ã—ã€dataset.jsonã¨ã—ã¦ä¿å­˜ï¼ˆå€‹åˆ¥JSONã¯å–å¾—ã—ãªã„ï¼‰"""
    # ãƒ‘ã‚¹åŒºåˆ‡ã‚Šã‚’çµ±ä¸€
    output_dir = os.path.normpath(output_dir or OUTPUT_RDE_DATA_DIR)

    headers = _make_headers(bearer_token, host="rde-api.nims.go.jp", origin="https://rde.nims.go.jp", referer="https://rde.nims.go.jp/")

    try:
        dataset_payload = _download_dataset_list_in_chunks(
            bearer_token=bearer_token,
            headers=headers,
            search_words=None,
        )
    except Exception as e:
        logger.error("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: %s", e)


    os.makedirs(output_dir, exist_ok=True)
    save_path = os.path.join(output_dir, "dataset.json")

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ
    if os.path.exists(save_path):
        backup_path = save_path + ".backup"
        try:
            shutil.copy2(save_path, backup_path)
            logger.info("æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ: %s", backup_path)
        except Exception as backup_error:
            logger.warning("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆã«å¤±æ•—: %s", backup_error)

    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(dataset_payload, f, ensure_ascii=False, indent=2)
    logger.info("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§(dataset.json)ã®å–å¾—ãƒ»ä¿å­˜ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
def get_json_status_info():
    """
    JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å–å¾—çŠ¶æ³ï¼ˆæ—¥æ™‚ã€ãƒ•ã‚¡ã‚¤ãƒ«æ•°ç­‰ï¼‰ã‚’å–å¾—
    """
    import glob
    from datetime import datetime
    
    json_info = {}
    base_path = os.path.join(OUTPUT_DIR, "rde", "data")
    
    # 10ç¨®é¡žã®å…±é€šJSONãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±ã‚’è¿½åŠ ï¼‰
    common_files = [
        "self.json", "group.json", "groupDetail.json", "subGroup.json",
        "organization.json", "instrumentType.json", "template.json", 
        "instruments.json", "licenses.json", "info.json", "dataset.json"
    ]
    
    for file_name in common_files:
        file_path = os.path.join(base_path, file_name)
        if os.path.exists(file_path):
            mtime = os.path.getmtime(file_path)
            json_info[file_name] = {
                "exists": True,
                "modified": datetime.fromtimestamp(mtime).strftime("%Y-%m-%d %H:%M:%S"),
                "size_kb": round(os.path.getsize(file_path) / 1024, 2)
            }
        else:
            json_info[file_name] = {"exists": False, "modified": "æœªå–å¾—", "size_kb": 0}
    
    # å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆJSONæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    datasets_dir = os.path.join(base_path, "datasets")
    dataset_count = len(glob.glob(os.path.join(datasets_dir, "*.json"))) if os.path.exists(datasets_dir) else 0
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªJSONæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    dataentry_dir = os.path.join(base_path, "dataEntry")
    dataentry_count = len(glob.glob(os.path.join(dataentry_dir, "*.json"))) if os.path.exists(dataentry_dir) else 0
    
    # ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±JSONæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    samples_dir = os.path.join(base_path, "samples")
    sample_count = len(glob.glob(os.path.join(samples_dir, "*.json"))) if os.path.exists(samples_dir) else 0
    
    json_info["summary"] = {
        "individual_datasets": dataset_count,
        "data_entries": dataentry_count,
        "sample_files": sample_count,
        "common_files_count": len([f for f in common_files if json_info[f]["exists"]])
    }
    
    return json_info

# XLSXæ›¸ãå‡ºã—ãƒ­ã‚¸ãƒƒã‚¯ã¯ xlsx_exporter.py ã«åˆ†é›¢

## XLSXã‚µãƒžãƒªãƒ¼æ›¸ãå‡ºã—ãƒ­ã‚¸ãƒƒã‚¯ã‚‚ xlsx_exporter.py ã«åˆ†é›¢

def write_summary_sheet(wb, parent):
    import json, os
    logger.debug("[XLSX] write_summary_sheet called")

    def load_json(path):
        abs_path = os.path.abspath(path)
        if not os.path.exists(abs_path):
            logger.error("%sãŒå­˜åœ¨ã—ã¾ã›ã‚“: %s", path, abs_path)
            return None
        with open(abs_path, "r", encoding="utf-8") as f:
            return json.load(f)

    # å„ç¨®JSONãƒ­ãƒ¼ãƒ‰
    sub_group_json = load_json(SUBGROUP_JSON_PATH)
    dataset_json = load_json(get_dynamic_file_path("output/rde/data/dataset.json"))
    instruments_json = load_json(get_dynamic_file_path("output/rde/data/instruments.json"))
    templates_json = load_json(get_dynamic_file_path("output/rde/data/template.json"))
    if not all([sub_group_json, dataset_json, instruments_json, templates_json]):
        return

    subGroup_included = sub_group_json.get("included", [])

def get_stage_completion_status():
    """
    å„æ®µéšŽã®å®Œäº†çŠ¶æ³ã‚’å–å¾—ã™ã‚‹
    """
    base_path = os.path.join(OUTPUT_DIR, "rde", "data")

    def _dir_has_any_entry(path: str) -> bool:
        try:
            with os.scandir(path) as it:
                for _ in it:
                    return True
            return False
        except Exception:
            return False
    
    stages = {
        "ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±": ["self.json"],
        "ã‚°ãƒ«ãƒ¼ãƒ—é–¢é€£æƒ…å ±": ["group.json", "groupDetail.json", "subGroup.json"],
        "çµ„ç¹”ãƒ»è£…ç½®æƒ…å ±": ["organization.json", "instrumentType.json"],
        "ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±": ["samples"],  # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±": ["dataset.json", "datasets"],  # ãƒ•ã‚¡ã‚¤ãƒ«+ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        "ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±": ["dataEntry"],  # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        "ã‚¤ãƒ³ãƒœã‚¤ã‚¹æƒ…å ±": ["invoice"],  # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        "invoiceSchemaæƒ…å ±": ["invoiceSchemas"],  # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ»è¨­å‚™æƒ…å ±": ["template.json", "instruments.json"],
        "çµ±åˆæƒ…å ±ç”Ÿæˆ": ["info.json"]
    }
    
    status = {}
    
    for stage_name, required_items in stages.items():
        completed_items = 0
        total_items = len(required_items)
        
        for item in required_items:
            item_path = os.path.join(base_path, item)
            if os.path.exists(item_path):
                if os.path.isfile(item_path):
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯
                    if os.path.getsize(item_path) > 0:
                        completed_items += 1
                elif os.path.isdir(item_path):
                    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å ´åˆã¯ä¸­èº«ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if _dir_has_any_entry(item_path):
                        completed_items += 1
        
        completion_rate = (completed_items / total_items) * 100 if total_items > 0 else 0
        status[stage_name] = {
            "completed": completed_items,
            "total": total_items,
            "rate": completion_rate,
            "status": "å®Œäº†" if completion_rate == 100 else "æœªå®Œäº†" if completion_rate == 0 else "éƒ¨åˆ†å®Œäº†"
        }
    
    return status
    dataset_data = dataset_json.get("data", [])
    instruments_data = instruments_json.get("data", [])

    # --- 3å±¤æ§‹é€ ãƒ˜ãƒƒãƒ€å®šç¾© ---
    HEADER_DEF = [
        {"id": "subGroupName", "label": "ã‚µãƒ–ã‚°ãƒ«ãƒ¼ãƒ—å"},
        {"id": "dataset_manager_name", "label": "ç®¡ç†è€…å"},
        {"id": "dataset_applicant_name", "label": "ç”³è«‹è€…å"},
        {"id": "dataset_owner_names_str", "label": "ã‚ªãƒ¼ãƒŠãƒ¼åãƒªã‚¹ãƒˆ"},
        {"id": "grantNumber", "label": "èª²é¡Œç•ªå·"},
        {"id": "title", "label": "èª²é¡Œå"},
        {"id": "datasetName", "label": "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå"},
        {"id": "instrument_name", "label": "è£…ç½®å"},
        {"id": "instrument_local_id", "label": "è£…ç½® ID"},
        {"id": "template_id", "label": "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆID"},
        {"id": "datasetId", "label": "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆID"},
        {"id": "dataEntryName", "label": "ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªå"},
        {"id": "dataEntryId", "label": "ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªID"},
        {"id": "number_of_files", "label": "ãƒ•ã‚¡ã‚¤ãƒ«æ•°"},
        {"id": "number_of_image_files", "label": "ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«æ•°"},
        {"id": "date_of_dataEntry_creation", "label": "ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªä½œæˆæ—¥"},
        {"id": "total_file_size_MB", "label": "ãƒ•ã‚¡ã‚¤ãƒ«åˆè¨ˆã‚µã‚¤ã‚º(MB)"},
        {"id": "dataset_embargoDate", "label": "ã‚¨ãƒ³ãƒãƒ¼ã‚´æ—¥"},
        {"id": "dataset_isAnonymized", "label": "åŒ¿ååŒ–"},
        {"id": "dataset_description", "label": "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª¬æ˜Ž"},
        {"id": "dataset_relatedLinks", "label": "é–¢é€£ãƒªãƒ³ã‚¯"},
        {"id": "dataset_relatedDatasets", "label": "é–¢é€£ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"},
    ]
    # instrument_local_idåˆ—ã«ã¯instruments.jsonã®attributes.programs[].localIdã‚’å‡ºåŠ›
    instrument_id_to_localid = {}
    for inst in instruments_data:
        inst_id = inst.get("id")
        programs = inst.get("attributes", {}).get("programs", [])
        # è¤‡æ•°programsãŒã‚ã‚‹å ´åˆã¯ã‚«ãƒ³ãƒžåŒºåˆ‡ã‚Šã§é€£çµ
        local_ids = [prog.get("localId", "") for prog in programs if prog.get("localId")]
        if inst_id and local_ids:
            instrument_id_to_localid[inst_id] = ",".join(local_ids)
    SHEET_NAME = "summary"
    if SHEET_NAME in wb.sheetnames:
        ws = wb[SHEET_NAME]
        # æ—¢å­˜ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼ˆ1è¡Œç›®ï¼‰ã‚’å–å¾—
        existing_id_row = [cell.value for cell in ws[1]] if ws.max_row >= 1 else []
    else:
        ws = wb.create_sheet(SHEET_NAME)
        existing_id_row = []

    # æ—¢å­˜IDåˆ—ã®é †ç•ªã‚’å„ªå…ˆã—ã€ãªã‘ã‚Œã°HEADER_DEFé †ã§è¿½åŠ ï¼ˆç©ºæ–‡å­—åˆ—ã‚„Noneã¯é™¤å¤–ï¼‰
    header_ids = []
    id_to_label = {coldef["id"]: coldef["label"] for coldef in HEADER_DEF}
    if existing_id_row and any(existing_id_row):
        # æ—¢å­˜ãƒ˜ãƒƒãƒ€ãƒ¼ã®é †ç•ªï¼ˆç©ºæ–‡å­—åˆ—ã‚„Noneã¯é™¤å¤–ï¼‰
        header_ids = [id_ for id_ in existing_id_row if id_ not in (None, "") and str(id_).strip() != ""]
        # HEADER_DEFã«ã‚ã‚‹ãŒæ—¢å­˜ãƒ˜ãƒƒãƒ€ãƒ¼ã«ãªã„ã‚‚ã®ã‚’è¿½åŠ 
        for coldef in HEADER_DEF:
            if coldef["id"] not in header_ids:
                header_ids.append(coldef["id"])
    else:
        header_ids = [coldef["id"] for coldef in HEADER_DEF]
    # 1è¡Œç›®:IDï¼ˆç©ºå€¤åˆ—ã¯é™¤å¤–æ¸ˆã¿ã ãŒå¿µã®ãŸã‚ï¼‰
    for col_idx, id_ in enumerate(header_ids, 1):
        if id_ not in (None, "") and str(id_).strip() != "":
            ws.cell(row=1, column=col_idx, value=id_)
    # 2è¡Œç›®:ãƒ©ãƒ™ãƒ«
    for col_idx, id_ in enumerate(header_ids, 1):
        if id_ not in (None, "") and str(id_).strip() != "":
            ws.cell(row=2, column=col_idx, value=id_to_label.get(id_, id_))
    id_to_col = {id_: idx+1 for idx, id_ in enumerate(header_ids) if id_ not in (None, "") and str(id_).strip() != ""}

    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ï¼ˆ3è¡Œç›®ä»¥é™ï¼‰
    # datasetId, dataEntryId ã‚’ã‚­ãƒ¼ã«ã€æ‰‹å‹•åˆ—ï¼ˆHEADER_DEFã«ãªã„åˆ—ï¼‰ã®å€¤ã‚’ä¿å­˜
    manual_col_ids = [id_ for id_ in header_ids if id_ not in [coldef["id"] for coldef in HEADER_DEF] and id_ not in (None, "") and str(id_).strip() != ""]
    manual_data_map = {}  # key: ("dataEntryId", id) or ("datasetId", id) -> {manual_col: value, ...}
    for row in ws.iter_rows(min_row=3, max_row=ws.max_row, max_col=len(header_ids)):
        # header_idsã¨rowã®é•·ã•ãŒç•°ãªã‚‹å ´åˆã‚‚å®‰å…¨ã«ãƒšã‚¢åŒ–
        row_dict = {id_: cell.value for id_, cell in zip(header_ids, row) if id_ not in (None, "") and str(id_).strip() != ""}
        dataset_id = row_dict.get("datasetId", "")
        data_entry_id = row_dict.get("dataEntryId", "")
        if data_entry_id:
            manual_data_map[("dataEntryId", data_entry_id)] = {col: row_dict.get(col, None) for col in manual_col_ids if col not in (None, "") and str(col).strip() != ""}
        elif dataset_id:
            manual_data_map[("datasetId", dataset_id)] = {col: row_dict.get(col, None) for col in manual_col_ids if col not in (None, "") and str(col).strip() != ""}
    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ—¦å…¨å‰Šé™¤ï¼ˆ3è¡Œç›®ä»¥é™ï¼‰
    if ws.max_row >= 3:
        ws.delete_rows(3, ws.max_row - 2)

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDâ†’åå‰è¾žæ›¸
    user_id_to_name = {user.get("id"): user.get("attributes", {}).get("userName", "") for user in subGroup_included if user.get("type") == "user"}
    instrument_id_to_name = {inst.get("id"): inst.get("attributes", {}).get("nameJa", "") for inst in instruments_data}

    from dateutil.parser import parse as parse_datetime
    def to_ymd(date_str):
        if not date_str:
            return ""
        try:
            dt = parse_datetime(date_str)
            return dt.strftime("%Y-%m-%d")
        except Exception:
            return date_str

    def get_dataset_related_info(dataset_datum):
        attr = dataset_datum.get("attributes", {})
        rel = dataset_datum.get("relationships", {})
        return {
            "id": dataset_datum.get("id", ""),
            "manager_id": rel.get("manager", {}).get("data", {}).get("id", ""),
            "owners": rel.get("dataOwners", {}).get("data", []),
            "applicant_id": rel.get("applicant", {}).get("data", {}).get("id", ""),
            "template_id": rel.get("template", {}).get("data", {}).get("id", ""),
            "instrument_id": rel.get("instruments", {}).get("data", [{}])[0].get("id", "") if rel.get("instruments", {}).get("data") else "",
            "embargoDate": to_ymd(attr.get("embargoDate", "")),
            "isAnonymized": attr.get("isAnonymized", ""),
            "description": attr.get("description", ""),
            "relatedLinks_str": "\n".join([link.get("url", "") for link in attr.get("relatedLinks", []) if isinstance(link, dict)]),
            "relatedDatasets_urls_str": "\n".join([f"https://rde.nims.go.jp/datasets/rde/{rd.get('id', '')}" for rd in rel.get("relatedDatasets", {}).get("data", []) if isinstance(rd, dict)]),
            "grantNumber": attr.get("grantNumber", ""),
            "name": attr.get("name", ""),
            "title": attr.get("subjectTitle", ""),
            
        }

    row_idx = 3
    for subGroup in subGroup_included:
        if subGroup.get("type") != "group":
            continue
        subGroup_attr = subGroup.get("attributes", {})
        subGroup_name = subGroup_attr.get("name", "")
        subGroup_subjects = subGroup_attr.get("subjects", {})
        for subject in subGroup_subjects:
            grantNumber = subject.get("grantNumber", "") if isinstance(subject, dict) else ""
            title = subject.get("title", "") if isinstance(subject, dict) else ""
            for dataset in dataset_data:
                ds_info = get_dataset_related_info(dataset)
                if ds_info["grantNumber"] != grantNumber:
                    continue
                manager_name = user_id_to_name.get(ds_info["manager_id"], "æœªè¨­å®š" if ds_info["manager_id"] in [None, ""] else "")
                applicant_name = user_id_to_name.get(ds_info["applicant_id"], "")
                owner_names = [user_id_to_name.get(owner.get("id", ""), "") for owner in ds_info["owners"] if owner.get("id", "")]
                owner_names_str = "\n".join([n for n in owner_names if n])
                instrument_name = instrument_id_to_name.get(ds_info["instrument_id"], "")
                instrument_local_id = instrument_id_to_localid.get(ds_info["instrument_id"], "")
                dataset_url = f"https://rde.nims.go.jp/datasets/rde/{ds_info['id']}"

                dataEntry_path = get_dynamic_file_path(f"output/rde/data/dataEntry/{ds_info['id']}.json")
                dataEntry_json = load_json(dataEntry_path)
                if not dataEntry_json:
                    print(f"[ERROR] dataEntry JSONãŒå­˜åœ¨ã—ã¾ã›ã‚“: {dataEntry_path} for dataset_id={ds_info['id']}" )
                    continue
                dataEntry_data = dataEntry_json.get("data", [])
                dataEntry_included = dataEntry_json.get("included", [])
                total_file_size = sum(
                    inc.get("attributes", {}).get("fileSize", 0)
                    for inc in dataEntry_included if inc.get("type") == "file"
                )
                total_file_size_MB = total_file_size / (1024 * 1024) if total_file_size else 0

                def write_row(value_dict):
                    # æ—¢å­˜åˆ—ã«ãƒ‡ãƒ¼ã‚¿ãŒãªã‘ã‚Œã°æ—¢å­˜å€¤ã‚’ç¶­æŒ
                    # datasetId, dataEntryIdã§æ‰‹å‹•åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒ
                    dataset_id = value_dict.get("datasetId", "")
                    data_entry_id = value_dict.get("dataEntryId", "")
                    if data_entry_id and ("dataEntryId", data_entry_id) in manual_data_map:
                        manual_restore = manual_data_map[("dataEntryId", data_entry_id)]
                    elif dataset_id and ("datasetId", dataset_id) in manual_data_map:
                        manual_restore = manual_data_map[("datasetId", dataset_id)]
                    else:
                        manual_restore = {}
                    for id_ in header_ids:
                        col = id_to_col[id_]
                        if id_ in value_dict:
                            ws.cell(row=row_idx, column=col, value=value_dict[id_])
                        elif id_ in manual_restore:
                            ws.cell(row=row_idx, column=col, value=manual_restore[id_])
                        else:
                            # æ—¢å­˜å€¤ç¶­æŒï¼ˆopenpyxlã¯æ–°è¦è¡Œã¯Noneãªã®ã§ä½•ã‚‚ã—ãªã„ï¼‰
                            pass
                    # value_dictã«ã®ã¿å­˜åœ¨ã™ã‚‹æ–°è¦IDã¯æœ«å°¾ã«è¿½åŠ 
                    for id_ in value_dict:
                        if id_ not in header_ids:
                            header_ids.append(id_)
                            col = len(header_ids)
                            ws.cell(row=1, column=col, value=id_)
                            ws.cell(row=2, column=col, value=id_to_label.get(id_, id_))
                            ws.cell(row=row_idx, column=col, value=value_dict[id_])
                            id_to_col[id_] = col

                # è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªå¯¾å¿œ
                if dataEntry_data:
                    for entry in dataEntry_data:
                        entry_attr = entry.get("attributes", {})
                        dataEntry_name = entry_attr.get("name", "")
                        dataEntry_id = entry.get("id", "")
                        number_of_files = entry_attr.get("numberOfFiles", "")
                        number_of_image_files = entry_attr.get("numberOfImageFiles", "")
                        date_of_dataEntry_creation = entry_attr.get("created", "")
                        value_dict = {
                            "subGroupName": subGroup_name,
                            "dataset_manager_name": manager_name,
                            "dataset_applicant_name": applicant_name,
                            "dataset_owner_names_str": owner_names_str,
                            "grantNumber": grantNumber,
                            "title": title,
                            "datasetName": ds_info["name"],
                            "instrument_name": instrument_name,
                            "instrument_local_id": instrument_local_id,
                            "template_id": ds_info["template_id"],
                            "datasetId": dataset_url,
                            "dataEntryName": dataEntry_name,
                            "dataEntryId": dataEntry_id,
                            "number_of_files": number_of_files,
                            "number_of_image_files": number_of_image_files,
                            "date_of_dataEntry_creation": to_ymd(date_of_dataEntry_creation),
                            "total_file_size_MB": total_file_size_MB,
                            "dataset_embargoDate": ds_info["embargoDate"],
                            "dataset_isAnonymized": ds_info["isAnonymized"],
                            "dataset_description": ds_info["description"],
                            "dataset_relatedLinks": ds_info["relatedLinks_str"],
                            "dataset_relatedDatasets": ds_info["relatedDatasets_urls_str"],
                        }
                        write_row(value_dict)
                        row_idx += 1
                else:
                    # ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªãŒãªã„å ´åˆã‚‚ç©ºã§1è¡Œå‡ºã™
                    value_dict = {
                        "subGroupName": subGroup_name,
                        "dataset_manager_name": manager_name,
                        "dataset_applicant_name": applicant_name,
                        "dataset_owner_names_str": owner_names_str,
                        "grantNumber": grantNumber,
                        "title": title,
                        "datasetName": ds_info["name"],
                        "instrument_name": instrument_name,
                        "instrument_local_id": instrument_local_id,
                        "template_id": ds_info["template_id"],
                        "datasetId": dataset_url,
                        "dataEntryName": "",
                        "dataEntryId": "",
                        "number_of_files": "",
                        "number_of_image_files": "",
                        "date_of_dataEntry_creation": "",
                        "total_file_size_MB": total_file_size_MB,
                        "dataset_embargoDate": ds_info["embargoDate"],
                        "dataset_isAnonymized": ds_info["isAnonymized"],
                        "dataset_description": ds_info["description"],
                        "dataset_relatedLinks": ds_info["relatedLinks_str"],
                        "dataset_relatedDatasets": ds_info["relatedDatasets_urls_str"],
                    }
                    write_row(value_dict)
                    row_idx += 1

def fetch_invoice_schema_from_api(
    bearer_token,
    template_id,
    output_dir,
    summary,
    log_path,
    summary_path,
    team_id_candidates=None,
    summary_lock=None,
):
    """
    æŒ‡å®štemplate_idã®invoiceSchemasã‚’å–å¾—ã—ä¿å­˜ã€‚æˆåŠŸãƒ»å¤±æ•—ã‚’summary/logã«è¨˜éŒ²ã€‚
    æ—¢ã«summary.success/ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—ã€‚
    """
    filepath = os.path.join(output_dir, "invoiceSchemas", f"{template_id}.json")
    # æ—¢ã«æˆåŠŸè¨˜éŒ²ãŒã‚ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—
    if template_id in summary.get("success", []):
        with open(summary_path, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        return "skipped_summary"
    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—
    if os.path.exists(filepath):
        with open(summary_path, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        return "skipped_file"
    # NOTE: teamId ãŒç„¡ã„ã¨å¤šãã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§403/404ã«ãªã‚Šã€å–å¾—ä»¶æ•°ãŒæ¿€æ¸›ã™ã‚‹ã€‚
    # ãã®ãŸã‚ teamId å€™è£œã‚’ä»˜ã‘ã¦å–å¾—ã™ã‚‹ï¼ˆå€™è£œã¯ subGroup.json ã® TEAM group ã‹ã‚‰æŠ½å‡ºï¼‰ã€‚
    headers = _make_headers(bearer_token, host="rde-api.nims.go.jp", origin="https://rde.nims.go.jp", referer="https://rde.nims.go.jp/")
    from contextlib import nullcontext

    lock_ctx = summary_lock if summary_lock is not None else nullcontext()
    candidates = team_id_candidates if isinstance(team_id_candidates, list) and team_id_candidates else [DEFAULT_TEAM_ID]
    # NOTE: 2025-12-18: teamIdå€™è£œã®ãƒªãƒˆãƒ©ã‚¤ã¯è¡Œã‚ãªã„ã€‚
    # åŸºæœ¬æƒ…å ±/InvoiceSchemaå–å¾—ãƒœã‚¿ãƒ³ã®å‹•ä½œã¨ã—ã¦ã€æœ€åˆã®å€™è£œã®ã¿ã‚’ä½¿ç”¨ã—ã€å¤±æ•—ã—ã¦ã‚‚ä»–å€™è£œã¯è©¦ã•ãªã„ã€‚
    candidates = candidates[:1]

    # summaryã®æœ€ä½Žé™ã®æ•´åˆæ€§ã‚’ä¿è¨¼
    if not isinstance(summary, dict):
        return "failed: invalid summary"
    summary.setdefault("success", [])
    if not isinstance(summary.get("success"), list):
        summary["success"] = []
    summary.setdefault("failed", {})
    if not isinstance(summary.get("failed"), dict):
        summary["failed"] = {}

    try:
        team_id = candidates[0]
        url = f"https://rde-api.nims.go.jp/invoiceSchemas/{template_id}?teamId={team_id}"
        resp = api_request("GET", url, bearer_token=bearer_token, headers=headers, timeout=10)

        if resp is None:
            last_status = 0
            last_error = "Request failed"
        else:
            last_status = getattr(resp, "status_code", None)

            # tokenç„¡åŠ¹ã¯teamIdã«ä¾ã‚‰ãªã„ã®ã§å³çµ‚äº†
            if last_status == 401:
                last_error = "HTTP 401 Unauthorized"
            elif last_status in (403, 404):
                # teamIdé•ã„/æ¨©é™/å­˜åœ¨å·®åˆ†ã®å¯èƒ½æ€§ã¯ã‚ã‚‹ãŒã€å€™è£œã®ãƒªãƒˆãƒ©ã‚¤ã¯è¡Œã‚ãªã„
                last_error = f"HTTP {last_status}"
            else:
                resp.raise_for_status()
                data = resp.json()
                with open(filepath, "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)

                with lock_ctx:
                    summary["success"].append(template_id)
                    summary["failed"].pop(template_id, None)
                    with open(log_path, "a", encoding="utf-8") as logf:
                        logf.write(
                            f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} [SUCCESS] template_id={template_id} teamId={team_id}\n"
                        )
                    with open(summary_path, "w", encoding="utf-8") as f:
                        json.dump(summary, f, ensure_ascii=False, indent=2)

                return "success"

        with lock_ctx:
            summary["failed"][template_id] = last_error or "failed"
            with open(log_path, "a", encoding="utf-8") as logf:
                logf.write(
                    f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} [FAILED] template_id={template_id} "
                    f"status={last_status} error={last_error}\n"
                )
            with open(summary_path, "w", encoding="utf-8") as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)

        return "failed"

    except Exception as e:
        with lock_ctx:
            summary["failed"][template_id] = str(e)
            with open(log_path, "a", encoding="utf-8") as logf:
                logf.write(
                    f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} [FAILED] template_id={template_id} error={e}\n"
                )
            with open(summary_path, "w", encoding="utf-8") as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
        return "failed"


# ========================================
# UIControllerç”¨ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°
# ========================================

