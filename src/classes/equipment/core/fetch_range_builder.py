"""
è¨­å‚™ãƒ‡ãƒ¼ã‚¿å–å¾— - ãƒ•ã‚§ãƒƒãƒç¯„å›²ãƒ“ãƒ«ãƒ€ãƒ¼

å…¨ä»¶å–å¾—ãƒ¢ãƒ¼ãƒ‰ç”¨ã®IDã‚¹ã‚­ãƒ£ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã‚’æä¾›ã—ã¾ã™ã€‚
"""

from __future__ import annotations

import logging
import math
from typing import Callable, Optional

from classes.equipment.core.facility_listing import (
    FacilityListingScraper,
    LISTING_PER_PAGE,
)

logger = logging.getLogger(__name__)

LogCallback = Optional[Callable[[str], None]]
CancelChecker = Optional[Callable[[], bool]]


def collect_valid_facility_ids(
    start_id: int,
    end_id: int,
    chunk_size: int,
    stop_threshold: int,
    log_callback: LogCallback = None,
    cancel_checker: CancelChecker = None,
    listing_scraper: Optional[FacilityListingScraper] = None,
) -> list[int]:
    """è¨­å‚™ä¸€è¦§ã®ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½¿ã£ã¦æœ‰åŠ¹ãªè¨­å‚™IDã‚’åé›†ã™ã‚‹

    Args:
        start_id: å–å¾—å¯¾è±¡ã®é–‹å§‹è¨­å‚™ID
        end_id: å–å¾—å¯¾è±¡ã®çµ‚äº†è¨­å‚™ID
        chunk_size: ãƒšãƒ¼ã‚¸å˜ä½æ›ç®—ç”¨ã®IDä»¶æ•°ï¼ˆé€šå¸¸ã¯100ä»¶ï¼‰
        stop_threshold: äº’æ›æ€§ç¶­æŒç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆç¾è¡Œå®Ÿè£…ã§ã¯ãƒšãƒ¼ã‚¸ã‚¹ã‚­ãƒ£ãƒ³åœæ­¢æ¡ä»¶ã«åˆ©ç”¨ï¼‰
        log_callback: ãƒ­ã‚°å‡ºåŠ›ç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        cancel_checker: ã‚­ãƒ£ãƒ³ã‚»ãƒ«çŠ¶æ…‹ã‚’è¿”ã™ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        listing_scraper: FacilityListingScraper ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆæœªæŒ‡å®šã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰

    Returns:
        list[int]: è¨­å‚™ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰åé›†ã—ãŸè¨­å‚™IDã®ãƒªã‚¹ãƒˆ
    """

    if start_id > end_id:
        logger.warning("start_id ãŒ end_id ã‚ˆã‚Šå¤§ãã„ãŸã‚ã€ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã—ã¾ã™")
        return []

    if chunk_size <= 0:
        raise ValueError("chunk_size must be positive")
    if stop_threshold <= 0:
        raise ValueError("stop_threshold must be positive")

    log = log_callback or (lambda _msg: None)
    is_cancelled = cancel_checker or (lambda: False)
    scraper = listing_scraper or FacilityListingScraper()

    start_page = max(1, math.ceil(start_id / chunk_size))
    requested_end_page = max(start_page, math.ceil(end_id / chunk_size))

    log(
        f"ğŸ” è¨­å‚™ä¸€è¦§ãƒ¢ãƒ¼ãƒ‰: display_result=2 / 1ãƒšãƒ¼ã‚¸ {LISTING_PER_PAGE}ä»¶"
    )

    summary = scraper.get_listing_summary()
    if summary:
        end_page = min(requested_end_page, summary.final_page)
        log(
            f"ğŸ“‹ è¨­å‚™ä¸€è¦§ã‚µãƒãƒª: ç·ä»¶æ•° {summary.total_count} ä»¶ / æœ€çµ‚ãƒšãƒ¼ã‚¸ {summary.final_page}"
        )
    else:
        end_page = requested_end_page
        log("âš  è¨­å‚™ä¸€è¦§ã‚µãƒãƒªã®å–å¾—ã«å¤±æ•—ã—ãŸãŸã‚ã€æŒ‡å®šãƒšãƒ¼ã‚¸ç¯„å›²ã®ã¿ã‚¹ã‚­ãƒ£ãƒ³ã—ã¾ã™")

    if end_page < start_page:
        log("âš  æŒ‡å®šç¯„å›²ã«è©²å½“ã™ã‚‹ãƒšãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“")
        return []

    max_empty_pages = max(1, math.ceil(stop_threshold / chunk_size))
    current_empty_pages = 0
    collected_ids: list[int] = []

    for page in range(start_page, end_page + 1):
        if is_cancelled():
            log("âš  ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
            break

        page_ids = scraper.collect_facility_ids(
            start_page=page,
            end_page=page,
            log_callback=log,
            cancel_checker=is_cancelled,
        )

        if page_ids:
            current_empty_pages = 0
            collected_ids.extend(page_ids)
        else:
            current_empty_pages += 1
            if current_empty_pages >= max_empty_pages:
                log(
                    f"âœ‹ {LISTING_PER_PAGE * current_empty_pages}ä»¶åˆ†ã®è¨­å‚™ãŒé€£ç¶šã§è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚åœæ­¢"
                )
                break

        if len(page_ids) < LISTING_PER_PAGE:
            # æœ€çµ‚ãƒšãƒ¼ã‚¸ã«åˆ°é”ã—ãŸã¨åˆ¤æ–­
            break

    unique_ids = list(dict.fromkeys(collected_ids))
    log(f"âœ… æœ‰åŠ¹ãªè¨­å‚™ID: {len(unique_ids)}ä»¶")
    return unique_ids
