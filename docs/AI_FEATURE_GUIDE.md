# AI機能ガイド - ARIM RDE Tool v1.12.5

## 概要

ARIM RDE Toolに統合されたAI機能の包括的ガイドです。  
v1.12.5では、レスポンス情報表示機能、GPT-5対応、CLI版テストツール等が追加されました。

## 🚀 v1.12.5の新機能

### レスポンス情報表示機能
- **問い合わせ結果の詳細情報**: テキストエリア展開時にモデル名・応答時間・トークン使用量を自動表示
- **レスポンス情報ポップアップ**: 絵文字付きの詳細情報ポップアップ表示
- **統一的情報管理**: 成功・失敗時の包括的なレスポンス情報追跡

### GPT-5モデル対応
- **新パラメータ体系**: `max_completion_tokens` vs `max_tokens`の条件分岐処理
- **後方互換性**: 従来モデルとの完全互換性維持
- **エラーハンドリング**: パラメータ不整合時の適切な処理

### CLI版テストツール
- **独立実行**: `tools/ai_test_cli.py`による単体テスト機能
- **マルチプロバイダー**: OpenAI、Gemini、Local LLMの一括テスト
- **パフォーマンス計測**: レスポンス時間・トークン使用量の詳細測定

## 設定方法

### 1. AI設定ファイルの作成・編集

`input/ai_config.json` ファイルを作成または編集して、使用したいAIプロバイダーを有効にしてください。

#### 設定ファイルテンプレート

以下の内容で `input/ai_config.json` を作成してください：

```json
{
  "ai_providers": {
    "openai": {
      "enabled": true,
      "api_key": "YOUR_OPENAI_API_KEY_HERE",
      "base_url": "https://api.openai.com/v1",
      "models": [
        "gpt-5","gpt-5-mini","gpt-5-nano",
        "gpt-4.1","gpt-4.1-mini","gpt-4.1-nano","o4-mini-deep-research",
        "gpt-4o",
        "gpt-4o-mini", 
        "gpt-4-turbo",
        "gpt-3.5-turbo"
      ],
      "default_model": "gpt-4o-mini"
    },
    "gemini": {
      "enabled": true,
      "api_key": "YOUR_GEMINI_API_KEY_HERE",
      "base_url": "https://generativelanguage.googleapis.com/v1beta",
      "models": [
        "gemini-2.0-flash",
        "gemini-1.5-pro",
        "gemini-1.5-flash",
        "gemini-1.0-pro"
      ],
      "default_model": "gemini-2.0-flash"
    },
    "local_llm": {
      "enabled": false,
      "base_url": "http://localhost:11434/v1",
      "models": [
        "llama3.2:3b",
        "llama3.2:1b",
        "codellama:7b",
        "mistral:7b"
      ],
      "default_model": "llama3.2:3b",
      "note": "Ollama等のローカルLLMサーバーが必要です"
    }
  },
  "default_provider": "gemini",
  "timeout": 120,
  "max_tokens": 1000,
  "temperature": 0.7
}
```

#### 設定項目の説明

- `enabled`: そのプロバイダーを有効にするかどうか (true/false)
- `api_key`: APIアクセスキー（OpenAI、Geminiで必要）
- `base_url`: APIのベースURL
- `models`: 利用可能なモデル一覧
- `default_model`: デフォルトで選択されるモデル
- `timeout`: API呼び出しのタイムアウト時間（秒）
- `max_tokens`: 最大トークン数
- `temperature`: 生成時の創造性（0.0-1.0）

### 2. APIキーの設定

#### OpenAI
1. [OpenAI Platform](https://platform.openai.com/)でアカウント作成
2. API Keysセクションで新しいAPIキーを生成
3. `ai_config.json`の`openai.api_key`に設定

#### Gemini
1. [Google AI Studio](https://aistudio.google.com/)でアカウント作成
2. API Keyを生成
3. `ai_config.json`の`gemini.api_key`に設定

#### ローカルLLM（Ollama）
1. [Ollama](https://ollama.com/)をインストール
2. 使用したいモデルをダウンロード：
   ```bash
   ollama pull llama3.2:3b
   ollama serve
   ```
3. `ai_config.json`の`local_llm.base_url`を確認（デフォルト: http://localhost:11434/v1）

## 使用方法

### 1. AIテスト機能の起動

1. ARIM RDE Toolを起動
2. 左側メニューから「AIテスト」ボタンをクリック
3. AIテスト画面が表示されます

#### 基本機能

**1. 接続テスト**
- AIプロバイダーとモデルを選択
- 「接続テスト」ボタンをクリック
- 接続が成功すると「Hello!」の応答が表示されます

**2. 自由プロンプト送信**
- プロンプト欄に質問やタスクを入力
- 「送信」ボタンをクリック
- AIからの応答がレスポンス欄に表示されます

**3. マテリアルインデックス分析**（NEW!）
- 課題番号をドロップダウンリストから選択（289件の課題番号から検索・選択可能）
  - 検索機能: 課題番号や課題名で部分一致検索
  - 実験件数表示: 各課題番号の実験データ件数を表示
  - 詳細情報: 選択した課題の課題名と目的を自動表示
- 「更新」ボタンで最新の実験データを再読み込み
- 「MI分析」ボタンをクリックして分析実行
- 該当する実験データを自動分析し、適切なマテリアルインデックスを推奨

#### 課題番号ドロップダウンの機能詳細

**検索・補完機能**
- ドロップダウンリストは編集可能で、タイピングによる検索が可能
- 部分一致検索（課題番号、課題名で検索）
- オートコンプリート機能により候補が自動表示

**表示形式**
```
JPMXP1222TU0014 (14件) - 酸化物分散強化合金微細組織解析
```
- 課題番号
- 実験データ件数
- 課題名

**エラーハンドリング**
- ファイルが見つからない場合の適切なエラーメッセージ
- 空ファイルや読み込みエラーの検出
- データ形式不正の検出と対処

#### マテリアルインデックス分析機能の詳細

この機能は以下の処理を自動実行します：

1. **実験データの読み込み**: `input/ai/exp.xlsx`から実験情報を取得
2. **マテリアルインデックスの読み込み**: `input/ai/MI.json`から材料分類データを取得
3. **プロンプトテンプレートの適用**: `input/ai/prompts/material_index.txt`のテンプレートを使用
4. **AI分析の実行**: 実験データとマテリアルインデックスを組み合わせてAI分析
5. **結果表示**: 研究概要の要約と推奨マテリアルインデックス3つを表示

#### 分析結果の出力形式

```
**研究概要要約（200文字程度）**
（実験内容の要約）

**推奨マテリアルインデックス**
1. 【カテゴリ】→【サブカテゴリ】→【具体的材料名】
   理由：（選択理由）

2. 【カテゴリ】→【サブカテゴリ】→【具体的材料名】
   理由：（選択理由）

3. 【カテゴリ】→【サブカテゴリ】→【具体的材料名】
   理由：（選択理由）
```

### 2. AI設定

1. **AIプロバイダー**: 使用するAIサービスを選択
2. **モデル**: 選択したプロバイダーの利用可能なモデルから選択
3. **接続テスト**: 「接続テスト」ボタンでAIとの接続を確認

### 3. プロンプト送信

1. **プロンプト**: テキストエリアに送信したいメッセージを入力
2. **送信**: 「送信」ボタンでAIにプロンプトを送信
3. **レスポンス**: AIからの応答が下部のテキストエリアに表示されます

## 機能

### 対応AIプロバイダー

- **OpenAI**: GPT-4o, GPT-4o-mini, GPT-4-turbo, GPT-3.5-turbo
- **Gemini**: Gemini-1.5-pro, Gemini-1.5-flash, Gemini-1.0-pro  
- **ローカルLLM**: Ollama等のOpenAI互換API

### 主要機能

- 複数AIプロバイダーの統合管理
- モデル選択機能
- 接続テスト機能
- プロンプト履歴表示
- エラーハンドリング
- 使用量表示（対応APIの場合）

## 今後の拡張予定

- 既存機能への統合（データ取得時のサジェスト等）
- プロンプト履歴の保存・呼び出し
- バッチ処理機能
- カスタムプロンプトテンプレート
- ファイルアップロード対応（画像解析等）

## トラブルシューティング

### よくある問題

1. **「設定なし」と表示される**
   - `ai_config.json`でプロバイダーが`enabled: true`になっているか確認
   - APIキーが正しく設定されているか確認

2. **接続エラー**
   - インターネット接続を確認
   - APIキーが有効か確認
   - ローカルLLMの場合、サーバーが起動しているか確認

3. **モデルが表示されない**
   - プロバイダーの設定を確認
   - 利用可能なモデル一覧が正しく設定されているか確認

4. **「データセットを選択してください」エラー（データ取得2機能）**
   - データ取得2でデータセットが選択されていない場合に表示されます
   - ドロップダウンからデータセットを選択してから実行してください

### ログの確認

エラーが発生した場合は、レスポンス欄にエラーメッセージが表示されます。  
詳細なログは`output/log/rde_tool.log`で確認できます。

## v1.12.5 詳細機能ガイド

### レスポンス情報表示機能

#### 問い合わせ結果の詳細情報表示
AI分析実行後、問い合わせ結果のテキストエリア展開時に以下の情報が自動表示されます：

```
=== 問い合わせ結果 ===
[AI分析結果の内容]

==================================================
【レスポンス情報】
モデル: gemini-2.0-flash
応答時間: 2.45秒
トークン使用量:
  totalTokens: 1234
分析種別: AI問い合わせ
==================================================
```

#### レスポンス情報ポップアップ
「レスポンス情報」ボタンクリックで表示される詳細ポップアップ：

```
=== AIレスポンス内容 ===

📊 モデル: gemini-2.0-flash
⏱️ 応答時間: 2.45秒
🪙 トークン使用量: 1234
🔍 分析タイプ: AI問い合わせ

==================================================
[AI分析結果の内容]
```

### GPT-5モデル対応

#### 新パラメータ体系
GPT-5系モデルでは `max_completion_tokens` パラメータを使用：

```python
# GPT-5系: max_completion_tokens
# 従来モデル: max_tokens
```

#### 対応モデル一覧
- `gpt-5`
- `gpt-5-mini`
- `gpt-5-nano`

### CLI版テストツール

#### 基本的な使用方法
```bash
# 基本テスト
python tools/ai_test_cli.py --model gpt-4o-mini --chars 500

# Geminiでテスト
python tools/ai_test_cli.py --provider gemini --model gemini-2.0-flash --chars 1000

# 複数回実行
python tools/ai_test_cli.py --model gpt-4o-mini --chars 500 --repeat 3
```

#### 一括テスト実行
```powershell
# PowerShell版
.\tools\run_ai_tests.ps1

# Batch版
.\tools\run_ai_tests.bat
```

#### テスト結果分析
```bash
# 結果ファイルの分析
python tools/ai_result_analyzer.py output/ai_test_results_*.json
```

### デフォルトプロバイダー設定

#### 設定ファイルでの指定
`ai_config.json`の`default_provider`で指定：

```json
{
  "default_provider": "gemini",
  // その他の設定...
}
```

#### アプリケーション起動時の自動選択
- アプリ起動時に設定ファイルの`default_provider`を自動読み込み
- 指定されたプロバイダーが無効な場合は最初の有効なプロバイダーを選択

### トラブルシューティング

#### レスポンス情報が表示されない場合
1. AI分析が完了していることを確認
2. 問い合わせ結果のテキストエリア展開ボタンをクリック
3. `last_response_info`が適切に保存されているか確認

#### GPT-5モデルでエラーが発生する場合
1. `max_completion_tokens`パラメータ対応の確認
2. 従来モデル（gpt-4o-mini等）での動作確認
3. APIキーの権限確認

#### CLI版テストツールの問題
1. 仮想環境のアクティベーション確認
2. 依存ライブラリのインストール確認：`pip install -r src/requirements.txt`
3. 設定ファイル（ai_config.json）の存在・内容確認

## v1.11.0 更新内容

### 新機能
- AI統合機能の追加（OpenAI, Gemini, ローカルLLM対応）
- AIテスト用UI機能（プロンプト入力・レスポンス表示・モデル選択）
- AI設定管理システム（JSON設定ファイルによる柔軟な設定）

### バグ修正
- データ取得2機能のエラーハンドリング強化（異常終了防止）
- データセット未選択時のポップアップエラー対応

### 改善
- 必要モジュールの自動検出・インストール機能
- requirements.txt の更新・整備

## セキュリティ注意事項

- APIキーは安全に管理してください
- `ai_config.json`をバージョン管理システムにコミットしないよう注意
- 機密情報を含むプロンプトの送信は避けてください
- ローカルLLMを使用する場合は、適切なセキュリティ設定を行ってください
